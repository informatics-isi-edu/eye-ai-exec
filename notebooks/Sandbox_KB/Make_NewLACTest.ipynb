{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2006ce9-6ca1-402d-b9ad-8853b85509cf",
   "metadata": {},
   "source": [
    "# Connect Eye-AI and Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "003248d9-3365-4aaa-874a-858234cb2f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo_dir = \"Repos\"   # Set this to be where your github repos are located.\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# # Update the load path so python can find modules for the model\n",
    "# import sys\n",
    "# from pathlib import Path\n",
    "# sys.path.insert(0, str(Path.home() / repo_dir / \"eye-ai-ml\"))\n",
    "# sys.path.insert(0, str(Path.home() / repo_dir / \"deriva-ml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8499060d-4de0-4ab1-bf23-8f766d24dd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 14:34:59.098069: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-24 14:34:59.098115: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-24 14:34:59.239103: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-24 14:34:59.514999: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-24 14:35:01.575672: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Prerequisites\n",
    "import json\n",
    "import os\n",
    "from eye_ai.eye_ai import EyeAI\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path, PurePath\n",
    "import logging\n",
    "\n",
    "#from deriva_ml import DatasetBag, Workflow, ExecutionConfiguration\n",
    "from deriva_ml import DerivaML, Workflow, ExecutionConfiguration, VersionPart\n",
    "from deriva_ml import MLVocab as vc\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f254b11-9f01-4185-95a0-cd7f80995c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 14:35:03,492 - INFO - Creating client of type <class 'globus_sdk.services.auth.client.native_client.NativeAppAuthClient'> for service \"auth\"\n",
      "2025-03-24 14:35:03,494 - INFO - Finished initializing AuthLoginClient. client_id='8ef15ba9-2b4a-469c-a163-7fd910c9d111', type(authorizer)=<class 'globus_sdk.authorizers.base.NullAuthorizer'>\n",
      "2025-03-24 14:35:03,496 - INFO - Setting up RefreshTokenAuthorizer with auth_client=[instance:139875693761664]\n",
      "2025-03-24 14:35:03,497 - INFO - Setting up a RenewingAuthorizer. It will use an auth type of Bearer and can handle 401s.\n",
      "2025-03-24 14:35:03,497 - INFO - RenewingAuthorizer will start by using access_token with hash \"5eb628ddd4882be3813093734615056ecacd99e8cc0717c1430965acb42a3e07\"\n",
      "2025-03-24 14:35:03,498 - INFO - Executing token refresh without client credentials\n",
      "2025-03-24 14:35:03,499 - INFO - Fetching new token from Globus Auth\n",
      "2025-03-24 14:35:03,938 - INFO - request done (success)\n",
      "2025-03-24 14:35:03,939 - INFO - RenewingAuthorizer.access_token updated to token with hash \"b93aa8e21843512eeaa7b267543803c04ee5dad58b74acd8b760204980bcc6c4\"\n",
      "2025-03-24 14:35:03,939 - INFO - Setting up RefreshTokenAuthorizer with auth_client=[instance:139875693761664]\n",
      "2025-03-24 14:35:03,940 - INFO - Setting up a RenewingAuthorizer. It will use an auth type of Bearer and can handle 401s.\n",
      "2025-03-24 14:35:03,940 - INFO - RenewingAuthorizer will start by using access_token with hash \"adf62bb288f3c368fcaeef67720372939f804fb973e362eb043305fd4005d1e2\"\n",
      "2025-03-24 14:35:03,941 - INFO - Executing token refresh without client credentials\n",
      "2025-03-24 14:35:03,941 - INFO - Fetching new token from Globus Auth\n",
      "2025-03-24 14:35:04,097 - INFO - request done (success)\n",
      "2025-03-24 14:35:04,098 - INFO - RenewingAuthorizer.access_token updated to token with hash \"6b5b0f17effecd2caca05f5026f19ac02057990c42a9cd40f1855ba0262662f3\"\n",
      "2025-03-24 14:35:04,098 - INFO - Setting up RefreshTokenAuthorizer with auth_client=[instance:139875693761664]\n",
      "2025-03-24 14:35:04,099 - INFO - Setting up a RenewingAuthorizer. It will use an auth type of Bearer and can handle 401s.\n",
      "2025-03-24 14:35:04,099 - INFO - RenewingAuthorizer will start by using access_token with hash \"643c0bdea060313d2492c8e690e742072ac3f8e89e4a5aa5fb8b64b3f0e3ccee\"\n",
      "2025-03-24 14:35:04,100 - INFO - Executing token refresh without client credentials\n",
      "2025-03-24 14:35:04,100 - INFO - Fetching new token from Globus Auth\n",
      "2025-03-24 14:35:04,236 - INFO - request done (success)\n",
      "2025-03-24 14:35:04,236 - INFO - RenewingAuthorizer.access_token updated to token with hash \"f6adf9d87abee1e8699d7518a7c11957152de0a3c382c3727c9a0a52e6289a3d\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are already logged in.\n"
     ]
    }
   ],
   "source": [
    "# Login\n",
    "from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\n",
    "host = 'www.eye-ai.org'\n",
    "#host = 'dev.eye-ai.org' #for dev testing\n",
    "catalog_id = \"eye-ai\"\n",
    "\n",
    "gnl = GlobusNativeLogin(host=host)\n",
    "if gnl.is_logged_in([host]):\n",
    "    print(\"You are already logged in.\")\n",
    "else:\n",
    "    gnl.login([host], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n",
    "    print(\"Login Successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420ee433-9dce-460f-ab4d-0aff7d4eea0e",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a40db10-6f64-4b6e-a22d-3c7011e2a3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 14:35:23,511 - INFO - Creating client of type <class 'globus_sdk.services.auth.client.native_client.NativeAppAuthClient'> for service \"auth\"\n",
      "2025-03-24 14:35:23,512 - INFO - Finished initializing AuthLoginClient. client_id='8ef15ba9-2b4a-469c-a163-7fd910c9d111', type(authorizer)=<class 'globus_sdk.authorizers.base.NullAuthorizer'>\n"
     ]
    }
   ],
   "source": [
    "cache_dir = '/data'\n",
    "working_dir = '/data'\n",
    "EA = EyeAI(hostname = host, catalog_id = catalog_id, cache_dir= cache_dir, working_dir=working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85ad0a5a-4bcc-423b-a020-3e84e7506055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetVersion(major=2, minor=0, patch=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#ml_instance.increment_dataset_version(dataset_rid='2-N93J', component= VersionPart.patch, description='Update to latest deriva-ml schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c7340a9-2356-481b-bbd9-2f1f30955ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 14:35:29,800 - INFO - Creating client of type <class 'globus_sdk.services.auth.client.native_client.NativeAppAuthClient'> for service \"auth\"\n",
      "2025-03-24 14:35:29,801 - INFO - Finished initializing AuthLoginClient. client_id='8ef15ba9-2b4a-469c-a163-7fd910c9d111', type(authorizer)=<class 'globus_sdk.authorizers.base.NullAuthorizer'>\n",
      "2025-03-24 14:35:31,456 - INFO - Loading /home/kb_766/deriva-ml/cache/2-N93J_9a75d3c2b53de08134358a4de4fd06d182218938b6a737bce545ac887954bcb1/Dataset_2-N93J\n",
      "2025-03-24 14:35:32,882 - INFO - Creating new database for dataset: 2-N93J in /home/kb_766/deriva-ml/cache/2-N93J_9a75d3c2b53de08134358a4de4fd06d182218938b6a737bce545ac887954bcb1/2-N93J@330-1H6W-MEKW.db\n",
      "2025-03-24 14:35:33,369 - INFO - File [/home/kb_766/deriva-ml/DerivaML_working/4-S42C/asset/predictions_results.csv] transfer successful. 59.57 KB transferred. Elapsed time: 0:00:00.000946.\n",
      "2025-03-24 14:35:33,369 - INFO - Verifying SHA256 checksum for downloaded file [/home/kb_766/deriva-ml/DerivaML_working/4-S42C/asset/predictions_results.csv]\n"
     ]
    }
   ],
   "source": [
    "source_dataset = \"2-N93J\"\n",
    "asset_RID = [\"2-C8JM\"]\n",
    "ml_instance = DerivaML(host, catalog_id=\"eye-ai\")\n",
    "\n",
    "\n",
    "preds_workflow = EA.add_workflow( \n",
    "    Workflow(\n",
    "        name=\"LAC data template\",\n",
    "        url=\"https://github.com/informatics-isi-edu/eye-ai-exec/blob/main/notebooks/Sandbox_KB/Get_VGGPreds.ipynb\",\n",
    "        workflow_type=\"Test Workflow\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "config = ExecutionConfiguration(\n",
    "    datasets=[\n",
    "        {\n",
    "            \"rid\": source_dataset,\n",
    "            \"materialize\": False,\n",
    "            \"version\": ml_instance.dataset_version(source_dataset),\n",
    "        }\n",
    "    ],\n",
    "    assets=asset_RID,\n",
    "    workflow=preds_workflow,\n",
    "    description=\"Instance of linking VGG19 predictions to patient-level data\",\n",
    "    )\n",
    "\n",
    "exec = ml_instance.create_execution(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9db71483-dce2-4fdf-bcbc-0c952989d8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caching_dir: /home/kb_766/deriva-ml/cache\n",
      "_working_dir: /home/kb_766/deriva-ml/DerivaML_working\n",
      "execution_rid: 4-S42C\n",
      "workflow_rid: 4-M4TT\n",
      "asset_paths: [PosixPath('/home/kb_766/deriva-ml/DerivaML_working/4-S42C/asset/predictions_results.csv')]\n",
      "configuration: datasets=[DatasetSpec(rid='2-N93J', materialize=False, version=DatasetVersion(major=2, minor=0, patch=1))] assets=['2-C8JM'] workflow='4-M4TT' description='Instance of linking VGG19 predictions to patient-level data'\n"
     ]
    }
   ],
   "source": [
    "print(exec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97e75f7-8eb7-490c-8ba7-0e00d738a723",
   "metadata": {},
   "source": [
    "# Get Pertinent Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "64273618-87e3-40ed-8e39-a8d176728fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_bag = exec.datasets[0]\n",
    "diagsTall = EA.image_tall(ds_bag, 'Initial Diagnosis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9ead0ede-d1ba-4002-9b4d-b65cd01d5d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets to create master dataframe\n",
    "\n",
    "# Function to update column names\n",
    "pd.options.mode.copy_on_write = True\n",
    "def updateCols(df, cols, colDict):\n",
    "    df = df[cols]\n",
    "    df.rename( columns = colDict, inplace = True )\n",
    "    for c in set(cols).intersection( set(colDict) ): cols[cols.index(c)] = colDict.get(c)\n",
    "    return df\n",
    "\n",
    "cols = ['Subject_RID', 'Image_RID', 'Image_Side', 'Diagnosis_Image']\n",
    "colDict = {}\n",
    "masterDF = updateCols(diagsTall, cols, colDict)\n",
    "\n",
    "masterDF = pd.merge( masterDF,\n",
    "        ds_bag.get_table_as_dataframe('Subject'),\n",
    "        left_on = 'Subject_RID',\n",
    "        right_on = 'RID',\n",
    "        how = 'left')\n",
    "\n",
    "cols.extend(['Subject_ID','Subject_Gender','Subject_Ethnicity'])\n",
    "colDict = {'Subject_ID':'EyePacs_ID'}\n",
    "masterDF = updateCols(masterDF, cols, colDict)\n",
    "\n",
    "masterDF = pd.merge( masterDF,\n",
    "        ds_bag.get_table_as_dataframe('Image'),\n",
    "        left_on = 'Image_RID',\n",
    "        right_on = 'RID',\n",
    "        suffixes = ('', '_right'),\n",
    "        how = 'left')\n",
    "\n",
    "cols.extend(['Observation'])\n",
    "colDict = {'Observation':'Observation_RID'}\n",
    "masterDF = updateCols(masterDF, cols, colDict)\n",
    "\n",
    "masterDF = pd.merge( masterDF,\n",
    "        ds_bag.get_table_as_dataframe('Observation'),\n",
    "        left_on = 'Observation_RID',\n",
    "        right_on = 'RID',\n",
    "        how = 'left')\n",
    "\n",
    "cols.extend(['date_of_encounter', 'Age'])\n",
    "masterDF = updateCols(masterDF, cols, colDict)\n",
    "\n",
    "patientDF = masterDF.drop_duplicates(subset=['Subject_RID', 'Diagnosis_Image']).reset_index() # patient level\n",
    "eyeDF = masterDF.drop_duplicates(subset=['Subject_RID', 'Diagnosis_Image', 'Image_Side']).reset_index() # eye level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a59b7c9-33e6-434a-8605-ec4517ee2079",
   "metadata": {},
   "source": [
    "# Select Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cafa91a2-6a35-4fc7-a4b7-b7ae68d127b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target total size ~600 images, so ~300 patients\n",
    "# Select ~150 case patients\n",
    "\n",
    "# Drop \"Indian subcontinent origin\" because there are so few, also none in original test set\n",
    "caseDF = patientDF[ patientDF['Diagnosis_Image'] == 'Suspected Glaucoma']\n",
    "caseDF = caseDF[ - caseDF['Subject_Ethnicity'].isin(['Latin American', 'Indian subcontinent origin', '']) ]\n",
    "\n",
    "# Drop cases if too many, for African, Asian, ethnicity not specified\n",
    "caseDF = caseDF.drop( caseDF[ caseDF['Subject_Ethnicity'] == 'African Descent'].iloc[0:45].index )\n",
    "caseDF = caseDF.drop( caseDF[ caseDF['Subject_Ethnicity'] == 'Asian'].iloc[0:22].index )\n",
    "caseDF = caseDF.drop( caseDF[ caseDF['Subject_Ethnicity'] == 'ethnicity not specified'].iloc[0:81].index )\n",
    "\n",
    "caseKey, caseCount = np.unique( caseDF['Subject_Ethnicity'], return_counts=True )\n",
    "caseCounts = dict( zip( caseKey, caseCount ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6f2b1113-d2a4-4698-9de8-af7028362d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select controls\n",
    "cntrlRatio = 1  # meaning 1 case to 2 controls\n",
    "bins = list(range(0, 101, 10))  # age bins\n",
    "\n",
    "cntrlDF = patientDF[ patientDF['Diagnosis_Image'] == 'No Glaucoma']\n",
    "cntrlDF = cntrlDF[ - cntrlDF['Subject_Ethnicity'].isin(['Latin American', 'Indian subcontinent origin', 'Multi-racial', '']) ]\n",
    "\n",
    "cntrlKey, cntrlCount = np.unique( cntrlDF['Subject_Ethnicity'], return_counts=True )\n",
    "cntrlCounts = dict( zip( cntrlKey, cntrlCount ))\n",
    "\n",
    "cntrlRIDs = []\n",
    "for e in pd.unique( cntrlDF['Subject_Ethnicity'] ):\n",
    "    needMore = 0\n",
    "    if cntrlCounts[e] > (cntrlRatio * caseCounts[e]):\n",
    "        # Enough controls to try gender matching\n",
    "        for s in pd.unique( caseDF[ caseDF['Subject_Ethnicity'] == e ].loc[:,'Subject_Gender']):\n",
    "            tempCaseDF = caseDF[ (caseDF['Subject_Ethnicity'] == e) &  (caseDF['Subject_Gender'] == s) ]\n",
    "            tempCntrlDF = cntrlDF[ (cntrlDF['Subject_Ethnicity'] == e) & (cntrlDF['Subject_Gender'] == s) ]\n",
    "\n",
    "            if tempCntrlDF.shape[0] > (cntrlRatio * tempCaseDF.shape[0]):\n",
    "                # Enough to try age matching\n",
    "                i, c = np.unique( pd.cut(tempCaseDF['Age'], bins = bins), return_counts=True )\n",
    "                tCounts = dict( zip( i, c ))\n",
    "                cntrlAges = pd.cut(tempCntrlDF['Age'], bins = bins )\n",
    "                for ind in i:\n",
    "                    cntrlMatch = np.where( cntrlAges == ind )[0]\n",
    "                    if len(cntrlMatch) < ( cntrlRatio * tCounts[ind] ):\n",
    "                        # Not enough controls for this age bin matching, take all of them\n",
    "                        cntrlRIDs.extend( tempCntrlDF.loc[ cntrlAges.index[cntrlMatch]].loc[:,'Subject_RID'] )\n",
    "                        needMore = needMore + ( ( cntrlRatio * tCounts[ind] ) - len(cntrlMatch) )\n",
    "                    else:\n",
    "                        # More than enough controls for this age bin matching, only take enough\n",
    "                        cntrlRIDs.extend( tempCntrlDF.loc[ cntrlAges.index[cntrlMatch]].loc[:,'Subject_RID'].iloc[0:( cntrlRatio * tCounts[ind] )] )\n",
    "            \n",
    "                if needMore > 0:\n",
    "                    # Not enough were age matched, take more gender matched\n",
    "                    cntrlRIDs.extend( list(set(tempCntrlDF['Subject_RID']) - set(cntrlRIDs))[0:needMore] )\n",
    "                    needMore = 0\n",
    "\n",
    "            else:\n",
    "                # Not enough for gender + age matching in this gender, do ethnicity matching\n",
    "                # First take all for that gender, then use needMore\n",
    "                cntrlRIDs.extend(tempCntrlDF['Subject_RID'])\n",
    "                needMore = needMore + ( (cntrlRatio * tempCaseDF.shape[0]) - tempCntrlDF.shape[0] )\n",
    "\n",
    "        if needMore > 0:\n",
    "            # Not enough were gender matched, take more ethnicity matched\n",
    "            cntrlRIDs.extend( list( set(caseDF[caseDF['Subject_Ethnicity'] == e].loc[:,'Subject_RID']) - set(cntrlRIDs))[0:needMore] )\n",
    "            needMore = 0\n",
    "            \n",
    "    else:\n",
    "        # Not enough controls for ethnicity + gender + age matching, take all of them\n",
    "        cntrlRIDs.extend( cntrlDF[ cntrlDF['Subject_Ethnicity'] == e ].loc[:,'Subject_RID'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bcf273e6-c570-4092-b763-93325687bcbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5       2-CDB4\n",
       "6       2-CDB4\n",
       "41      2-CDCE\n",
       "42      2-CDCE\n",
       "81      2-CDDW\n",
       "         ...  \n",
       "6968    2-CN8P\n",
       "6969    2-CN8P\n",
       "6985    2-CN9A\n",
       "6986    2-CN9A\n",
       "6987    2-CN9A\n",
       "Name: Subject_RID, Length: 645, dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put it all together in new TEST SET\n",
    "testDF = masterDF[ masterDF['Subject_RID'].isin( cntrlRIDs + list( caseDF['Subject_RID'] ) )]\n",
    "testDF['Subject_RID']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7f4c40-d50b-481a-b55b-d253ea01c37f",
   "metadata": {},
   "source": [
    "# Create Child Dataset to New LAC with New Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d6e047d0-e9b0-42cc-af13-a23ea24c6315",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset = exec.create_dataset(['LAC', 'Test'], description='A race/gender/age matched test dataset')\n",
    "EA.add_dataset_members( dataset_rid = test_dataset, members = testDF['Subject_RID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97724d8d-2d2c-4f60-8a4c-75f8c8d06b33",
   "metadata": {},
   "source": [
    "# Upload Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f27d4849-0230-4722-afa5-47f03a9d90a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 16:16:09,766 - INFO - Initializing uploader: GenericUploader v1.7.6 [Python 3.10.13, Linux-5.10.210-201.852.amzn2.x86_64-x86_64-with-glibc2.26]\n",
      "2025-03-24 16:16:09,767 - INFO - Creating client of type <class 'globus_sdk.services.auth.client.native_client.NativeAppAuthClient'> for service \"auth\"\n",
      "2025-03-24 16:16:09,767 - INFO - Finished initializing AuthLoginClient. client_id='8ef15ba9-2b4a-469c-a163-7fd910c9d111', type(authorizer)=<class 'globus_sdk.authorizers.base.NullAuthorizer'>\n",
      "2025-03-24 16:16:09,804 - INFO - Scanning files in directory [/home/kb_766/deriva-ml/DerivaML_working/deriva-ml/execution/4-S42C]...\n",
      "2025-03-24 16:16:09,805 - INFO - Including file: [/home/kb_766/deriva-ml/DerivaML_working/deriva-ml/execution/4-S42C/execution-metadata/Execution_Config/configuration.json].\n",
      "2025-03-24 16:16:09,806 - INFO - Including file: [/home/kb_766/deriva-ml/DerivaML_working/deriva-ml/execution/4-S42C/execution-metadata/Runtime_Env/environment_snapshot_ytvbrm53.txt].\n",
      "2025-03-24 16:16:09,807 - INFO - Processing: [/home/kb_766/deriva-ml/DerivaML_working/deriva-ml/execution/4-S42C/execution-metadata/Execution_Config/configuration.json]\n",
      "2025-03-24 16:16:09,808 - INFO - Computed metadata for: [/home/kb_766/deriva-ml/DerivaML_working/deriva-ml/execution/4-S42C/execution-metadata/Execution_Config/configuration.json].\n",
      "2025-03-24 16:16:09,808 - INFO - Computing checksums for file: [/home/kb_766/deriva-ml/DerivaML_working/deriva-ml/execution/4-S42C/execution-metadata/Execution_Config/configuration.json]. Please wait...\n",
      "2025-03-24 16:16:09,830 - INFO - Uploading file: [/home/kb_766/deriva-ml/DerivaML_working/deriva-ml/execution/4-S42C/execution-metadata/Execution_Config/configuration.json] to host https://www.eye-ai.org. Please wait...\n",
      "2025-03-24 16:16:10,399 - INFO - Processing: [/home/kb_766/deriva-ml/DerivaML_working/deriva-ml/execution/4-S42C/execution-metadata/Runtime_Env/environment_snapshot_ytvbrm53.txt]\n",
      "2025-03-24 16:16:10,400 - INFO - Computed metadata for: [/home/kb_766/deriva-ml/DerivaML_working/deriva-ml/execution/4-S42C/execution-metadata/Runtime_Env/environment_snapshot_ytvbrm53.txt].\n",
      "2025-03-24 16:16:10,400 - INFO - Computing checksums for file: [/home/kb_766/deriva-ml/DerivaML_working/deriva-ml/execution/4-S42C/execution-metadata/Runtime_Env/environment_snapshot_ytvbrm53.txt]. Please wait...\n",
      "2025-03-24 16:16:10,417 - INFO - Uploading file: [/home/kb_766/deriva-ml/DerivaML_working/deriva-ml/execution/4-S42C/execution-metadata/Runtime_Env/environment_snapshot_ytvbrm53.txt] to host https://www.eye-ai.org. Please wait...\n",
      "2025-03-24 16:16:10,711 - INFO - File upload processing completed: 2 files were uploaded successfully, 0 files failed to upload due to errors, 0 files were skipped because they did not satisfy the matching criteria of the configuration.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Execution_Config/configuration.json': FileUploadState(state=<UploadState.success: 0>, status='Complete', result={'URL': '/hatrac/execution_metadata/c574c9dbd4d2f1c98a1120b20c9c2b3a.configuration.json:dzyDIqTmdshSDmojKAIzT8JYnBDH1Mdx', 'RID': '4-S4R0', 'RCT': '2025-03-24T23:16:10.356903+00:00', 'RMT': '2025-03-24T23:16:10.356903+00:00', 'RCB': 'https://auth.globus.org/6022643c-876c-4a47-bafa-5b9fac2c7782', 'RMB': 'https://auth.globus.org/6022643c-876c-4a47-bafa-5b9fac2c7782', 'Filename': 'configuration.json', 'Description': None, 'Length': 226, 'MD5': 'c574c9dbd4d2f1c98a1120b20c9c2b3a', 'Execution_Metadata_Type': 'Execution_Config'}, rid='4-S4R0'),\n",
       " 'Runtime_Env/environment_snapshot_ytvbrm53.txt': FileUploadState(state=<UploadState.success: 0>, status='Complete', result={'URL': '/hatrac/execution_metadata/4bdd399d7cc638e1bb0e992d330c240b.environment_snapshot_ytvbrm53.txt:yPxwkhgeO6Ub5AcWr6kVDY8hvXhRfPpx', 'RID': '4-S4R2', 'RCT': '2025-03-24T23:16:10.635156+00:00', 'RMT': '2025-03-24T23:16:10.635156+00:00', 'RCB': 'https://auth.globus.org/6022643c-876c-4a47-bafa-5b9fac2c7782', 'RMB': 'https://auth.globus.org/6022643c-876c-4a47-bafa-5b9fac2c7782', 'Filename': 'environment_snapshot_ytvbrm53.txt', 'Description': None, 'Length': 12047, 'MD5': '4bdd399d7cc638e1bb0e992d330c240b', 'Execution_Metadata_Type': 'Runtime_Env'}, rid='4-S4R2')}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # crete asset path\n",
    "# asset_type_name = \"Diagnosis_Analysis\"\n",
    "# asset_path = exec.execution_asset_path(asset_type_name)\n",
    "\n",
    "# # save assets to asset_path\n",
    "# linkdDF.to_csv(asset_path/'ImagesToVGG19.csv', index=False)\n",
    "\n",
    "# upload assets to catalog\n",
    "exec.upload_execution_outputs(clean_folder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf78fd5-8f9e-4abf-a946-c716b9c86ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My TensorFlow (Conda)",
   "language": "python",
   "name": "my-tensorflow-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
