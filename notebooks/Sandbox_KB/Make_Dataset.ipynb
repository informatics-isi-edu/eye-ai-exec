{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2006ce9-6ca1-402d-b9ad-8853b85509cf",
   "metadata": {},
   "source": [
    "# Connect Eye-AI and Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "003248d9-3365-4aaa-874a-858234cb2f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "repo_dir = \"Repos\"   # Set this to be where your github repos are located.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# # Update the load path so python can find modules for the model\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.home() / repo_dir / \"eye-ai-ml\"))\n",
    "sys.path.insert(0, str(Path.home() / repo_dir / \"eye-ai-exec\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8499060d-4de0-4ab1-bf23-8f766d24dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "import json\n",
    "import os\n",
    "\n",
    "# EyeAI, Deriva, VGG19\n",
    "from deriva_ml import DatasetSpec, DatasetBag, Workflow, ExecutionConfiguration, VersionPart\n",
    "from deriva_ml import MLVocab as vc\n",
    "from eye_ai.eye_ai import EyeAI\n",
    "\n",
    "# ML Analytics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Other Utilities\n",
    "from pathlib import Path, PurePath\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f254b11-9f01-4185-95a0-cd7f80995c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 08:19:47,621 - INFO - Creating client of type <class 'globus_sdk.services.auth.client.native_client.NativeAppAuthClient'> for service \"auth\"\n",
      "2025-07-03 08:19:47,622 - INFO - Finished initializing AuthLoginClient. client_id='8ef15ba9-2b4a-469c-a163-7fd910c9d111', type(authorizer)=<class 'globus_sdk.authorizers.base.NullAuthorizer'>\n",
      "2025-07-03 08:19:47,624 - INFO - Setting up RefreshTokenAuthorizer with auth_client=[instance:140220945086400]\n",
      "2025-07-03 08:19:47,624 - INFO - Setting up a RenewingAuthorizer. It will use an auth type of Bearer and can handle 401s.\n",
      "2025-07-03 08:19:47,625 - INFO - RenewingAuthorizer will start by using access_token with hash \"e5b617d200a9e9a145ca7fb531df7efae087ba12052804b426fac09533479771\"\n",
      "2025-07-03 08:19:47,625 - INFO - Executing token refresh without client credentials\n",
      "2025-07-03 08:19:47,626 - INFO - Fetching new token from Globus Auth\n",
      "2025-07-03 08:19:48,069 - INFO - request done (success)\n",
      "2025-07-03 08:19:48,070 - INFO - RenewingAuthorizer.access_token updated to token with hash \"749963e76acc3dcfec542ecad6e9a8e7d5db7bffe6d5c540a188f82930621c8c\"\n",
      "2025-07-03 08:19:48,070 - INFO - Setting up RefreshTokenAuthorizer with auth_client=[instance:140220945086400]\n",
      "2025-07-03 08:19:48,071 - INFO - Setting up a RenewingAuthorizer. It will use an auth type of Bearer and can handle 401s.\n",
      "2025-07-03 08:19:48,071 - INFO - RenewingAuthorizer will start by using access_token with hash \"6934637e342d5f916b3d77c4300fe762f57315395ff86f68d50448cea3156538\"\n",
      "2025-07-03 08:19:48,072 - INFO - Executing token refresh without client credentials\n",
      "2025-07-03 08:19:48,072 - INFO - Fetching new token from Globus Auth\n",
      "2025-07-03 08:19:48,219 - INFO - request done (success)\n",
      "2025-07-03 08:19:48,220 - INFO - RenewingAuthorizer.access_token updated to token with hash \"e66d3bf2841a16c76c477db5fa24f3878cc4c6850debe403cf6403ae1aa4699b\"\n",
      "2025-07-03 08:19:48,220 - INFO - Setting up RefreshTokenAuthorizer with auth_client=[instance:140220945086400]\n",
      "2025-07-03 08:19:48,221 - INFO - Setting up a RenewingAuthorizer. It will use an auth type of Bearer and can handle 401s.\n",
      "2025-07-03 08:19:48,221 - INFO - RenewingAuthorizer will start by using access_token with hash \"fce888c1fefd979c2d2cf25d194f048b0f799bebc0cbc4ee7f877806f01588d3\"\n",
      "2025-07-03 08:19:48,221 - INFO - Executing token refresh without client credentials\n",
      "2025-07-03 08:19:48,222 - INFO - Fetching new token from Globus Auth\n",
      "2025-07-03 08:19:48,371 - INFO - request done (success)\n",
      "2025-07-03 08:19:48,372 - INFO - RenewingAuthorizer.access_token updated to token with hash \"e45f02c6212e909ed7542229f319a77767799954408223f6ade3025255a7e9b7\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are already logged in.\n"
     ]
    }
   ],
   "source": [
    "# Login\n",
    "from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\n",
    "host = 'www.eye-ai.org'\n",
    "#host = 'dev.eye-ai.org' #for dev testing\n",
    "catalog_id = \"eye-ai\"\n",
    "\n",
    "gnl = GlobusNativeLogin(host=host)\n",
    "if gnl.is_logged_in([host]):\n",
    "    print(\"You are already logged in.\")\n",
    "else:\n",
    "    gnl.login([host], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n",
    "    print(\"Login Successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420ee433-9dce-460f-ab4d-0aff7d4eea0e",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a40db10-6f64-4b6e-a22d-3c7011e2a3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 08:19:53,501 - INFO - Creating client of type <class 'globus_sdk.services.auth.client.native_client.NativeAppAuthClient'> for service \"auth\"\n",
      "2025-07-03 08:19:53,501 - INFO - Finished initializing AuthLoginClient. client_id='8ef15ba9-2b4a-469c-a163-7fd910c9d111', type(authorizer)=<class 'globus_sdk.authorizers.base.NullAuthorizer'>\n"
     ]
    }
   ],
   "source": [
    "cache_dir = '/data'\n",
    "working_dir = '/data'\n",
    "EA = EyeAI(hostname = host, catalog_id = catalog_id, cache_dir= cache_dir, working_dir=working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ad0a5a-4bcc-423b-a020-3e84e7506055",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#ml_instance.increment_dataset_version(dataset_rid='2-N93J', component= VersionPart.patch, description='Update to latest deriva-ml schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c7340a9-2356-481b-bbd9-2f1f30955ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 08:20:40,432 - INFO - Materialize bag 4-S42W... \n",
      "2025-07-03 08:20:40,542 - INFO - Creating client of type <class 'globus_sdk.services.auth.client.native_client.NativeAppAuthClient'> for service \"auth\"\n",
      "2025-07-03 08:20:40,543 - INFO - Finished initializing AuthLoginClient. client_id='8ef15ba9-2b4a-469c-a163-7fd910c9d111', type(authorizer)=<class 'globus_sdk.authorizers.base.NullAuthorizer'>\n",
      "2025-07-03 08:20:42,236 - INFO - Using cached bag for  4-S42W Version:0.2.1\n",
      "2025-07-03 08:20:42,236 - INFO - Loading /data/4-S42W_2c92df824a3b41ec944c1a5dce5c71fd97f72b0fbcbe53d3a229327621274dfd/Dataset_4-S42W\n",
      "2025-07-03 08:20:42,986 - INFO - Creating new database for dataset: 4-S42W in /data/kb_766/EyeAI_working/4-S42W@33E-APAZ-TTM4.db\n",
      "2025-07-03 08:20:43,048 - INFO - Downloading assets ...\n",
      "2025-07-03 08:20:43,333 - INFO - Initialize status finished.\n"
     ]
    }
   ],
   "source": [
    "datasets = [ \"4-S42W\" ] # for new LAC balanced\n",
    "#datasets = [ '2-C9PR' ], '4-YQVM' ] # for USC healthy/glaucoma\n",
    "#datasets = [ \"2-N93J\" ] # for new LAC balanced\n",
    "\n",
    "data_to_download = []\n",
    "for dataset in datasets:\n",
    "    ds_dict = {\n",
    "        'rid': dataset,\n",
    "        'materialize':False,\n",
    "        'version':EA.dataset_version(dataset_rid=dataset),\n",
    "    }\n",
    "    data_to_download.append(ds_dict)\n",
    "\n",
    "dataset_workflow = EA.add_workflow( \n",
    "    Workflow(\n",
    "        name=\"Make Dataset by KB\",\n",
    "        url=\"https://github.com/informatics-isi-edu/eye-ai-exec/blob/main/notebooks/Sandbox_KB/Make_Dataset.ipynb\",\n",
    "        workflow_type=\"Test Workflow\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "config = ExecutionConfiguration(\n",
    "    datasets=data_to_download,\n",
    "    workflow=dataset_workflow,\n",
    "    description=\"Instance of making a dataset; in this case, creating an angle-2 subset of 4-S42W\",\n",
    "    )\n",
    "\n",
    "execution = EA.create_execution(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9db71483-dce2-4fdf-bcbc-0c952989d8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caching_dir: /data\n",
      "_working_dir: /data/kb_766/EyeAI_working\n",
      "execution_rid: 4-Z6K4\n",
      "workflow_rid: 4-Z5Y4\n",
      "asset_paths: {}\n",
      "configuration: datasets=[DatasetSpec(rid='4-S42W', materialize=False, version=DatasetVersion(major=0, minor=2, patch=1))] assets=[] workflow='4-Z5Y4' parameters={} description='Instance of making a dataset; in this case, creating an angle-2 subset of 4-S42W' argv=['/home/kb_766/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages/ipykernel_launcher.py', '-f', '/home/kb_766/.local/share/jupyter/runtime/kernel-9b90ba4d-9b2b-4f49-b3ee-c52e2a09d828.json']\n"
     ]
    }
   ],
   "source": [
    "print(execution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97e75f7-8eb7-490c-8ba7-0e00d738a723",
   "metadata": {},
   "source": [
    "# Get Pertinent Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f4d890f-f8fc-4110-829f-f4e6f09f3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_bag = execution.datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a51e0-e149-4806-9080-ef48994fceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update column names\n",
    "pd.options.mode.copy_on_write = True\n",
    "def updateCols(df, cols, colDict):\n",
    "    df = df[cols]\n",
    "    df.rename( columns = colDict, inplace = True )\n",
    "    for c in set(cols).intersection( set(colDict) ): cols[cols.index(c)] = colDict.get(c)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4c3096-51a8-4ece-bf9c-1b48eb5b2713",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# For USC Multimodal dataset healthy/glaucoma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64273618-87e3-40ed-8e39-a8d176728fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For building the USC dataset for healthy/glaucoma\n",
    "ds_bag_OG = execution.datasets[0]\n",
    "ds_bag_healthy = execution.datasets[1]\n",
    "\n",
    "# For building the new LAC dataset with matching\n",
    "# ds_bag = exec.datasets[0]\n",
    "# diagsTall = EA.image_tall(ds_bag, 'Initial Diagnosis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d79cdc-a526-4e3b-9629-b43e53e13639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For building the USC dataset for healthy/glaucoma\n",
    "\n",
    "# Merge datasets to create master dataframe\n",
    "\n",
    "modalities = EA.extract_modality(ds_bag_OG)\n",
    "\n",
    "masterDF = pd.merge(\n",
    "    ds_bag_OG.get_table_as_dataframe('Image')[[ 'RID', 'Observation' ]],\n",
    "    ds_bag_OG.get_table_as_dataframe('Execution_Image_Fundus_Laterality')[[ 'Image', 'Image_Side' ]],\n",
    "    left_on = 'RID',\n",
    "    right_on = 'Image',\n",
    "    how = 'right').drop( 'RID', axis = 1 )\n",
    "\n",
    "masterDF = pd.merge(\n",
    "    ds_bag_OG.get_table_as_dataframe('Observation')[[ 'RID', 'Subject' ]],\n",
    "    masterDF,\n",
    "    left_on = 'RID',\n",
    "    right_on = 'Observation',\n",
    "    how = 'right').drop( 'RID', axis = 1 )\n",
    "\n",
    "masterDF = pd.merge(\n",
    "    masterDF,\n",
    "    EA.multimodal_wide(ds_bag_OG)[[ 'RID_Subject', 'Image_Side', 'RID_Clinic' ]],\n",
    "    left_on = [ 'Subject', 'Image_Side' ],\n",
    "    right_on = [ 'RID_Subject', 'Image_Side' ],\n",
    "    how = 'left').drop( 'RID_Subject', axis = 1 )\n",
    "\n",
    "masterDF = masterDF[ ~ pd.isna(masterDF['RID_Clinic']) ]\n",
    "\n",
    "masterDF = pd.merge(\n",
    "    masterDF,\n",
    "    modalities['Clinic'][[ 'RID_Clinic', 'CDR', 'Condition_Label' ]],\n",
    "    on = 'RID_Clinic',\n",
    "    how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc04913-4688-4f7b-991c-420b91426356",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# For LAC dataset with matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ead0ede-d1ba-4002-9b4d-b65cd01d5d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For building the new LAC dataset with matching\n",
    "\n",
    "# Merge datasets to create master dataframe\n",
    "# Used for unifying all data in order to make a matched dataset\n",
    "\n",
    "\n",
    "cols = ['Subject_RID', 'Image_RID', 'Image_Side', 'Diagnosis_Image']\n",
    "colDict = {}\n",
    "masterDF = updateCols(diagsTall, cols, colDict)\n",
    "\n",
    "masterDF = pd.merge( masterDF,\n",
    "        ds_bag.get_table_as_dataframe('Subject'),\n",
    "        left_on = 'Subject_RID',\n",
    "        right_on = 'RID',\n",
    "        how = 'left')\n",
    "\n",
    "cols.extend(['Subject_ID','Subject_Gender','Subject_Ethnicity'])\n",
    "colDict = {'Subject_ID':'EyePacs_ID'}\n",
    "masterDF = updateCols(masterDF, cols, colDict)\n",
    "\n",
    "masterDF = pd.merge( masterDF,\n",
    "        ds_bag.get_table_as_dataframe('Image'),\n",
    "        left_on = 'Image_RID',\n",
    "        right_on = 'RID',\n",
    "        suffixes = ('', '_right'),\n",
    "        how = 'left')\n",
    "\n",
    "cols.extend(['Observation'])\n",
    "colDict = {'Observation':'Observation_RID'}\n",
    "masterDF = updateCols(masterDF, cols, colDict)\n",
    "\n",
    "masterDF = pd.merge( masterDF,\n",
    "        ds_bag.get_table_as_dataframe('Observation'),\n",
    "        left_on = 'Observation_RID',\n",
    "        right_on = 'RID',\n",
    "        how = 'left')\n",
    "\n",
    "cols.extend(['date_of_encounter', 'Age'])\n",
    "masterDF = updateCols(masterDF, cols, colDict)\n",
    "\n",
    "patientDF = masterDF.drop_duplicates(subset=['Subject_RID', 'Diagnosis_Image']).reset_index() # patient level\n",
    "eyeDF = masterDF.drop_duplicates(subset=['Subject_RID', 'Diagnosis_Image', 'Image_Side']).reset_index() # eye level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a59b7c9-33e6-434a-8605-ec4517ee2079",
   "metadata": {},
   "source": [
    "# Select Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d93b4-8d58-46d5-9db2-3039533353e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# For USC Multimodal dataset healthy/glaucoma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd8b5d-2946-490a-963a-85257a9e341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For building the USC dataset for healthy/glaucoma\n",
    "\n",
    "# Select cases - 170 total, take 150\n",
    "caseDF = masterDF[ masterDF['Condition_Label'].isin( ['POAG', 'PACG'] ) ]\n",
    "caseDF = caseDF[ caseDF['CDR'] != '' ]\n",
    "caseDF = caseDF[ caseDF['CDR'] > 0.6 ]\n",
    "testDF = caseDF.sample( n = 150 , random_state = 42, replace = False )\n",
    "testImage_RIDS = testDF['Image']\n",
    "\n",
    "# Select controls\n",
    "testImage_RIDS = pd.concat( [ testImage_RIDS, \n",
    "        EA.filter_angle_2( ds_bag_healthy ).sample( n = 150 , random_state = 42, replace = False )['RID'] ], ignore_index=True )\n",
    "\n",
    "testImage_RIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281b40d1-a3fc-4bac-ba63-693a2da0128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "masterDF[ masterDF['Image'].isin(testImage_RIDS) ]['Condition_Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b600cce-1ac5-4965-98d0-10b9cb98887b",
   "metadata": {},
   "source": [
    "# For LAC dataset with matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafa91a2-6a35-4fc7-a4b7-b7ae68d127b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For building the new LAC dataset with matching\n",
    "# Select cases\n",
    "\n",
    "\n",
    "# Target total size ~600 images, so ~300 patients\n",
    "# Select ~150 case patients\n",
    "\n",
    "# Drop \"Indian subcontinent origin\" because there are so few, also none in original test set\n",
    "caseDF = patientDF[ patientDF['Diagnosis_Image'] == 'Suspected Glaucoma']\n",
    "caseDF = caseDF[ - caseDF['Subject_Ethnicity'].isin(['Latin American', 'Indian subcontinent origin', '']) ]\n",
    "\n",
    "# Drop cases if too many, for African, Asian, ethnicity not specified\n",
    "caseDF = caseDF.drop( caseDF[ caseDF['Subject_Ethnicity'] == 'African Descent'].iloc[0:45].index )\n",
    "caseDF = caseDF.drop( caseDF[ caseDF['Subject_Ethnicity'] == 'Asian'].iloc[0:22].index )\n",
    "caseDF = caseDF.drop( caseDF[ caseDF['Subject_Ethnicity'] == 'ethnicity not specified'].iloc[0:81].index )\n",
    "\n",
    "caseKey, caseCount = np.unique( caseDF['Subject_Ethnicity'], return_counts=True )\n",
    "caseCounts = dict( zip( caseKey, caseCount ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2b1113-d2a4-4698-9de8-af7028362d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For building the new LAC dataset with matching\n",
    "# Select controls\n",
    "\n",
    "\n",
    "cntrlRatio = 1  # meaning 1 case to 2 controls\n",
    "bins = list(range(0, 101, 10))  # age bins\n",
    "\n",
    "cntrlDF = patientDF[ patientDF['Diagnosis_Image'] == 'No Glaucoma']\n",
    "cntrlDF = cntrlDF[ - cntrlDF['Subject_Ethnicity'].isin(['Latin American', 'Indian subcontinent origin', 'Multi-racial', '']) ]\n",
    "\n",
    "cntrlKey, cntrlCount = np.unique( cntrlDF['Subject_Ethnicity'], return_counts=True )\n",
    "cntrlCounts = dict( zip( cntrlKey, cntrlCount ))\n",
    "\n",
    "cntrlRIDs = []\n",
    "for e in pd.unique( cntrlDF['Subject_Ethnicity'] ):\n",
    "    needMore = 0\n",
    "    if cntrlCounts[e] > (cntrlRatio * caseCounts[e]):\n",
    "        # Enough controls to try gender matching\n",
    "        for s in pd.unique( caseDF[ caseDF['Subject_Ethnicity'] == e ].loc[:,'Subject_Gender']):\n",
    "            tempCaseDF = caseDF[ (caseDF['Subject_Ethnicity'] == e) &  (caseDF['Subject_Gender'] == s) ]\n",
    "            tempCntrlDF = cntrlDF[ (cntrlDF['Subject_Ethnicity'] == e) & (cntrlDF['Subject_Gender'] == s) ]\n",
    "\n",
    "            if tempCntrlDF.shape[0] > (cntrlRatio * tempCaseDF.shape[0]):\n",
    "                # Enough to try age matching\n",
    "                i, c = np.unique( pd.cut(tempCaseDF['Age'], bins = bins), return_counts=True )\n",
    "                tCounts = dict( zip( i, c ))\n",
    "                cntrlAges = pd.cut(tempCntrlDF['Age'], bins = bins )\n",
    "                for ind in i:\n",
    "                    cntrlMatch = np.where( cntrlAges == ind )[0]\n",
    "                    if len(cntrlMatch) < ( cntrlRatio * tCounts[ind] ):\n",
    "                        # Not enough controls for this age bin matching, take all of them\n",
    "                        cntrlRIDs.extend( tempCntrlDF.loc[ cntrlAges.index[cntrlMatch]].loc[:,'Subject_RID'] )\n",
    "                        needMore = needMore + ( ( cntrlRatio * tCounts[ind] ) - len(cntrlMatch) )\n",
    "                    else:\n",
    "                        # More than enough controls for this age bin matching, only take enough\n",
    "                        cntrlRIDs.extend( tempCntrlDF.loc[ cntrlAges.index[cntrlMatch]].loc[:,'Subject_RID'].iloc[0:( cntrlRatio * tCounts[ind] )] )\n",
    "            \n",
    "                if needMore > 0:\n",
    "                    # Not enough were age matched, take more gender matched\n",
    "                    cntrlRIDs.extend( list(set(tempCntrlDF['Subject_RID']) - set(cntrlRIDs))[0:needMore] )\n",
    "                    needMore = 0\n",
    "\n",
    "            else:\n",
    "                # Not enough for gender + age matching in this gender, do ethnicity matching\n",
    "                # First take all for that gender, then use needMore\n",
    "                cntrlRIDs.extend(tempCntrlDF['Subject_RID'])\n",
    "                needMore = needMore + ( (cntrlRatio * tempCaseDF.shape[0]) - tempCntrlDF.shape[0] )\n",
    "\n",
    "        if needMore > 0:\n",
    "            # Not enough were gender matched, take more ethnicity matched\n",
    "            cntrlRIDs.extend( list( set(caseDF[caseDF['Subject_Ethnicity'] == e].loc[:,'Subject_RID']) - set(cntrlRIDs))[0:needMore] )\n",
    "            needMore = 0\n",
    "            \n",
    "    else:\n",
    "        # Not enough controls for ethnicity + gender + age matching, take all of them\n",
    "        cntrlRIDs.extend( cntrlDF[ cntrlDF['Subject_Ethnicity'] == e ].loc[:,'Subject_RID'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf273e6-c570-4092-b763-93325687bcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all together in new TEST SET\n",
    "testDF = masterDF[ masterDF['Subject_RID'].isin( cntrlRIDs + list( caseDF['Subject_RID'] ) )]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7f4c40-d50b-481a-b55b-d253ea01c37f",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e047d0-e9b0-42cc-af13-a23ea24c6315",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# test_dataset = execution.create_dataset(['LAC', 'Test'], description='A race/gender/age matched test dataset')\n",
    "# EA.add_dataset_members( dataset_rid = test_dataset, members = testDF['Subject_RID'])\n",
    "\n",
    "test_dataset = execution.create_dataset(['USC', 'Test'], description='A test dataset for photograph interpretation for referable glaucoma - UPDATED TO ANGLE 2')\n",
    "EA.add_dataset_members( dataset_rid = test_dataset, members = testImage_RIDS )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17809f27-966e-48b3-8a91-92d666f87c01",
   "metadata": {},
   "source": [
    "# Angle-2 Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08828fc8-491f-459d-9ac5-dd80b8b86241",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Dataset.add_dataset_members\nmembers\n  Input should be a valid list [type=list_type, input_value='4-Z6K8', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/list_type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m angle2_dataset \u001b[38;5;241m=\u001b[39m execution\u001b[38;5;241m.\u001b[39mcreate_dataset([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m'\u001b[39m], description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA test dataset of images for photograph interpretation for referable glaucoma - FILTERED TO ANGLE 2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m EA\u001b[38;5;241m.\u001b[39madd_dataset_members( dataset_rid \u001b[38;5;241m=\u001b[39m angle2_dataset, members \u001b[38;5;241m=\u001b[39m angle2_image[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRID\u001b[39m\u001b[38;5;124m'\u001b[39m] )\n\u001b[0;32m----> 4\u001b[0m \u001b[43mEA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_dataset_members\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_rid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmembers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mangle2_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages/pydantic/_internal/_validate_call.py:38\u001b[0m, in \u001b[0;36mupdate_wrapper_attributes.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(wrapped)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_function\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages/pydantic/_internal/_validate_call.py:111\u001b[0m, in \u001b[0;36mValidateCallWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 111\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpydantic_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mArgsKwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__return_pydantic_validator__:\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__return_pydantic_validator__(res)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Dataset.add_dataset_members\nmembers\n  Input should be a valid list [type=list_type, input_value='4-Z6K8', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/list_type"
     ]
    }
   ],
   "source": [
    "angle2_image = EA.filter_angle_2(ds_bag)\n",
    "angle2_dataset = execution.create_dataset(['USC', 'Test'], description='A test dataset of images for photograph interpretation for referable glaucoma - FILTERED TO ANGLE 2')\n",
    "EA.add_dataset_members( dataset_rid = angle2_dataset, members = angle2_image['RID'] )\n",
    "EA.add_dataset_members( dataset_rid = datasets[0], members = [angle2_dataset] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My TensorFlow (Conda)",
   "language": "python",
   "name": "my-tensorflow-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
