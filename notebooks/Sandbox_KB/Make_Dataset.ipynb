{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2006ce9-6ca1-402d-b9ad-8853b85509cf",
   "metadata": {},
   "source": [
    "# Connect Eye-AI and Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "003248d9-3365-4aaa-874a-858234cb2f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = \"Repos\"   # Set this to be where your github repos are located.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# # Update the load path so python can find modules for the model\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.home() / repo_dir / \"eye-ai-ml\"))\n",
    "sys.path.insert(0, str(Path.home() / repo_dir / \"eye-ai-exec\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8499060d-4de0-4ab1-bf23-8f766d24dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "import json\n",
    "import os\n",
    "\n",
    "# EyeAI, Deriva, VGG19\n",
    "from deriva_ml import DatasetSpec, DatasetBag, Workflow, ExecutionConfiguration, VersionPart\n",
    "from deriva_ml import MLVocab as vc\n",
    "from eye_ai.eye_ai import EyeAI\n",
    "\n",
    "# ML Analytics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Other Utilities\n",
    "from pathlib import Path, PurePath\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f254b11-9f01-4185-95a0-cd7f80995c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 13:42:45,026 - INFO - Creating client of type <class 'globus_sdk.services.auth.client.native_client.NativeAppAuthClient'> for service \"auth\"\n",
      "2025-07-09 13:42:45,027 - INFO - Finished initializing AuthLoginClient. client_id='8ef15ba9-2b4a-469c-a163-7fd910c9d111', type(authorizer)=<class 'globus_sdk.authorizers.base.NullAuthorizer'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are already logged in.\n"
     ]
    }
   ],
   "source": [
    "# Login\n",
    "from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\n",
    "host = 'www.eye-ai.org'\n",
    "#host = 'dev.eye-ai.org' #for dev testing\n",
    "catalog_id = \"eye-ai\"\n",
    "\n",
    "gnl = GlobusNativeLogin(host=host)\n",
    "if gnl.is_logged_in([host]):\n",
    "    print(\"You are already logged in.\")\n",
    "else:\n",
    "    gnl.login([host], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n",
    "    print(\"Login Successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420ee433-9dce-460f-ab4d-0aff7d4eea0e",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a40db10-6f64-4b6e-a22d-3c7011e2a3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 13:42:51,050 - INFO - Creating client of type <class 'globus_sdk.services.auth.client.native_client.NativeAppAuthClient'> for service \"auth\"\n",
      "2025-07-09 13:42:51,051 - INFO - Finished initializing AuthLoginClient. client_id='8ef15ba9-2b4a-469c-a163-7fd910c9d111', type(authorizer)=<class 'globus_sdk.authorizers.base.NullAuthorizer'>\n"
     ]
    }
   ],
   "source": [
    "cache_dir = '/data'\n",
    "working_dir = '/data'\n",
    "EA = EyeAI(hostname = host, catalog_id = catalog_id, cache_dir= cache_dir, working_dir=working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ad0a5a-4bcc-423b-a020-3e84e7506055",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#ml_instance.increment_dataset_version(dataset_rid='2-N93J', component= VersionPart.patch, description='Update to latest deriva-ml schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c7340a9-2356-481b-bbd9-2f1f30955ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 13:43:35,891 - INFO - Materialize bag 2-277M... \n",
      "2025-07-09 13:43:36,097 - INFO - Creating client of type <class 'globus_sdk.services.auth.client.native_client.NativeAppAuthClient'> for service \"auth\"\n",
      "2025-07-09 13:43:36,098 - INFO - Finished initializing AuthLoginClient. client_id='8ef15ba9-2b4a-469c-a163-7fd910c9d111', type(authorizer)=<class 'globus_sdk.authorizers.base.NullAuthorizer'>\n",
      "2025-07-09 13:43:37,621 - INFO - Using cached bag for  2-277M Version:4.5.1\n",
      "2025-07-09 13:43:37,621 - INFO - Loading /data/2-277M_395876f849b6a2381769d7f9b9667f243d1295f1ae6b8e739c2a46d20110c6e6/Dataset_2-277M\n",
      "2025-07-09 13:43:38,649 - INFO - Creating new database for dataset: 2-277M in /data/kb_766/EyeAI_working/2-277M@33E-K95Z-QE1T.db\n",
      "2025-07-09 13:43:38,650 - INFO - Materialize bag 4-Z6K8... \n",
      "2025-07-09 13:43:38,757 - INFO - Creating client of type <class 'globus_sdk.services.auth.client.native_client.NativeAppAuthClient'> for service \"auth\"\n",
      "2025-07-09 13:43:38,757 - INFO - Finished initializing AuthLoginClient. client_id='8ef15ba9-2b4a-469c-a163-7fd910c9d111', type(authorizer)=<class 'globus_sdk.authorizers.base.NullAuthorizer'>\n",
      "2025-07-09 13:43:40,185 - INFO - Creating new MINID for dataset 4-Z6K8\n",
      "2025-07-09 13:43:41,421 - INFO - Downloading dataset minid for catalog: 4-Z6K8@0.4.0\n",
      "2025-07-09 13:43:41,422 - INFO - Creating client of type <class 'globus_sdk.services.auth.client.native_client.NativeAppAuthClient'> for service \"auth\"\n",
      "2025-07-09 13:43:41,423 - INFO - Finished initializing AuthLoginClient. client_id='8ef15ba9-2b4a-469c-a163-7fd910c9d111', type(authorizer)=<class 'globus_sdk.authorizers.base.NullAuthorizer'>\n",
      "2025-07-09 13:43:41,459 - INFO - Processing export config file: /tmp/tmpsufbg6ls/download_spec.json\n",
      "2025-07-09 13:43:41,460 - INFO - Requesting bdbag export at: https://www.eye-ai.org/deriva/export/bdbag\n",
      "2025-07-09 13:44:08,407 - INFO - Export successful. Service responded with URL list: ['https://identifiers.fair-research.org/hdl:20.500.12582/162TJxDQuw5hy', 'https://www.eye-ai.org/deriva/export/bdbag/40f50d5e-60d0-4371-978c-143c49998aa9']\n",
      "2025-07-09 13:44:08,595 - INFO - Attempting GET from URL: https://eye-ai-shared.s3.amazonaws.com/2d9eac4317de4f06b31b70546003deca/2025-07-09_13.44.06/Dataset_4-Z6K8.zip\n",
      "2025-07-09 13:44:08,662 - INFO - File [/home/kb_766/Repos/eye-ai-exec/notebooks/Sandbox_KB/Dataset_4-Z6K8.zip] transfer complete. 337.827 KB transferred. Elapsed time: 0:00:00.003444.\n",
      "2025-07-09 13:44:08,664 - INFO - Extracting ZIP archived file: /home/kb_766/Repos/eye-ai-exec/notebooks/Sandbox_KB/Dataset_4-Z6K8.zip\n",
      "2025-07-09 13:44:08,696 - INFO - File /home/kb_766/Repos/eye-ai-exec/notebooks/Sandbox_KB/Dataset_4-Z6K8.zip was successfully extracted to directory /data/4-Z6K8_f6cbfe36acb830c1c8c5744fc1d93979965f4f529bf126e06aa04f1767bfd29b/Dataset_4-Z6K8\n",
      "2025-07-09 13:44:08,697 - INFO - Validating bag structure: /data/4-Z6K8_f6cbfe36acb830c1c8c5744fc1d93979965f4f529bf126e06aa04f1767bfd29b/Dataset_4-Z6K8\n",
      "2025-07-09 13:44:08,784 - INFO - Checking payload consistency. This can take some time for large bags with many payload files...\n",
      "2025-07-09 13:44:08,871 - INFO - The directory /data/4-Z6K8_f6cbfe36acb830c1c8c5744fc1d93979965f4f529bf126e06aa04f1767bfd29b/Dataset_4-Z6K8 is a valid bag structure\n",
      "2025-07-09 13:44:08,872 - INFO - Loading /data/4-Z6K8_f6cbfe36acb830c1c8c5744fc1d93979965f4f529bf126e06aa04f1767bfd29b/Dataset_4-Z6K8\n",
      "2025-07-09 13:44:09,648 - INFO - Creating new database for dataset: 4-Z6K8 in /data/kb_766/EyeAI_working/4-Z6K8@33J-22CS-NTVA.db\n",
      "2025-07-09 13:44:09,827 - INFO - Downloading assets ...\n",
      "2025-07-09 13:44:10,356 - INFO - Initialize status finished.\n"
     ]
    }
   ],
   "source": [
    "datasets = [ \"2-277M\", \"4-Z6K8\" ]\n",
    "#datasets = [ \"4-S42W\" ] # for new LAC balanced, to make image-only dataset\n",
    "#datasets = [ '2-C9PR', '4-YQVM' ] # for USC healthy/glaucoma\n",
    "#datasets = [ \"2-N93J\" ] # for new LAC, to make balanced LAC dataset\n",
    "\n",
    "data_to_download = []\n",
    "for dataset in datasets:\n",
    "    ds_dict = {\n",
    "        'rid': dataset,\n",
    "        'materialize':False,\n",
    "        'version':EA.dataset_version(dataset_rid=dataset),\n",
    "    }\n",
    "    data_to_download.append(ds_dict)\n",
    "\n",
    "dataset_workflow = EA.add_workflow( \n",
    "    Workflow(\n",
    "        name=\"Make Dataset by KB\",\n",
    "        url=\"https://github.com/informatics-isi-edu/eye-ai-exec/blob/main/notebooks/Sandbox_KB/Make_Dataset.ipynb\",\n",
    "        workflow_type=\"Test Workflow\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "config = ExecutionConfiguration(\n",
    "    datasets=data_to_download,\n",
    "    workflow=dataset_workflow,\n",
    "    description=\"Instance of making a dataset; in this case, creating an angle-2 subset of 2-277M, 4-Z6K8\"\n",
    "    )\n",
    "\n",
    "execution = EA.create_execution(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9db71483-dce2-4fdf-bcbc-0c952989d8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caching_dir: /data\n",
      "_working_dir: /data/kb_766/EyeAI_working\n",
      "execution_rid: 4-Z6K4\n",
      "workflow_rid: 4-Z5Y4\n",
      "asset_paths: {}\n",
      "configuration: datasets=[DatasetSpec(rid='4-S42W', materialize=False, version=DatasetVersion(major=0, minor=2, patch=1))] assets=[] workflow='4-Z5Y4' parameters={} description='Instance of making a dataset; in this case, creating an angle-2 subset of 4-S42W' argv=['/home/kb_766/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages/ipykernel_launcher.py', '-f', '/home/kb_766/.local/share/jupyter/runtime/kernel-9b90ba4d-9b2b-4f49-b3ee-c52e2a09d828.json']\n"
     ]
    }
   ],
   "source": [
    "print(execution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97e75f7-8eb7-490c-8ba7-0e00d738a723",
   "metadata": {},
   "source": [
    "# Get Pertinent Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f4d890f-f8fc-4110-829f-f4e6f09f3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_bag_277M = execution.datasets[0]\n",
    "ds_bag_Z6K8 = execution.datasets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a51e0-e149-4806-9080-ef48994fceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update column names\n",
    "pd.options.mode.copy_on_write = True\n",
    "def updateCols(df, cols, colDict):\n",
    "    df = df[cols]\n",
    "    df.rename( columns = colDict, inplace = True )\n",
    "    for c in set(cols).intersection( set(colDict) ): cols[cols.index(c)] = colDict.get(c)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4c3096-51a8-4ece-bf9c-1b48eb5b2713",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# For USC Multimodal dataset healthy/glaucoma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64273618-87e3-40ed-8e39-a8d176728fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For building the USC dataset for healthy/glaucoma\n",
    "ds_bag_OG = execution.datasets[0]\n",
    "ds_bag_healthy = execution.datasets[1]\n",
    "\n",
    "# For building the new LAC dataset with matching\n",
    "# ds_bag = exec.datasets[0]\n",
    "# diagsTall = EA.image_tall(ds_bag, 'Initial Diagnosis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d79cdc-a526-4e3b-9629-b43e53e13639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For building the USC dataset for healthy/glaucoma\n",
    "\n",
    "# Merge datasets to create master dataframe\n",
    "\n",
    "modalities = EA.extract_modality(ds_bag_OG)\n",
    "\n",
    "masterDF = pd.merge(\n",
    "    ds_bag_OG.get_table_as_dataframe('Image')[[ 'RID', 'Observation' ]],\n",
    "    ds_bag_OG.get_table_as_dataframe('Execution_Image_Fundus_Laterality')[[ 'Image', 'Image_Side' ]],\n",
    "    left_on = 'RID',\n",
    "    right_on = 'Image',\n",
    "    how = 'right').drop( 'RID', axis = 1 )\n",
    "\n",
    "masterDF = pd.merge(\n",
    "    ds_bag_OG.get_table_as_dataframe('Observation')[[ 'RID', 'Subject' ]],\n",
    "    masterDF,\n",
    "    left_on = 'RID',\n",
    "    right_on = 'Observation',\n",
    "    how = 'right').drop( 'RID', axis = 1 )\n",
    "\n",
    "masterDF = pd.merge(\n",
    "    masterDF,\n",
    "    EA.multimodal_wide(ds_bag_OG)[[ 'RID_Subject', 'Image_Side', 'RID_Clinic' ]],\n",
    "    left_on = [ 'Subject', 'Image_Side' ],\n",
    "    right_on = [ 'RID_Subject', 'Image_Side' ],\n",
    "    how = 'left').drop( 'RID_Subject', axis = 1 )\n",
    "\n",
    "masterDF = masterDF[ ~ pd.isna(masterDF['RID_Clinic']) ]\n",
    "\n",
    "masterDF = pd.merge(\n",
    "    masterDF,\n",
    "    modalities['Clinic'][[ 'RID_Clinic', 'CDR', 'Condition_Label' ]],\n",
    "    on = 'RID_Clinic',\n",
    "    how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc04913-4688-4f7b-991c-420b91426356",
   "metadata": {},
   "source": [
    "# For LAC dataset with matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ead0ede-d1ba-4002-9b4d-b65cd01d5d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For building the new LAC dataset with matching\n",
    "\n",
    "# Merge datasets to create master dataframe\n",
    "# Used for unifying all data in order to make a matched dataset\n",
    "\n",
    "\n",
    "cols = ['Subject_RID', 'Image_RID', 'Image_Side', 'Diagnosis_Image']\n",
    "colDict = {}\n",
    "masterDF = updateCols(diagsTall, cols, colDict)\n",
    "\n",
    "masterDF = pd.merge( masterDF,\n",
    "        ds_bag.get_table_as_dataframe('Subject'),\n",
    "        left_on = 'Subject_RID',\n",
    "        right_on = 'RID',\n",
    "        how = 'left')\n",
    "\n",
    "cols.extend(['Subject_ID','Subject_Gender','Subject_Ethnicity'])\n",
    "colDict = {'Subject_ID':'EyePacs_ID'}\n",
    "masterDF = updateCols(masterDF, cols, colDict)\n",
    "\n",
    "masterDF = pd.merge( masterDF,\n",
    "        ds_bag.get_table_as_dataframe('Image'),\n",
    "        left_on = 'Image_RID',\n",
    "        right_on = 'RID',\n",
    "        suffixes = ('', '_right'),\n",
    "        how = 'left')\n",
    "\n",
    "cols.extend(['Observation'])\n",
    "colDict = {'Observation':'Observation_RID'}\n",
    "masterDF = updateCols(masterDF, cols, colDict)\n",
    "\n",
    "masterDF = pd.merge( masterDF,\n",
    "        ds_bag.get_table_as_dataframe('Observation'),\n",
    "        left_on = 'Observation_RID',\n",
    "        right_on = 'RID',\n",
    "        how = 'left')\n",
    "\n",
    "cols.extend(['date_of_encounter', 'Age'])\n",
    "masterDF = updateCols(masterDF, cols, colDict)\n",
    "\n",
    "patientDF = masterDF.drop_duplicates(subset=['Subject_RID', 'Diagnosis_Image']).reset_index() # patient level\n",
    "eyeDF = masterDF.drop_duplicates(subset=['Subject_RID', 'Diagnosis_Image', 'Image_Side']).reset_index() # eye level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a59b7c9-33e6-434a-8605-ec4517ee2079",
   "metadata": {},
   "source": [
    "# Select Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d93b4-8d58-46d5-9db2-3039533353e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# For USC Multimodal dataset healthy/glaucoma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd8b5d-2946-490a-963a-85257a9e341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For building the USC dataset for healthy/glaucoma\n",
    "\n",
    "# Select cases - 170 total, take 150\n",
    "caseDF = masterDF[ masterDF['Condition_Label'].isin( ['POAG', 'PACG'] ) ]\n",
    "caseDF = caseDF[ caseDF['CDR'] != '' ]\n",
    "caseDF = caseDF[ caseDF['CDR'] > 0.6 ]\n",
    "testDF = caseDF.sample( n = 150 , random_state = 42, replace = False )\n",
    "testImage_RIDS = testDF['Image']\n",
    "\n",
    "# Select controls\n",
    "testImage_RIDS = pd.concat( [ testImage_RIDS, \n",
    "        EA.filter_angle_2( ds_bag_healthy ).sample( n = 150 , random_state = 42, replace = False )['RID'] ], ignore_index=True )\n",
    "\n",
    "testImage_RIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281b40d1-a3fc-4bac-ba63-693a2da0128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "masterDF[ masterDF['Image'].isin(testImage_RIDS) ]['Condition_Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b600cce-1ac5-4965-98d0-10b9cb98887b",
   "metadata": {},
   "source": [
    "# For LAC dataset with matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafa91a2-6a35-4fc7-a4b7-b7ae68d127b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For building the new LAC dataset with matching\n",
    "# Select cases\n",
    "\n",
    "\n",
    "# Target total size ~600 images, so ~300 patients\n",
    "# Select ~150 case patients\n",
    "\n",
    "# Drop \"Indian subcontinent origin\" because there are so few, also none in original test set\n",
    "caseDF = patientDF[ patientDF['Diagnosis_Image'] == 'Suspected Glaucoma']\n",
    "caseDF = caseDF[ - caseDF['Subject_Ethnicity'].isin(['Latin American', 'Indian subcontinent origin', '']) ]\n",
    "\n",
    "# Drop cases if too many, for African, Asian, ethnicity not specified\n",
    "caseDF = caseDF.drop( caseDF[ caseDF['Subject_Ethnicity'] == 'African Descent'].iloc[0:45].index )\n",
    "caseDF = caseDF.drop( caseDF[ caseDF['Subject_Ethnicity'] == 'Asian'].iloc[0:22].index )\n",
    "caseDF = caseDF.drop( caseDF[ caseDF['Subject_Ethnicity'] == 'ethnicity not specified'].iloc[0:81].index )\n",
    "\n",
    "caseKey, caseCount = np.unique( caseDF['Subject_Ethnicity'], return_counts=True )\n",
    "caseCounts = dict( zip( caseKey, caseCount ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2b1113-d2a4-4698-9de8-af7028362d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For building the new LAC dataset with matching\n",
    "# Select controls\n",
    "\n",
    "\n",
    "cntrlRatio = 1  # meaning 1 case to 2 controls\n",
    "bins = list(range(0, 101, 10))  # age bins\n",
    "\n",
    "cntrlDF = patientDF[ patientDF['Diagnosis_Image'] == 'No Glaucoma']\n",
    "cntrlDF = cntrlDF[ - cntrlDF['Subject_Ethnicity'].isin(['Latin American', 'Indian subcontinent origin', 'Multi-racial', '']) ]\n",
    "\n",
    "cntrlKey, cntrlCount = np.unique( cntrlDF['Subject_Ethnicity'], return_counts=True )\n",
    "cntrlCounts = dict( zip( cntrlKey, cntrlCount ))\n",
    "\n",
    "cntrlRIDs = []\n",
    "for e in pd.unique( cntrlDF['Subject_Ethnicity'] ):\n",
    "    needMore = 0\n",
    "    if cntrlCounts[e] > (cntrlRatio * caseCounts[e]):\n",
    "        # Enough controls to try gender matching\n",
    "        for s in pd.unique( caseDF[ caseDF['Subject_Ethnicity'] == e ].loc[:,'Subject_Gender']):\n",
    "            tempCaseDF = caseDF[ (caseDF['Subject_Ethnicity'] == e) &  (caseDF['Subject_Gender'] == s) ]\n",
    "            tempCntrlDF = cntrlDF[ (cntrlDF['Subject_Ethnicity'] == e) & (cntrlDF['Subject_Gender'] == s) ]\n",
    "\n",
    "            if tempCntrlDF.shape[0] > (cntrlRatio * tempCaseDF.shape[0]):\n",
    "                # Enough to try age matching\n",
    "                i, c = np.unique( pd.cut(tempCaseDF['Age'], bins = bins), return_counts=True )\n",
    "                tCounts = dict( zip( i, c ))\n",
    "                cntrlAges = pd.cut(tempCntrlDF['Age'], bins = bins )\n",
    "                for ind in i:\n",
    "                    cntrlMatch = np.where( cntrlAges == ind )[0]\n",
    "                    if len(cntrlMatch) < ( cntrlRatio * tCounts[ind] ):\n",
    "                        # Not enough controls for this age bin matching, take all of them\n",
    "                        cntrlRIDs.extend( tempCntrlDF.loc[ cntrlAges.index[cntrlMatch]].loc[:,'Subject_RID'] )\n",
    "                        needMore = needMore + ( ( cntrlRatio * tCounts[ind] ) - len(cntrlMatch) )\n",
    "                    else:\n",
    "                        # More than enough controls for this age bin matching, only take enough\n",
    "                        cntrlRIDs.extend( tempCntrlDF.loc[ cntrlAges.index[cntrlMatch]].loc[:,'Subject_RID'].iloc[0:( cntrlRatio * tCounts[ind] )] )\n",
    "            \n",
    "                if needMore > 0:\n",
    "                    # Not enough were age matched, take more gender matched\n",
    "                    cntrlRIDs.extend( list(set(tempCntrlDF['Subject_RID']) - set(cntrlRIDs))[0:needMore] )\n",
    "                    needMore = 0\n",
    "\n",
    "            else:\n",
    "                # Not enough for gender + age matching in this gender, do ethnicity matching\n",
    "                # First take all for that gender, then use needMore\n",
    "                cntrlRIDs.extend(tempCntrlDF['Subject_RID'])\n",
    "                needMore = needMore + ( (cntrlRatio * tempCaseDF.shape[0]) - tempCntrlDF.shape[0] )\n",
    "\n",
    "        if needMore > 0:\n",
    "            # Not enough were gender matched, take more ethnicity matched\n",
    "            cntrlRIDs.extend( list( set(caseDF[caseDF['Subject_Ethnicity'] == e].loc[:,'Subject_RID']) - set(cntrlRIDs))[0:needMore] )\n",
    "            needMore = 0\n",
    "            \n",
    "    else:\n",
    "        # Not enough controls for ethnicity + gender + age matching, take all of them\n",
    "        cntrlRIDs.extend( cntrlDF[ cntrlDF['Subject_Ethnicity'] == e ].loc[:,'Subject_RID'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf273e6-c570-4092-b763-93325687bcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all together in new TEST SET\n",
    "testDF = masterDF[ masterDF['Subject_RID'].isin( cntrlRIDs + list( caseDF['Subject_RID'] ) )]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7f4c40-d50b-481a-b55b-d253ea01c37f",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e047d0-e9b0-42cc-af13-a23ea24c6315",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# test_dataset = execution.create_dataset(['LAC', 'Test'], description='A race/gender/age matched test dataset')\n",
    "# EA.add_dataset_members( dataset_rid = test_dataset, members = testDF['Subject_RID'])\n",
    "\n",
    "test_dataset = execution.create_dataset(['USC', 'Test'], description='A test dataset for photograph interpretation for referable glaucoma - UPDATED TO ANGLE 2')\n",
    "EA.add_dataset_members( dataset_rid = test_dataset, members = testImage_RIDS )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17809f27-966e-48b3-8a91-92d666f87c01",
   "metadata": {},
   "source": [
    "# Angle-2 Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08828fc8-491f-459d-9ac5-dd80b8b86241",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Dataset.add_dataset_members\nmembers\n  Input should be a valid list [type=list_type, input_value='4-Z6K8', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/list_type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m angle2_dataset \u001b[38;5;241m=\u001b[39m execution\u001b[38;5;241m.\u001b[39mcreate_dataset([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m'\u001b[39m], description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA test dataset of images for photograph interpretation for referable glaucoma - FILTERED TO ANGLE 2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m EA\u001b[38;5;241m.\u001b[39madd_dataset_members( dataset_rid \u001b[38;5;241m=\u001b[39m angle2_dataset, members \u001b[38;5;241m=\u001b[39m angle2_image[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRID\u001b[39m\u001b[38;5;124m'\u001b[39m] )\n\u001b[0;32m----> 4\u001b[0m \u001b[43mEA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_dataset_members\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_rid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmembers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mangle2_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages/pydantic/_internal/_validate_call.py:38\u001b[0m, in \u001b[0;36mupdate_wrapper_attributes.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(wrapped)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_function\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages/pydantic/_internal/_validate_call.py:111\u001b[0m, in \u001b[0;36mValidateCallWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 111\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpydantic_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mArgsKwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__return_pydantic_validator__:\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__return_pydantic_validator__(res)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Dataset.add_dataset_members\nmembers\n  Input should be a valid list [type=list_type, input_value='4-Z6K8', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/list_type"
     ]
    }
   ],
   "source": [
    "angle2_image = EA.filter_angle_2(ds_bag)\n",
    "angle2_dataset = execution.create_dataset(['USC', 'Test'], description='A test dataset of images for photograph interpretation for referable glaucoma - FILTERED TO ANGLE 2')\n",
    "EA.add_dataset_members( dataset_rid = angle2_dataset, members = angle2_image['RID'] )\n",
    "EA.add_dataset_members( dataset_rid = datasets[0], members = [angle2_dataset] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdefb15-6a0a-4c00-8225-9aad3f381857",
   "metadata": {},
   "source": [
    "# Angle-2 Subset from 2-277M and 4-Z6K8 for Intragrader Agreement Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dfc032a-d2d2-496f-92a3-35e659087fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Angle-2 images\n",
    "angle2_image_277M = EA.filter_angle_2(ds_bag_277M)\n",
    "angle2_image_Z6K8 = EA.filter_angle_2(ds_bag_Z6K8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7fba858-28de-49fe-a494-86a30d6bba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to 2-277M images that were graded by all pertinent experts with a CDR\n",
    "experts = ['Benjamin Xu', 'Brandon Wong', 'Van Nguyen', 'Jiun Do']\n",
    "graded_277M = EA.image_tall(ds_bag_277M, 'AI_glaucomasuspect_test')\n",
    "graded_277M = graded_277M[ graded_277M['Image_Quality' ] == 'Good']\n",
    "graded_277M = graded_277M[ graded_277M['Full_Name' ].isin( experts )]\n",
    "graded_277M['Cup_Disk_Ratio'] = pd.to_numeric( graded_277M['Cup_Disk_Ratio'], errors='coerce')\n",
    "\n",
    "def is_valid_group(group):\n",
    "    graders_present = set(group['Full_Name'])\n",
    "    all_graders_present = set(experts).issubset(graders_present)\n",
    "    all_grades_numeric = group['Cup_Disk_Ratio'].notna().all()\n",
    "    return all_graders_present and all_grades_numeric\n",
    "\n",
    "valid_rids = graded_277M.groupby('Image_RID').filter(is_valid_group)['Image_RID'].unique()\n",
    "valid_rids = valid_rids[ np.isin( valid_rids, angle2_image_277M['RID'] ) ]\n",
    "graded_277M = graded_277M[ graded_277M['Image_RID'].isin(valid_rids) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a0715363-2f34-4d9d-8810-45a02f8b863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Optom labels and keep only rows Angle-2 and for 2-277M in valid_rids (i.e., all graders graded)\n",
    "optom_277M = EA.image_tall(ds_bag_277M, 'Initial Diagnosis')\n",
    "optom_277M = optom_277M[ optom_277M['Image_RID'].isin(valid_rids) ]\n",
    "\n",
    "optom_Z6K8 = EA.image_tall(ds_bag_Z6K8, 'Initial Diagnosis')\n",
    "optom_Z6K8 = optom_Z6K8[ optom_Z6K8['Image_RID'].isin(angle2_image_Z6K8['RID']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bbd75cb2-8870-4bf2-b193-9a460dd5ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random sample \n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "test_rids = []\n",
    "for df in [optom_277M, optom_Z6K8]:\n",
    "    test_rids.extend( random.sample( sorted(df[ df['Diagnosis_Image'] == 'Suspected Glaucoma' ].loc[:,'Image_RID']), 25 ) )\n",
    "    test_rids.extend( random.sample( sorted(df[ df['Diagnosis_Image'] == 'No Glaucoma' ].loc[:,'Image_RID']), 25 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c1feedf2-9b44-4014-8e30-16049b38cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataset\n",
    "intragrader_dataset = execution.create_dataset(['LAC', 'Test'], description='A test dataset of images from 2-277M and 4-Z6K8 to test intragrader agreement')\n",
    "EA.add_dataset_members( dataset_rid = intragrader_dataset, members = test_rids )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10b9266-7a33-401a-93f7-61f8e6701f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My TensorFlow (Conda)",
   "language": "python",
   "name": "my-tensorflow-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
