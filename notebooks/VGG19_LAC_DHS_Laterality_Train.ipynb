{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3f6e9b-57c2-4dd5-82bc-5834f32b1472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# if IN_COLAB:\n",
    "#     !pip install deriva\n",
    "#     !pip install bdbag\n",
    "#     !pip install --upgrade --force pydantic\n",
    "#     !pip install git+https://github.com/informatics-isi-edu/deriva-ml git+https://github.com/informatics-isi-edu/eye-ai-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d235d595-c68f-4146-a0b7-fb2c754c9cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = \"Repos\"   # Set this to be where your github repos are located.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Update the load path so python can find modules for the model\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.home() / repo_dir / \"eye-ai-ml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c603cb-e1e1-4c90-8a91-a3e44451cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "\n",
    "import json\n",
    "import os\n",
    "from eye_ai.eye_ai import EyeAI\n",
    "import pandas as pd\n",
    "from pathlib import Path, PurePath\n",
    "import logging\n",
    "# import torch\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633c4d60-a01f-4b16-816f-228dc849ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\n",
    "catalog_id = \"eye-ai\" #@param\n",
    "host = 'www.eye-ai.org'\n",
    "\n",
    "\n",
    "gnl = GlobusNativeLogin(host=host)\n",
    "if gnl.is_logged_in([host]):\n",
    "    print(\"You are already logged in.\")\n",
    "else:\n",
    "    gnl.login([host], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n",
    "    print(\"Login Successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b7779-06d0-4678-adbe-a411da14ceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to configure the rest of the notebook.\n",
    "\n",
    "cache_dir = '/data'        # Directory in which to cache materialized BDBags for datasets\n",
    "working_dir = '/data'    # Directory in which to place output files for later upload.\n",
    "\n",
    "configuration_rid = \"2-C8RG\" # rid\n",
    "# Change the confi_file with bag_url=[\"minid: train\", \"minid: Valid\", \"minid: test\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ee651c-3c59-4f75-8f8b-2f8f41d79f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EA = EyeAI(hostname = host, catalog_id = catalog_id, cache_dir= cache_dir, working_dir=working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c50050-65da-4130-b56a-1d3fde5959b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Initiate an Execution\n",
    "configuration_records = EA.execution_init(configuration_rid=configuration_rid)\n",
    "configuration_records.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d005a2a-2edc-4d35-b508-969f2f2be5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff0eff0-f063-412c-b382-01d0a82ab33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_train = pd.read_csv(configuration_records.assets_paths[1])['ID'].to_list()\n",
    "exclude_valid = pd.read_csv(configuration_records.assets_paths[2])['ID'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f6175f-2fc7-48b5-b2a7-6bc537e7857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def create_LACDHS_laterality_dataset(train_dir: str, validation_dir: str, test_dir: str, output_dir: str, exclude_train: list = [], exclude_valid: list = []) -> tuple:\n",
    "    \"\"\"\n",
    "    Creates a dataset for LACDHS laterality classification by organizing images into train, valid, and test folders\n",
    "    based on their Image_Laterality.\n",
    "\n",
    "    Parameters:\n",
    "    - train_dir (str): Path to the raw train dataset bag.\n",
    "    - validation_dir (str): Path to the raw validation dataset bag.\n",
    "    - test_dir (str): Path to the raw test dataset bag.\n",
    "    - output_dir (str): Path to the output directory where the organized dataset will be created.\n",
    "    - exclude_train (list): List of image RIDs to exclude from the train set.\n",
    "    - exclude_valid (list): List of image RIDs to exclude from the validation set.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the paths to the train, validation, and test directories.\n",
    "    \"\"\"\n",
    "    def process_dataset(bag_path: str, output_subdir: str, exclude_list: list = []):\n",
    "        image_csv_path = os.path.join(bag_path, 'data', 'Image.csv')\n",
    "        image_df = pd.read_csv(image_csv_path)\n",
    "        image_root_path = os.path.join(bag_path, 'data', 'assets', 'Image')\n",
    "\n",
    "        for _, row in image_df.iterrows():\n",
    "            if row['RID'] not in exclude_list:\n",
    "                laterality = row['Image_Side_Vocab']\n",
    "                filename = row['Filename']\n",
    "                src_path = os.path.join(image_root_path, filename)\n",
    "                dst_dir = os.path.join(output_dir, output_subdir, laterality)\n",
    "                os.makedirs(dst_dir, exist_ok=True)\n",
    "                dst_path = os.path.join(dst_dir, filename)\n",
    "                shutil.copy2(src_path, dst_path)\n",
    "\n",
    "    # Process train dataset\n",
    "    process_dataset(train_dir, 'train', exclude_train)\n",
    "\n",
    "    # Process validation dataset\n",
    "    process_dataset(validation_dir, 'valid', exclude_valid)\n",
    "\n",
    "    # Process test dataset\n",
    "    process_dataset(test_dir, 'test')\n",
    "\n",
    "    train_path = os.path.join(output_dir, 'train')\n",
    "    valid_path = os.path.join(output_dir, 'valid')\n",
    "    test_path = os.path.join(output_dir, 'test')\n",
    "\n",
    "    return train_path, valid_path, test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ed1a5-4a9c-4478-a110-6555a5066622",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_records.working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1076c83-5acc-4668-bcef-bd00998ace6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Data Preprocessing (Filtering Image.csv for just Field_2 Images)\n",
    "train_dir = configuration_records.bag_paths[0] # path to the raw train dataset\n",
    "validation_dir = configuration_records.bag_paths[1]\n",
    "test_dir = configuration_records.bag_paths[2]\n",
    "\n",
    "exclude_train = pd.read_csv(configuration_records.assets_paths[1])['ID'].to_list()\n",
    "exclude_valid = pd.read_csv(configuration_records.assets_paths[2])['ID'].to_list()\n",
    "\n",
    "# Call the create_LACDHS_angle_dataset function\n",
    "train_path, valid_path, test_path = create_LACDHS_laterality_dataset(\n",
    "    train_dir=str(train_dir),\n",
    "    validation_dir=str(validation_dir),\n",
    "    test_dir=str(test_dir),\n",
    "    output_dir=str(configuration_records.working_dir),\n",
    "    exclude_train=exclude_train,\n",
    "    exclude_valid=exclude_valid\n",
    ")\n",
    "\n",
    "# Print the paths to verify\n",
    "print(\"Train dataset path:\", train_path)\n",
    "print(\"Validation dataset path:\", valid_path)\n",
    "print(\"Test dataset path:\", test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45475801-2b48-4930-9fe2-befc1a79423d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b600173-42b1-49d1-8602-2dac5ad97f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_files(directory):\n",
    "    return len([name for name in os.listdir(directory) if os.path.isfile(os.path.join(directory, name))])\n",
    "\n",
    "def analyze_lacdhs_angle_dataset(base_path):\n",
    "    main_folders = ['train', 'valid', 'test']\n",
    "    \n",
    "    for main_folder in main_folders:\n",
    "        main_folder_path = os.path.join(base_path, main_folder)\n",
    "        if not os.path.exists(main_folder_path):\n",
    "            print(f\"{main_folder} folder not found\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nAnalyzing {main_folder} folder:\")\n",
    "        \n",
    "        total_files = 0\n",
    "        for angle_folder in os.listdir(main_folder_path):\n",
    "            angle_folder_path = os.path.join(main_folder_path, angle_folder)\n",
    "            if os.path.isdir(angle_folder_path):\n",
    "                file_count = count_files(angle_folder_path)\n",
    "                print(f\"  {angle_folder}: {file_count} images\")\n",
    "                total_files += file_count\n",
    "        \n",
    "        print(f\"Total images in {main_folder}: {total_files}\")\n",
    "\n",
    "# Usage\n",
    "base_path = \"/data/sreenidhi/EyeAI_working/\"\n",
    "analyze_lacdhs_angle_dataset(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380f149a-dbf7-44e3-8d15-0daa2f3158cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def visualize_lacdhs_angle_dataset(base_path, samples_per_angle=6):\n",
    "    main_folders = ['train', 'valid', 'test']\n",
    "    \n",
    "    for main_folder in main_folders:\n",
    "        main_folder_path = os.path.join(base_path, main_folder)\n",
    "        if not os.path.exists(main_folder_path):\n",
    "            print(f\"{main_folder} folder not found\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nVisualizing samples from {main_folder} folder:\")\n",
    "        \n",
    "        angle_folders = [f for f in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, f))]\n",
    "        \n",
    "        # Calculate grid size\n",
    "        n_angles = len(angle_folders)\n",
    "        n_cols = samples_per_angle\n",
    "        n_rows = n_angles\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*3, n_rows*3.5))\n",
    "        fig.suptitle(f'Sample Images from {main_folder.capitalize()} Set', fontsize=16)\n",
    "        \n",
    "        for i, angle_folder in enumerate(angle_folders):\n",
    "            angle_folder_path = os.path.join(main_folder_path, angle_folder)\n",
    "            image_files = [f for f in os.listdir(angle_folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            \n",
    "            if len(image_files) < samples_per_angle:\n",
    "                print(f\"Warning: Not enough images in {angle_folder}. Using all available images.\")\n",
    "                selected_files = image_files\n",
    "            else:\n",
    "                selected_files = random.sample(image_files, samples_per_angle)\n",
    "            \n",
    "            for j, image_file in enumerate(selected_files):\n",
    "                img_path = os.path.join(angle_folder_path, image_file)\n",
    "                img = Image.open(img_path)\n",
    "                axes[i, j].imshow(img)\n",
    "                axes[i, j].axis('off')\n",
    "                \n",
    "                # Add image filename as title for each subplot\n",
    "                axes[i, j].set_title(image_file, fontsize=8)\n",
    "                \n",
    "                if j == 0:\n",
    "                    axes[i, j].set_ylabel(angle_folder, rotation=0, labelpad=40, va='center', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.95, bottom=0.05, left=0.2, right=0.98)\n",
    "        plt.show()\n",
    "        \n",
    "        # Print confirmation of angles\n",
    "        print(f\"Angles in {main_folder} set:\")\n",
    "        for angle in angle_folders:\n",
    "            print(f\"  - {angle}\")\n",
    "\n",
    "# Usage\n",
    "base_path = \"/data/sreenidhi/EyeAI_working/\"\n",
    "# visualize_lacdhs_angle_dataset(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6312b8-9faa-4ea5-9f2e-4de7380ffb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_path = str(EA.working_dir) + \"/Execution_Assets/\" + configuration_records.vocabs['Execution_Asset_Type'][0].name\n",
    "os.mkdir(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49177652-13f5-4464-99fe-bc5a240e2557",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a799f240-96f7-473a-b9b4-3feb8cf6e248",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyper_parameters_json_path = str(configuration_records.assets_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ec10b-be38-404c-9a96-4df4f0e3ca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyper_parameters_json_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b57610-80ee-43a1-9d43-c27f4acf3896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(best_hyper_parameters_json_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the contents of the JSON file\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeccf0bb-c5b7-4d04-a84c-32e98e9870b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Execute Training algorithm\n",
    "\n",
    "from eye_ai.models.vgg19_lacdhs_laterality_train import main\n",
    "\n",
    "with EA.execution(execution_rid=configuration_records.execution_rid) as exec:\n",
    "  main(train_path=train_path,\n",
    "       valid_path=valid_path, \n",
    "       test_path=test_path, \n",
    "       output_path=output_path,\n",
    "       best_hyperparameters_json_path=best_hyper_parameters_json_path,\n",
    "       model_name=\"VGG19_Catalog_LAC_DHS_Laterality_Trained_model_June_27_2024\"\n",
    "      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916dc9b5-12d3-4b2f-81c3-25e01c2310e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b95cb-0c7a-4e6b-aaa8-b5be262412bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Execute Evaluation algorithm\n",
    "from eye_ai.models.vgg19_lacdhs_laterality_predict import prediction\n",
    "with EA.execution(execution_rid=configuration_records.execution_rid) as exec:\n",
    "    prediction(\n",
    "        model_path=output_path + '/VGG19_Catalog_LAC_DHS_Laterality_Trained_model_June_27_2024.h5',\n",
    "        cropped_image_path=test_path,\n",
    "        output_dir=Path(output_path),\n",
    "        best_hyperparameters_json_path=best_hyper_parameters_json_path\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933ec6fc-517b-4e4c-a62a-ec66c36b3bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def f1_score_normal(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "model = tf.keras.models.load_model(output_path + '/VGG19_Catalog_LAC_DHS_Laterality_Trained_model_June_27_2024.h5',\n",
    "                                       custom_objects={'f1_score_normal': f1_score_normal})\n",
    "def preprocess_input_vgg19(x):\n",
    "    return tf.keras.applications.vgg19.preprocess_input(x)\n",
    "\n",
    "graded_test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_vgg19)\n",
    "\n",
    "classes = {'2SK0': 0, '2SK2': 1}\n",
    "\n",
    "graded_test_generator = graded_test_datagen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=data['batch_size'],\n",
    "    class_mode='binary',\n",
    "    classes=classes,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "# Make sure to reset the generator before starting the predictions\n",
    "# graded_test_generator.reset()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "# Initialize lists to store file names, true labels, and predicted labels\n",
    "filenames = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "scores = []\n",
    "\n",
    "# Iterate over all batches in the graded_test_generator\n",
    "\n",
    "for i in range(len(graded_test_generator)):\n",
    "    # Get a batch of data\n",
    "    batch_data = graded_test_generator[i]\n",
    "    image_batch, label_batch = batch_data[0], batch_data[1]\n",
    "    batch_filenames = graded_test_generator.filenames[i * graded_test_generator.batch_size : (i + 1) * graded_test_generator.batch_size]\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict_on_batch(image_batch).flatten()\n",
    "\n",
    "    # append bacth data to lists\n",
    "    scores.extend(predictions)\n",
    "\n",
    "    # Binarize the predictions\n",
    "    predictions = tf.where(predictions < 0.5, 0, 1).numpy()\n",
    "\n",
    "    # Append batch data to lists\n",
    "    filenames.extend(batch_filenames)\n",
    "    y_true.extend(label_batch)\n",
    "    y_pred.extend(predictions)\n",
    "\n",
    "    # For debugging, print the first batch's results\n",
    "    if i == 0:\n",
    "        # print(\"Image batch:\")\n",
    "        # print(image_batch)\n",
    "        print(\"True labels:\")\n",
    "        print(label_batch)\n",
    "        print(\"Predictions:\")\n",
    "        print(predictions)\n",
    "\n",
    "# Write to CSV file\n",
    "# with open('predictions_with_correct_predictions_and_probability_score_SG_1_NG_0.csv', 'w', newline='') as file: #s\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow(['Filename', 'True Label', 'Prediction', 'Probability Score'])\n",
    "\n",
    "#     for i in range(len(filenames)):\n",
    "#         writer.writerow([filenames[i], y_true[i], y_pred[i], scores[i]])\n",
    "\n",
    "print(\"Data saved to predictions.csv\")\n",
    "\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy = np.mean(np.array(y_pred) == np.array(y_true))\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# Generate a classification report and AUC score\n",
    "print('Classification Report:\\n', classification_report(y_true, y_pred))\n",
    "print('AUC Score:', roc_auc_score(y_true, scores))\n",
    "\n",
    "# Plot the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_true, scores)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e9502f-634a-4c50-96a2-43e89d096f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, balanced_accuracy_score, matthews_corrcoef\n",
    "\n",
    "# Calculate metrics using scikit-learn\n",
    "sklearn_roc_auc = roc_auc_score(y_true, scores)\n",
    "sklearn_f1_score = f1_score(y_true, y_pred, average='macro')\n",
    "sklearn_f1_score_normal = f1_score(y_true, y_pred)\n",
    "sklearn_precision = precision_score(y_true, y_pred)\n",
    "sklearn_recall = recall_score(y_true, y_pred)\n",
    "sklearn_accuracy = accuracy_score(y_true, y_pred)\n",
    "sklearn_balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "sklearn_matthews_corrcoef = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "print(f'\\nScikit-learn Metrics:')\n",
    "print(f'ROC AUC: {sklearn_roc_auc}')\n",
    "print(f'F1 Score: {sklearn_f1_score}')\n",
    "print(f'F1 Score Normal: {sklearn_f1_score_normal}') #t\n",
    "print(f'Precision: {sklearn_precision}')\n",
    "print(f'Recall: {sklearn_recall}')\n",
    "print(f'Accuracy: {sklearn_accuracy}')\n",
    "print(f'Balanced Accuracy: {sklearn_balanced_accuracy}')\n",
    "print(f'Matthews correlation coefficient: {sklearn_matthews_corrcoef}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a367dd-445a-41f9-bc97-354e6def6100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # @title Save Execution Assets (model) and Metadata\n",
    "uploaded_assets = EA.execution_upload(configuration_records.execution_rid, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6bb207-df9b-4ddc-933c-045eb08388fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
