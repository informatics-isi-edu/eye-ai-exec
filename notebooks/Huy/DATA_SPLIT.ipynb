{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fc8d04-85cc-4170-80f8-a7b7532a1414",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = \"Repos\"   # Set this to be where your github repos are located.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Update the load path so python can find modules for the model\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.home() / repo_dir / \"eye-ai-ml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2b2627-42fb-439f-9f82-76542f9e9c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "import json\n",
    "import os\n",
    "from eye_ai.eye_ai import EyeAI\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path, PurePath\n",
    "import logging\n",
    "\n",
    "from deriva_ml import DatasetBag, Workflow, ExecutionConfiguration, DatasetVersion\n",
    "from deriva_ml import MLVocab as vc\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca223df5-f7d8-4844-9d08-66a788ca4b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\n",
    "catalog_id = \"eye-ai\" #@param\n",
    "host = 'www.eye-ai.org'\n",
    "\n",
    "\n",
    "gnl = GlobusNativeLogin(host=host)\n",
    "if gnl.is_logged_in([host]):\n",
    "    print(\"You are already logged in.\")\n",
    "else:\n",
    "    gnl.login([host], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n",
    "    print(\"Login Successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced49616-0402-4263-a965-56c333c91bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = '/data'\n",
    "working_dir = '/data'\n",
    "EA = EyeAI(hostname = host, catalog_id = catalog_id, cache_dir= cache_dir, working_dir=working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc41c627-4c58-4d42-b6ab-4621d58d62f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RID of source dataset, if any.\n",
    "datasets = [\n",
    "            '4-N9XE',\n",
    "            '4-NAPT',\n",
    "            '4-NBG6',\n",
    "            '4-NC9J',\n",
    "            '4-ND2Y', # 200\n",
    "            '4-NDWY',\n",
    "            '4-NFVT',\n",
    "            '4-NHTP',\n",
    "            '4-NKSJ',\n",
    "            '4-NNRE', # 500\n",
    "            '4-NQQY',\n",
    "            '4-NVNA',\n",
    "            '4-NZJP', # 1000\n",
    "            '2-277G',\n",
    "           ]\n",
    "\n",
    "to_be_download = []\n",
    "for dataset in datasets:\n",
    "    ds_dict = {\n",
    "        'rid': dataset,\n",
    "        'materialize':True,\n",
    "        'version':EA.dataset_version(dataset_rid=dataset),\n",
    "    }\n",
    "    to_be_download.append(ds_dict)\n",
    "\n",
    "\n",
    "# EA.add_term(vc.workflow_type, \"Create Dataset Workflow\", description=\"A workflow to test creating a new dataset in eyeAI\")\n",
    "# Workflow instance\n",
    "workflow_instance = EA.create_workflow(\n",
    "    name=\"Dataset splitter creation\",\n",
    "    workflow_type=\"Create Dataset Workflow\"\n",
    ")\n",
    "# Configuration instance.\n",
    "\n",
    "# Set to False if you only need the metadata from the bag, and not the assets.\n",
    "download_assets = False\n",
    "\n",
    "config = ExecutionConfiguration(\n",
    "    # Comment out the following line if you don't need the assets.\n",
    "    datasets=to_be_download,\n",
    "    workflow=workflow_instance,\n",
    "    description=\"Splitting the original dataset.\")\n",
    "\n",
    "# Initialize execution\n",
    "execution = EA.create_execution(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78735081-26e5-4be9-a23c-66a2ad89807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5cb25f-bafa-4b69-b5df-c203d0bbf2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_bag_list = [execution.datasets[i] for i in range(13)]\n",
    "ds_bag_train = execution.datasets[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9888f32b-7d63-4439-bb4d-453b98c18fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_2_df =  EA.filter_angle_2(ds_bag_train)\n",
    "angle_2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf63a8e-c532-4236-aa3e-0ba286707953",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_diag = ds_bag_train.get_table_as_dataframe('Image_Diagnosis')\n",
    "image_diag = image_diag[image_diag['Diagnosis_Tag'] == 'Initial Diagnosis']\n",
    "image_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb7ae9a-d1c4-4486-b03a-d1b7e2815f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_excluded_df = pd.read_csv(\"~/train_no_optic_disc_image_ids.csv\")\n",
    "train_excluded = train_excluded_df[\"ID\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb758b-b6c4-4278-9e80-a26bd55b29ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(angle_2_df, image_diag, left_on='RID', right_on='Image', how='inner')\n",
    "df_filtered = merged_df[['Filename', 'Diagnosis_Image' ,'RID_x']]\n",
    "df_filtered = df_filtered.rename(columns={'RID_x': 'RID'})\n",
    "df_filtered = df_filtered[~df_filtered[\"RID\"].isin(train_excluded)]\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8001136f-3092-4432-a408-d10c8f6646f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = len(df_filtered['Diagnosis_Image'])\n",
    "print(f\"Total values in Diagnosis_Image: {total_count}\")\n",
    "\n",
    "# Count occurrences of each unique value\n",
    "value_counts = df_filtered['Diagnosis_Image'].value_counts()\n",
    "print(\"\\nCounts of each unique value in Diagnosis_Image:\")\n",
    "print(value_counts)\n",
    "\n",
    "diagnosis_values = df_filtered['Diagnosis_Image'].unique()\n",
    "print(\"Values in Diagnosis_Image column:\")\n",
    "print(diagnosis_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e001602-53d0-4bdb-904a-550e700b4994",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baba99e5-db78-4429-bbe3-b911662d726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rids_in_other_ds = {200: [], 500: [], 1000: []}\n",
    "\n",
    "for ds_bag_item in ds_bag_list:\n",
    "    images = ds_bag_item.get_table_as_dataframe(\"Image\")[\"RID\"].tolist()\n",
    "    key = len(images) /2 \n",
    "    rid_list = rids_in_other_ds.get(key, [])\n",
    "    rid_list.extend(images)\n",
    "    rids_in_other_ds[key] = rid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce0ab7-f210-4ca7-a950-473584f1598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, copy\n",
    "\n",
    "\n",
    "def range_split(images):\n",
    "    num_of_split = 0\n",
    "    if images < 100:\n",
    "        num_of_split = 10\n",
    "    elif images >= 100 and images < 1000:\n",
    "        num_of_split = 3\n",
    "    elif images >= 1000 and images < 3000:\n",
    "        num_of_split = 2\n",
    "    else:\n",
    "        num_of_split = 1\n",
    "    return num_of_split\n",
    "\n",
    "def split_dataset(df):\n",
    "    # num_image_split = [10, 200, 500, 1000, 2000, 3000]\n",
    "    num_image_split = [200, 500, 1000]\n",
    "    res = {}\n",
    "\n",
    "    suspected_glaucoma = df[df['Diagnosis_Image'] == 'Suspected Glaucoma']['RID'].tolist()\n",
    "    no_glaucoma = df[df['Diagnosis_Image'] == 'No Glaucoma']['RID'].tolist()\n",
    "\n",
    "    suspected_glaucoma = [rid for rid in suspected_glaucoma if rid not in train_excluded]\n",
    "    no_glaucoma = [rid for rid in no_glaucoma if rid not in train_excluded]\n",
    "        \n",
    "    \n",
    "    for num_images in num_image_split:\n",
    "        current_subset_sets = []\n",
    "        num_split = range_split(num_images)\n",
    "        \n",
    "        random.shuffle(suspected_glaucoma)\n",
    "        random.shuffle(no_glaucoma)\n",
    "\n",
    "          \n",
    "        curr_suspected_glaucoma_rids = suspected_glaucoma\n",
    "        curr_no_glaucoma_rids = no_glaucoma\n",
    "\n",
    "        curr_suspected_glaucoma_rids = [rid for rid in curr_suspected_glaucoma_rids if rid not in rids_in_other_ds[num_images]]\n",
    "        curr_no_glaucoma_rids = [rid for rid in curr_no_glaucoma_rids if rid not in rids_in_other_ds[num_images]]\n",
    "        \n",
    "        print(len(curr_suspected_glaucoma_rids))\n",
    "        print(len(curr_no_glaucoma_rids))\n",
    "        print(\"wat\")\n",
    "        for _ in range(num_split):\n",
    "            if len(curr_suspected_glaucoma_rids) < num_images or len(curr_no_glaucoma_rids) < num_images:\n",
    "                curr_suspected_glaucoma_rids = suspected_glaucoma\n",
    "                curr_no_glaucoma_rids = no_glaucoma\n",
    "                random.shuffle(curr_suspected_glaucoma_rids)\n",
    "                random.shuffle(curr_no_glaucoma_rids)\n",
    "            subset_suspected_glaucoma = curr_suspected_glaucoma_rids[:num_images]\n",
    "            subset_no_glaucoma =  curr_no_glaucoma_rids[:num_images]\n",
    "            concat = subset_suspected_glaucoma+subset_no_glaucoma\n",
    "            current_subset_sets.append(concat)\n",
    "            curr_suspected_glaucoma_rids =  curr_suspected_glaucoma_rids[num_images:]\n",
    "            curr_no_glaucoma_rids =  curr_no_glaucoma_rids[num_images:]\n",
    "                \n",
    "        res[num_images] = current_subset_sets\n",
    "    return res \n",
    "\n",
    "sets = split_dataset(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2e9f13-3e35-47a6-ad65-2a1f8bceb584",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in sets.items():\n",
    "    flattened = [item for sublist in value for item in sublist]\n",
    "    all_unique = len(flattened) == len(set(flattened))\n",
    "    print(\"Unique?\", all_unique)\n",
    "    print(key)\n",
    "    print(len(value))\n",
    "    print(\"Length dataset\")\n",
    "    for v in value:\n",
    "        print(len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83fc4a8-4a6a-402d-aaed-8856267d5451",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    200: [],\n",
    "    500: [],\n",
    "    1000: [],\n",
    "}\n",
    "\n",
    "for ds_bag in ds_bag_list:\n",
    "    df_image_len = int(len(ds_bag.get_table_as_dataframe('Image')['RID'].tolist())/2)\n",
    "    if df_image_len in data:\n",
    "        dataset_list = data[df_image_len]\n",
    "    dataset_list.append(ds_bag)\n",
    "    data[df_image_len] = dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d59ba3-dd7d-4a1b-8487-10cb321a8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_bag_master = {200 : '4-N9X6', 500: '4-NDWP', 1000: '4-NQQP'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab56158-aed4-4a20-b1c7-15a1ac67ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "with execution.execute() as exec:\n",
    "    for key, value in sets.items():\n",
    "        master_dataset = ds_bag_master[key]\n",
    "        training_sets = []\n",
    "        val = 6 if key == 200 or key == 500 else 4\n",
    "        for i, item in enumerate(value, start=val):\n",
    "            training_dataset = execution.create_dataset(['LAC', 'Training'], description=f'A training dataset of {key} images for each diagnosis, No {i}')\n",
    "            EA.add_dataset_members(dataset_rid=training_dataset, members=item)\n",
    "            training_sets.append(training_dataset)\n",
    "        EA.add_dataset_members(dataset_rid=master_dataset, members=training_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe72ef3-b448-4a75-a836-79d3743c43ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.upload_execution_outputs(clean_folder=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
