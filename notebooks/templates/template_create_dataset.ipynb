{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003248d9-3365-4aaa-874a-858234cb2f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo_dir = \"Repos\"   # Set this to be where your github repos are located.\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# # Update the load path so python can find modules for the model\n",
    "# import sys\n",
    "# from pathlib import Path\n",
    "# sys.path.insert(0, str(Path.home() / repo_dir / \"eye-ai-ml\"))\n",
    "# sys.path.insert(0, str(Path.home() / repo_dir / \"deriva-ml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8499060d-4de0-4ab1-bf23-8f766d24dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "from eye_ai.eye_ai import EyeAI\n",
    "from deriva_ml import DatasetBag, Workflow, ExecutionConfiguration\n",
    "from deriva_ml import MLVocab as vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f254b11-9f01-4185-95a0-cd7f80995c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login\n",
    "from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\n",
    "host = 'www.eye-ai.org'\n",
    "catalog_id = \"eye-ai\"\n",
    "\n",
    "gnl = GlobusNativeLogin(host=host)\n",
    "if gnl.is_logged_in([host]):\n",
    "    print(\"You are already logged in.\")\n",
    "else:\n",
    "    gnl.login([host], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n",
    "    print(\"Login Successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a40db10-6f64-4b6e-a22d-3c7011e2a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = '/data'\n",
    "working_dir = 'data'\n",
    "EA = EyeAI(hostname = host, catalog_id = catalog_id, cache_dir= cache_dir, working_dir=working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2006ce9-6ca1-402d-b9ad-8853b85509cf",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "Set up an execution for dataset creation.  Note that this configuration will download all of the assets associated\n",
    "with the dataset.  If you only need the metadata,then set download_assets to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f92f4b0-438a-4e57-9edf-76d7f655f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RID of source dataset, if any.\n",
    "source_dataset = '2-7K8W'\n",
    "\n",
    "EA.add_term(vc.workflow_type, \"Test Workflow\", description=\"A test Workflow for new DM\")\n",
    "# Workflow instance\n",
    "test_workflow = Workflow(\n",
    "    name=\"Dataset creation template\",\n",
    "    url=\"https://github.com/informatics-isi-edu/eye-ai-exec/blob/main/notebooks/templates/template_dataset.ipynb\",\n",
    "    workflow_type=\"Test Workflow\"\n",
    ")\n",
    "# Configuration instance.\n",
    "\n",
    "# Set to False if you only need the metadata from the bag, and not the assets.\n",
    "download_assets = True\n",
    "\n",
    "config = ExecutionConfiguration(\n",
    "    # Comment out the following line if you don't need the assets.\n",
    "    datasets=[source_dataset] if download_assets else [],\n",
    "    workflow=test_workflow,\n",
    "    description=\"Template instance of a dataset partitioned workflow\")\n",
    "\n",
    "# Initialize execution\n",
    "execution = EA.create_execution(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc8f4b-2c6a-4945-ba90-9b3629789122",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(execution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97e75f7-8eb7-490c-8ba7-0e00d738a723",
   "metadata": {},
   "source": [
    "# Create DatasetBag\n",
    "\n",
    "All of the bags in the execution spec are automatically downloaded, so we just need to get the path where they are located from the\n",
    "execution configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64273618-87e3-40ed-8e39-a8d176728fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_bag = DatasetBag(execution.bag_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4e6e383c9bfce1",
   "metadata": {},
   "source": [
    "Now that we have a handle to the downloaded dataset, lets get the list of subjects in the dataset, so we can subset them to\n",
    "make a new dataset.  Once we have done that, we can compute whatever subset we want.\n",
    "\n",
    "If you don't want subjects, just generate the list of RIDs of whatever objects you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f5062f-00e4-4c9f-837d-845bb238fddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_df = ds_bag.get_table_as_dataframe('Subject')\n",
    "\n",
    "# Add code to select which subjects you want to include in this dataset.  The result should\n",
    "# be a list of Subject RIDs.\n",
    "subject_rids = subject_df.RID.tolist()\n",
    "training_rids = subject_rids[0:2] #slice the dataset and extract a list of subject rid\n",
    "test_rids = subject_rids[2:4]\n",
    "validation_rids = subject_rids[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536e4938-1092-4c27-b973-a43f6f28b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2505089f-a9d0-4891-87a3-9ba93c96a76b",
   "metadata": {},
   "source": [
    "# Create dataset\n",
    "\n",
    "We will create a dataset for each of the partitions, and one dataset to represent the complete set of data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43d780-57f9-4e5f-b6ed-6b9697b80f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_dataset = execution.create_dataset(['LAC'], description='A multimodal training dataset with partioning')\n",
    "training_dataset = execution.create_dataset(['LAC', 'Training'], description='A multimodal training dataset')\n",
    "test_dataset = execution.create_dataset(['LAC', 'Testing'], description='A multimodal test dataset')\n",
    "validation_dataset = execution.create_dataset(['LAC', 'Validation'], description='A multimodal validation dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b1471-2986-4fb6-82f1-7d2698ef52a4",
   "metadata": {},
   "source": [
    "# Add subjects into the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc452c2-4dac-47ed-83a4-6d0eea736bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EA.add_dataset_members(dataset_rid=training_dataset, members=training_rids)\n",
    "EA.add_dataset_members(dataset_rid=test_dataset, members=test_rids)\n",
    "EA.add_dataset_members(dataset_rid=validation_dataset, members=validation_rids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d71e38-2ca1-4cd7-b2eb-e8dde8758147",
   "metadata": {},
   "source": [
    "# Add subdatasets to a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f93f00-9db4-4687-b399-17197152a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EA.add_dataset_members(dataset_rid=partitioned_dataset, members= [training_dataset, test_dataset, validation_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3588de-cd3a-4c87-993b-cc0e5025a91f",
   "metadata": {},
   "source": [
    "# Upload results\n",
    "\n",
    "The datasets have already been uploaded to the catalog.  However, we want to record any metadata about the execution, hence we need to do this last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27d4849-0230-4722-afa5-47f03a9d90a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload assets to catalog\n",
    "execution.upload_execution_outputs(clean_folder=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
