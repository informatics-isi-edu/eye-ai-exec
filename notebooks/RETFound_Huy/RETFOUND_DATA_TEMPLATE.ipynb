{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ddd510-eaff-427f-b208-c900f3d00c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = \"Repos\"   # Set this to be where your github repos are located.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Update the load path so python can find modules for the model\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.home() / repo_dir / \"eye-ai-ml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80accef7-a8f3-4484-9fff-5f0e4ecd4c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 12:41:03.280586: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-26 12:41:03.280649: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-26 12:41:03.281747: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-26 12:41:03.288309: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-26 12:41:04.041158: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Prerequisites\n",
    "import json\n",
    "import os\n",
    "from eye_ai.eye_ai import EyeAI\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path, PurePath\n",
    "import logging\n",
    "\n",
    "from deriva_ml import DatasetBag, Workflow, ExecutionConfiguration, DatasetVersion\n",
    "from deriva_ml import MLVocab as vc\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a75e3e-e05f-4030-8fc6-eeed1ad964bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 12:41:04,636 - INFO - Creating client of type <class 'globus_sdk.services.auth.client.native_client.NativeAppAuthClient'> for service \"auth\"\n",
      "2025-02-26 12:41:04,637 - INFO - Finished initializing AuthLoginClient. client_id='8ef15ba9-2b4a-469c-a163-7fd910c9d111', type(authorizer)=<class 'globus_sdk.authorizers.base.NullAuthorizer'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are already logged in.\n"
     ]
    }
   ],
   "source": [
    "from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\n",
    "catalog_id = \"eye-ai\" #@param\n",
    "host = 'www.eye-ai.org'\n",
    "\n",
    "\n",
    "gnl = GlobusNativeLogin(host=host)\n",
    "if gnl.is_logged_in([host]):\n",
    "    print(\"You are already logged in.\")\n",
    "else:\n",
    "    gnl.login([host], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n",
    "    print(\"Login Successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8f047a4-e322-4dfe-8619-d46a44b0dde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 12:41:04,670 - INFO - Creating client of type <class 'globus_sdk.services.auth.client.native_client.NativeAppAuthClient'> for service \"auth\"\n",
      "2025-02-26 12:41:04,671 - INFO - Finished initializing AuthLoginClient. client_id='8ef15ba9-2b4a-469c-a163-7fd910c9d111', type(authorizer)=<class 'globus_sdk.authorizers.base.NullAuthorizer'>\n"
     ]
    }
   ],
   "source": [
    "cache_dir = '/data'\n",
    "working_dir = '/data'\n",
    "EA = EyeAI(hostname = host, catalog_id = catalog_id, cache_dir= cache_dir, working_dir=working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7c8d7de-c35d-4727-830c-641bb47f2699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 12:41:06,458 - INFO - Creating new database for dataset: 4-N9MC in /data/4-N9MC_3a958d562904149745d679fc533c7d7f1eda849a11244d72d6b1476153412a36/4-N9MC@32X-6NER-5J88.db\n",
      "2025-02-26 12:41:07,781 - INFO - Creating new database for dataset: 4-N9P0 in /data/4-N9P0_9510a3536d45eb432f075e7a70cf2c757a5ad96fcee43bdf19fc1c5ec1049012/4-N9P0@32X-6NGV-6DJJ.db\n",
      "2025-02-26 12:41:09,201 - INFO - Creating new database for dataset: 4-N9QM in /data/4-N9QM_a8c43ceda04c4f42198307eb347593e159e1612fdc6be140e87f78f1f601dea5/4-N9QM@32X-6NJX-5QHJ.db\n",
      "2025-02-26 12:41:10,338 - INFO - Creating new database for dataset: 4-N9S8 in /data/4-N9S8_2a1b3542a69cb8cc1e40465d475d8d6d3a28b048d3d0d747cce4f882ba66a91d/4-N9S8@32X-6NMX-PQ3T.db\n",
      "2025-02-26 12:41:11,540 - INFO - Creating new database for dataset: 4-N9TW in /data/4-N9TW_8d598b20f43acd09ad804b7e88c140456b5b6d9e98bd1bdce2915b25a8a78024/4-N9TW@32X-6NPV-F5MP.db\n",
      "2025-02-26 12:41:13,296 - INFO - Creating new database for dataset: 2-39FY in /data/2-39FY_5dba6cfaee43c5f88c77be50d97ffa1d094dbb86d6d338020568508d136aa53b/2-39FY@32V-RGZ6-9R7J.db\n",
      "2025-02-26 12:41:15,311 - INFO - Creating new database for dataset: 2-277M in /data/2-277M_29af9e571893617f5654e2b4000c6d06ef7707d3dc50a7d61a23a3d17e53a808/2-277M@32V-ZZJ5-FPW0.db\n"
     ]
    }
   ],
   "source": [
    "# RID of source dataset, if any.\n",
    "# RID of source dataset, if any.\n",
    "datasets = [\n",
    "                # '4-N9C8', \n",
    "                #   '4-N9DW', \n",
    "                #   '4-N9FG', \n",
    "                #   '4-N9H4', \n",
    "                #   '4-N9JR', \n",
    "                  '4-N9MC', \n",
    "                  '4-N9P0', \n",
    "                  '4-N9QM', \n",
    "                  '4-N9S8', \n",
    "                  '4-N9TW', \n",
    "                  '2-39FY', \n",
    "                  '2-277M']\n",
    "\n",
    "to_be_download = []\n",
    "for dataset in datasets:\n",
    "    ds_dict = {\n",
    "        'rid': dataset,\n",
    "        'materialize':True,\n",
    "        'version': DatasetVersion.parse('0.1.0'),\n",
    "    }\n",
    "    to_be_download.append(ds_dict)\n",
    "# EA.add_term(vc.workflow_type, \"RETFound Model Train\", description=\"A workflow to train RETFound model\")\n",
    "# Workflow instance\n",
    "workflow_instance = Workflow(\n",
    "    name=\"RETFound Model train - 10 images\",\n",
    "    url=\"https://github.com/informatics-isi-edu/eye-ai-exec/blob/main/notebooks/RETFound_Huy/RETFound_TRAIN_10.ipynb\",\n",
    "    workflow_type=\"RETFound Model Train\",\n",
    ")\n",
    "# Configuration instance.\n",
    "\n",
    "# Set to False if you only need the metadata from the bag, and not the assets.\n",
    "download_assets = True\n",
    "\n",
    "config = ExecutionConfiguration(\n",
    "    # Comment out the following line if you don't need the assets.\n",
    "    # datasets=[source_dataset] if download_assets else [],\n",
    "    datasets=to_be_download,\n",
    "    workflow=workflow_instance,\n",
    "    description=\"Instance of training RETFound model - 10 images\")\n",
    "\n",
    "# Initialize execution\n",
    "execution = EA.create_execution(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "989a24dc-132c-45aa-a197-cb887096dcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caching_dir: /data\n",
      "_working_dir: /data/nguyent8/EyeAI_working\n",
      "execution_rid: 4-Q7AW\n",
      "workflow_rid: 4-M516\n",
      "asset_paths: []\n",
      "configuration: datasets=[DatasetSpec(rid='4-N9MC', materialize=True, version=DatasetVersion(major=0, minor=1, patch=0)), DatasetSpec(rid='4-N9P0', materialize=True, version=DatasetVersion(major=0, minor=1, patch=0)), DatasetSpec(rid='4-N9QM', materialize=True, version=DatasetVersion(major=0, minor=1, patch=0)), DatasetSpec(rid='4-N9S8', materialize=True, version=DatasetVersion(major=0, minor=1, patch=0)), DatasetSpec(rid='4-N9TW', materialize=True, version=DatasetVersion(major=0, minor=1, patch=0)), DatasetSpec(rid='2-39FY', materialize=True, version=DatasetVersion(major=0, minor=1, patch=0)), DatasetSpec(rid='2-277M', materialize=True, version=DatasetVersion(major=0, minor=1, patch=0))] assets=[] workflow=Workflow(name='RETFound Model train - 10 images', url='https://github.com/informatics-isi-edu/eye-ai-exec/blob/main/notebooks/RETFound_Huy/RETFound_TRAIN_10.ipynb', workflow_type='RETFound Model Train', version=None, description=None) description='Instance of training RETFound model - 10 images'\n"
     ]
    }
   ],
   "source": [
    "print(execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "166d6baf-eebf-4190-9541-667ea593c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_bag_0 = execution.datasets[0]\n",
    "ds_bag_1 = execution.datasets[1]\n",
    "ds_bag_2 = execution.datasets[2]\n",
    "ds_bag_3 = execution.datasets[3]\n",
    "ds_bag_4 = execution.datasets[4]\n",
    "# ds_bag_5 = execution.datasets[5]\n",
    "# ds_bag_6 = execution.datasets[6]\n",
    "# ds_bag_7 = execution.datasets[7]\n",
    "# ds_bag_8 = execution.datasets[8]\n",
    "# ds_bag_9 = execution.datasets[9]\n",
    "\n",
    "# ds_bag_val = execution.datasets[10]\n",
    "# ds_bag_test = execution.datasets[11]\n",
    "\n",
    "ds_bag_val = execution.datasets[5]\n",
    "ds_bag_test = execution.datasets[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab2c01db-fd88-493a-8045-cce6c25878ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_bag_list = [ds_bag_0, ds_bag_1, ds_bag_2, ds_bag_3, ds_bag_4,]\n",
    "               # ds_bag_5, ds_bag_6, ds_bag_7, ds_bag_8, ds_bag_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c455354f-158c-4cc9-ac8f-77e19b6b0e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_excluded_df = pd.read_csv(\"valid_no_optic_disc_image_ids.csv\")\n",
    "val_excluded = val_excluded_df[\"ID\"].tolist()\n",
    "\n",
    "train_excluded_df = pd.read_csv(\"train_no_optic_disc_image_ids.csv\")\n",
    "train_excluded = train_excluded_df[\"ID\"].tolist()\n",
    "\n",
    "test_included_df = pd.read_csv(\"Graded_Test_Dataset_2-277M_With_Demographics_CDR_Diagnosis_Image_Quality_Model_Diagnosis_Predicitons_with_Jiun_Do_June8_2024_with_Catalog_model_predictions.csv\")\n",
    "test_included = test_included_df[\"Image_cd\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5864968c-d6b8-4f48-8c13-a0b1b73e3354",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = execution._working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8aa59ee3-2190-407f-902f-f05bdbaba431",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_image_path_cropped, validation_csv_cropped = EA.create_cropped_images(ds_bag_val,\n",
    "                                                                                 output_dir = output_dir ,\n",
    "                                                                                 crop_to_eye=True,\n",
    "                                                                                exclude_list= val_excluded)\n",
    "\n",
    "validation_image_path_uncropped, validation_csv_uncropped = EA.create_cropped_images(ds_bag_val,\n",
    "                                                                                 output_dir = output_dir,\n",
    "                                                                                 crop_to_eye=False,\n",
    "                                                                                    exclude_list= val_excluded)\n",
    "\n",
    "test_image_path_cropped, test_csv_cropped = EA.create_cropped_images(ds_bag_test,\n",
    "                                                                     output_dir = output_dir,\n",
    "                                                                     crop_to_eye=True,\n",
    "                                                                     include_only_list= test_included)\n",
    "\n",
    "test_image_path_uncropped, test_csv_uncropped = EA.create_cropped_images(ds_bag_test,\n",
    "                                                                         output_dir = output_dir ,\n",
    "                                                                         crop_to_eye=False,\n",
    "                                                                         include_only_list = test_included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ce965ff-43ff-43f8-a028-514abb0f869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyper_parameters_json_path = \"best_hyperparameters_exluding_no_optic_disc_images_june_24_2024.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dd9d170-cdf5-45d6-b147-9a395c24d344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best_hyperparameters_exluding_no_optic_disc_images_june_24_2024.json'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyper_parameters_json_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06398dda-6e4c-4947-b26d-58e8169dd875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crete asset path\n",
    "asset_path_models = execution.execution_asset_path(\"Diagnosis_Model\")\n",
    "asset_path_output = execution.execution_asset_path(\"Model_Prediction\")\n",
    "asset_path_logs = execution.execution_asset_path(\"Training_Log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb315b51-bd13-4d76-b558-b2e73fe918b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/data/nguyent8/EyeAI_working/deriva-ml/execution/4-Q7AW/execution-asset/Diagnosis_Model')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asset_path_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b3103b8-632f-4cc6-9ea7-d26f701ba38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/data/nguyent8/EyeAI_working/deriva-ml/execution/4-Q7AW/execution-asset/Model_Prediction')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asset_path_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2ea95a4-6174-4b04-a34c-17765e1f0c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/data/nguyent8/EyeAI_working/deriva-ml/execution/4-Q7AW/execution-asset/Training_Log')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asset_path_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b22a8b0b-257f-4a0f-a4d4-f9760fb8a4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/data/nguyent8/EyeAI_working')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "822843f4-127a-41a8-8490-d800215d0b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def create_retfound_ds(output, train_dir, val_dir, test_dir, ds_bag_name, crop):\n",
    "    ds_bag_out_path = os.path.join(output, f\"{ds_bag_name}_RETFound\") if not crop else  os.path.join(output, f\"{ds_bag_name}_RETFound_cropped\") \n",
    "    os.makedirs(ds_bag_out_path, exist_ok= True)\n",
    "    \n",
    "    for subdir in ['train', 'val', 'test']:\n",
    "        subdir_path = os.path.join(ds_bag_out_path, subdir)\n",
    "        os.makedirs(subdir_path, exist_ok= True)\n",
    "        \n",
    "\n",
    "    dirs = [(train_dir, 'train'), (val_dir, 'val'), (test_dir, 'test')]\n",
    "    \n",
    "    for source_dir, subdir in dirs:\n",
    "        for class_dir in os.listdir(source_dir):\n",
    "            class_path = os.path.join(source_dir, class_dir)\n",
    "            target_class_dir = os.path.join(ds_bag_out_path, subdir, class_dir)\n",
    "            os.makedirs(target_class_dir, exist_ok= True)\n",
    "            for file_name in os.listdir(class_path):\n",
    "                source_file = os.path.join(class_path, file_name)\n",
    "                target_file = os.path.join(target_class_dir, file_name)\n",
    "                shutil.copy(source_file, target_file)\n",
    "    return ds_bag_out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e4488f-ab91-4dba-b10d-5abd3cdd85db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  4-N9MC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 12:46:33.413384: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-26 12:46:33.413431: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-26 12:46:33.414517: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-26 12:46:33.420679: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-26 12:46:34.125907: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| distributed init (rank 0): env://, gpu 0\n",
      "[12:46:38.108966] job dir: /home/nguyent8/Repos/RETFound_MAE\n",
      "[12:46:38.109091] Namespace(batch_size=16,\n",
      "epochs=50,\n",
      "accum_iter=1,\n",
      "model='vit_large_patch16',\n",
      "input_size=224,\n",
      "drop_path=0.2,\n",
      "clip_grad=None,\n",
      "weight_decay=0.05,\n",
      "lr=None,\n",
      "blr=0.005,\n",
      "layer_decay=0.65,\n",
      "min_lr=1e-06,\n",
      "warmup_epochs=10,\n",
      "color_jitter=None,\n",
      "aa='rand-m9-mstd0.5-inc1',\n",
      "smoothing=0.1,\n",
      "reprob=0.25,\n",
      "remode='pixel',\n",
      "recount=1,\n",
      "resplit=False,\n",
      "mixup=0,\n",
      "cutmix=0,\n",
      "cutmix_minmax=None,\n",
      "mixup_prob=1.0,\n",
      "mixup_switch_prob=0.5,\n",
      "mixup_mode='batch',\n",
      "finetune='RETFound_cfp_weights.pth',\n",
      "task='/data/nguyent8/EyeAI_working/4-N9MC/RETFound_task/Uncropped_',\n",
      "global_pool=True,\n",
      "data_path='/data/nguyent8/EyeAI_working/4-N9MC_RETFound',\n",
      "nb_classes=5,\n",
      "output_dir='./output_dir',\n",
      "log_dir='./output_dir',\n",
      "device='cuda',\n",
      "seed=0,\n",
      "resume='',\n",
      "start_epoch=0,\n",
      "eval=False,\n",
      "dist_eval=False,\n",
      "num_workers=10,\n",
      "pin_mem=True,\n",
      "world_size=1,\n",
      "local_rank=-1,\n",
      "dist_on_itp=False,\n",
      "dist_url='env://',\n",
      "rank=0,\n",
      "gpu=0,\n",
      "distributed=True,\n",
      "dist_backend='nccl')\n",
      "[12:46:38.122618] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f6a7f0ddc90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:47:19.122205] Load pre-trained checkpoint from: RETFound_cfp_weights.pth\n",
      "[12:47:19.216151] _IncompatibleKeys(missing_keys=['head.weight', 'head.bias', 'fc_norm.weight', 'fc_norm.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])\n",
      "[12:47:19.343647] Model = VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1-23): 23 x Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): Linear(in_features=1024, out_features=5, bias=True)\n",
      "  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      ")\n",
      "[12:47:19.343700] number of params (M): 303.31\n",
      "[12:47:19.343723] base lr: 5.00e-03\n",
      "[12:47:19.343736] actual lr: 3.13e-04\n",
      "[12:47:19.343748] accumulate grad iterations: 1\n",
      "[12:47:19.343759] effective batch size: 16\n",
      "[12:47:19.400199] criterion = LabelSmoothingCrossEntropy()\n",
      "[12:47:19.400247] Start training for 50 epochs\n",
      "[12:47:19.401588] log_dir: ./output_dir/data/nguyent8/EyeAI_working/4-N9MC/RETFound_task/Uncropped_\n",
      "[12:47:23.240190] Epoch: [0]  [0/1]  eta: 0:00:03  lr: 0.000000  loss: 1.6094 (1.6094)  time: 3.8342  data: 2.2103  max mem: 7028\n",
      "[12:47:23.458668] Epoch: [0] Total time: 0:00:04 (4.0570 s / it)\n",
      "[12:47:23.459343] Averaged stats: lr: 0.000000  loss: 1.6094 (1.6094)\n",
      "[12:47:33.752116] Test:  [ 0/54]  eta: 0:09:15  loss: 1.6094 (1.6094)  acc1: 0.0000 (0.0000)  time: 10.2861  data: 9.9524  max mem: 7028\n",
      "[12:47:45.632546] Test:  [10/54]  eta: 0:01:28  loss: 1.6094 (1.6094)  acc1: 0.0000 (0.0000)  time: 2.0151  data: 1.9154  max mem: 7028\n",
      "[12:47:55.159178] Test:  [20/54]  eta: 0:00:51  loss: 1.6094 (1.6094)  acc1: 0.0000 (0.0000)  time: 1.0703  data: 0.9934  max mem: 7028\n",
      "[12:48:05.202388] Test:  [30/54]  eta: 0:00:32  loss: 1.6094 (1.6094)  acc1: 0.0000 (2.8226)  time: 0.9784  data: 0.9004  max mem: 7028\n",
      "[12:48:12.978404] Test:  [40/54]  eta: 0:00:16  loss: 1.6094 (1.6094)  acc1: 18.7500 (8.2317)  time: 0.8909  data: 0.8161  max mem: 7028\n",
      "[12:48:16.791509] Test:  [50/54]  eta: 0:00:04  loss: 1.6093 (1.6093)  acc1: 25.0000 (11.1520)  time: 0.5794  data: 0.5098  max mem: 7028\n",
      "[12:48:17.064540] Test:  [53/54]  eta: 0:00:00  loss: 1.6093 (1.6093)  acc1: 25.0000 (11.8467)  time: 0.5822  data: 0.5097  max mem: 7028\n",
      "[12:48:17.304583] Test: Total time: 0:00:53 (0.9970 s / it)\n",
      "[12:48:17.310326] Unique classes in true labels: [0 1]\n",
      "[12:48:17.310462] ðŸ”¹ true_label_onehot_list shape: (861, 5)\n",
      "[12:48:17.310480] ðŸ”¹ prediction_list shape: (861, 5)\n",
      "[12:48:17.320769] âœ… Data saved to: /data/nguyent8/EyeAI_working/4-N9MC/RETFound_task/Uncropped_/roc_data.csv\n",
      "[12:48:17.321434] Sklearn Metrics - Acc: 0.6821 AUC-roc: 0.4349 AUC-pr: 0.4620 F1-score: 0.0777 MCC: -0.0119\n",
      "[12:48:18.686107] log_dir: ./output_dir/data/nguyent8/EyeAI_working/4-N9MC/RETFound_task/Uncropped_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:48:20.979611] Epoch: [1]  [0/1]  eta: 0:00:02  lr: 0.000031  loss: 1.6094 (1.6094)  time: 2.2927  data: 2.0558  max mem: 7031\n",
      "[12:48:21.228229] Epoch: [1] Total time: 0:00:02 (2.5420 s / it)\n",
      "[12:48:21.229023] Averaged stats: lr: 0.000031  loss: 1.6094 (1.6094)\n",
      "[12:48:29.011921] Test:  [ 0/54]  eta: 0:07:00  loss: 1.6094 (1.6094)  acc1: 0.0000 (0.0000)  time: 7.7800  data: 7.7027  max mem: 7031\n",
      "[12:48:41.333883] Test:  [10/54]  eta: 0:01:20  loss: 1.6094 (1.6094)  acc1: 0.0000 (0.0000)  time: 1.8251  data: 1.7480  max mem: 7031\n",
      "[12:48:50.734376] Test:  [20/54]  eta: 0:00:47  loss: 1.6094 (1.6094)  acc1: 0.0000 (0.0000)  time: 1.0848  data: 1.0064  max mem: 7031\n",
      "[12:49:00.475800] Test:  [30/54]  eta: 0:00:30  loss: 1.6094 (1.6094)  acc1: 0.0000 (2.8226)  time: 0.9570  data: 0.8779  max mem: 7031\n",
      "[12:49:08.590545] Test:  [40/54]  eta: 0:00:16  loss: 1.6094 (1.6094)  acc1: 18.7500 (8.2317)  time: 0.8927  data: 0.8135  max mem: 7031\n",
      "[12:49:13.019396] Test:  [50/54]  eta: 0:00:04  loss: 1.6093 (1.6093)  acc1: 25.0000 (11.1520)  time: 0.6271  data: 0.5523  max mem: 7031\n",
      "[12:49:14.035829] Test:  [53/54]  eta: 0:00:00  loss: 1.6093 (1.6093)  acc1: 25.0000 (11.8467)  time: 0.6068  data: 0.5343  max mem: 7031\n",
      "[12:49:14.276498] Test: Total time: 0:00:53 (0.9823 s / it)\n",
      "[12:49:14.279116] Unique classes in true labels: [0 1]\n",
      "[12:49:14.279280] ðŸ”¹ true_label_onehot_list shape: (861, 5)\n",
      "[12:49:14.279307] ðŸ”¹ prediction_list shape: (861, 5)\n",
      "[12:49:14.283570] âœ… Data saved to: /data/nguyent8/EyeAI_working/4-N9MC/RETFound_task/Uncropped_/roc_data.csv\n",
      "[12:49:14.284281] Sklearn Metrics - Acc: 0.6821 AUC-roc: 0.4349 AUC-pr: 0.4620 F1-score: 0.0777 MCC: -0.0119\n",
      "[12:49:14.286619] log_dir: ./output_dir/data/nguyent8/EyeAI_working/4-N9MC/RETFound_task/Uncropped_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:49:16.790588] Epoch: [2]  [0/1]  eta: 0:00:02  lr: 0.000063  loss: 1.6093 (1.6093)  time: 2.5032  data: 2.2785  max mem: 7031\n",
      "[12:49:17.037111] Epoch: [2] Total time: 0:00:02 (2.7504 s / it)\n",
      "[12:49:17.037862] Averaged stats: lr: 0.000063  loss: 1.6093 (1.6093)\n",
      "[12:49:26.299049] Test:  [ 0/54]  eta: 0:08:19  loss: 1.6094 (1.6094)  acc1: 0.0000 (0.0000)  time: 9.2584  data: 9.1731  max mem: 7031\n",
      "[12:49:38.446442] Test:  [10/54]  eta: 0:01:25  loss: 1.6094 (1.6094)  acc1: 0.0000 (0.0000)  time: 1.9459  data: 1.8641  max mem: 7031\n",
      "[12:49:48.094332] Test:  [20/54]  eta: 0:00:50  loss: 1.6094 (1.6094)  acc1: 0.0000 (0.0000)  time: 1.0897  data: 1.0101  max mem: 7031\n",
      "[12:49:57.904515] Test:  [30/54]  eta: 0:00:31  loss: 1.6094 (1.6094)  acc1: 0.0000 (2.8226)  time: 0.9728  data: 0.8962  max mem: 7031\n",
      "[12:50:05.154496] Test:  [40/54]  eta: 0:00:16  loss: 1.6094 (1.6094)  acc1: 18.7500 (8.2317)  time: 0.8529  data: 0.7768  max mem: 7031\n",
      "[12:50:09.191153] Test:  [50/54]  eta: 0:00:04  loss: 1.6093 (1.6093)  acc1: 25.0000 (11.1520)  time: 0.5642  data: 0.4905  max mem: 7031\n",
      "[12:50:09.385578] Test:  [53/54]  eta: 0:00:00  loss: 1.6093 (1.6093)  acc1: 25.0000 (11.8467)  time: 0.5613  data: 0.4897  max mem: 7031\n",
      "[12:50:09.634862] Test: Total time: 0:00:52 (0.9740 s / it)\n",
      "[12:50:09.637324] Unique classes in true labels: [0 1]\n",
      "[12:50:09.637459] ðŸ”¹ true_label_onehot_list shape: (861, 5)\n",
      "[12:50:09.637478] ðŸ”¹ prediction_list shape: (861, 5)\n",
      "[12:50:09.641622] âœ… Data saved to: /data/nguyent8/EyeAI_working/4-N9MC/RETFound_task/Uncropped_/roc_data.csv\n",
      "[12:50:09.642300] Sklearn Metrics - Acc: 0.6821 AUC-roc: 0.4349 AUC-pr: 0.4620 F1-score: 0.0777 MCC: -0.0119\n",
      "[12:50:09.644666] log_dir: ./output_dir/data/nguyent8/EyeAI_working/4-N9MC/RETFound_task/Uncropped_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:50:11.968864] Epoch: [3]  [0/1]  eta: 0:00:02  lr: 0.000094  loss: 1.6094 (1.6094)  time: 2.3233  data: 1.9249  max mem: 7031\n",
      "[12:50:12.223054] Epoch: [3] Total time: 0:00:02 (2.5783 s / it)\n",
      "[12:50:12.223808] Averaged stats: lr: 0.000094  loss: 1.6094 (1.6094)\n",
      "[12:50:22.106683] Test:  [ 0/54]  eta: 0:08:53  loss: 1.5762 (1.5762)  acc1: 100.0000 (100.0000)  time: 9.8800  data: 9.7692  max mem: 7031\n",
      "[12:50:33.813384] Test:  [10/54]  eta: 0:01:26  loss: 1.5746 (1.5746)  acc1: 100.0000 (98.2955)  time: 1.9616  data: 1.8768  max mem: 7031\n",
      "[12:50:43.030809] Test:  [20/54]  eta: 0:00:49  loss: 1.5748 (1.5748)  acc1: 100.0000 (98.5119)  time: 1.0457  data: 0.9641  max mem: 7031\n",
      "[12:50:52.912527] Test:  [30/54]  eta: 0:00:31  loss: 1.5755 (1.5752)  acc1: 100.0000 (84.4758)  time: 0.9549  data: 0.8767  max mem: 7031\n",
      "[12:51:00.678497] Test:  [40/54]  eta: 0:00:16  loss: 1.5767 (1.5757)  acc1: 0.0000 (63.8720)  time: 0.8823  data: 0.8047  max mem: 7031\n",
      "[12:51:04.902567] Test:  [50/54]  eta: 0:00:04  loss: 1.5772 (1.5760)  acc1: 0.0000 (51.5931)  time: 0.5994  data: 0.5239  max mem: 7031\n",
      "[12:51:05.097229] Test:  [53/54]  eta: 0:00:00  loss: 1.5773 (1.5760)  acc1: 0.0000 (48.8966)  time: 0.5967  data: 0.5236  max mem: 7031\n",
      "[12:51:05.342911] Test: Total time: 0:00:53 (0.9836 s / it)\n",
      "[12:51:05.345458] Unique classes in true labels: [0 1]\n",
      "[12:51:05.345604] ðŸ”¹ true_label_onehot_list shape: (861, 5)\n",
      "[12:51:05.345628] ðŸ”¹ prediction_list shape: (861, 5)\n",
      "[12:51:05.349922] âœ… Data saved to: /data/nguyent8/EyeAI_working/4-N9MC/RETFound_task/Uncropped_/roc_data.csv\n",
      "[12:51:05.350617] Sklearn Metrics - Acc: 0.8722 AUC-roc: 0.4896 AUC-pr: 0.4911 F1-score: 0.0034 MCC: -0.0065\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "repo_path = os.path.expanduser(\"~/Repos/RETFound_MAE\")\n",
    "with execution.execute() as exec:\n",
    "    for index, ds_bag in enumerate(ds_bag_list):\n",
    "        image_path_ds_bag_path_cropped, csv_ds_bag_cropped = EA.create_cropped_images(\n",
    "                                                   ds_bag, \n",
    "                                                   output_dir, \n",
    "                                                   crop_to_eye=True,\n",
    "                                                    exclude_list= train_excluded,\n",
    "                                                   )\n",
    "        image_path_ds_bag_path_uncropped, csv_ds_bag_uncropped = EA.create_cropped_images(\n",
    "                                                   ds_bag, \n",
    "                                                   output_dir, \n",
    "                                                   crop_to_eye=False,\n",
    "                                                    exclude_list= train_excluded,\n",
    "                                                 )\n",
    "       \n",
    "        print(\"Dataset: \", ds_bag.dataset_rid)\n",
    "        retfound_ds_bag_path_uncropped= create_retfound_ds(output= output_dir, train_dir = image_path_ds_bag_path_uncropped, val_dir = validation_image_path_uncropped,  test_dir = test_image_path_uncropped, ds_bag_name =ds_bag.dataset_rid, crop = False)\n",
    "        retfound_ds_bag_path_cropped= create_retfound_ds(output= output_dir, train_dir = image_path_ds_bag_path_cropped, val_dir = validation_image_path_cropped,  test_dir = test_image_path_cropped, ds_bag_name =ds_bag.dataset_rid, crop = True)\n",
    "\n",
    "        \n",
    "\n",
    "        retfound_out_uncropped = output_dir / f\"{ds_bag.dataset_rid}/RETFound_task/Uncropped_\"\n",
    "        os.makedirs(retfound_out_uncropped, exist_ok=True)\n",
    "        \n",
    "        retfound_out_cropped = output_dir / f\"{ds_bag.dataset_rid}/RETFound_task/Cropped_\"\n",
    "        os.makedirs(retfound_out_cropped, exist_ok=True)\n",
    "        retfound_output_dirs = [\n",
    "           retfound_out_uncropped,\n",
    "            retfound_out_cropped\n",
    "        ]\n",
    "        \n",
    "        data_paths = [\n",
    "            retfound_ds_bag_path_uncropped,\n",
    "            retfound_ds_bag_path_cropped\n",
    "        ]\n",
    "        \n",
    "        for data_path, retfound_output_dir in zip(data_paths, retfound_output_dirs):\n",
    "            os.makedirs(retfound_output_dir, exist_ok=True)\n",
    "            \n",
    "            command = [\n",
    "                \"torchrun\",\n",
    "                \"--nproc_per_node=1\", \"--master_port=48798\", \"main_finetune.py\",\n",
    "                \"--batch_size\", \"16\",\n",
    "                \"--world_size\", \"1\",\n",
    "                \"--model\", \"vit_large_patch16\",\n",
    "                \"--epochs\", \"50\",\n",
    "                \"--blr\", \"5e-3\", \"--layer_decay\", \"0.65\",\n",
    "                \"--weight_decay\", \"0.05\", \"--drop_path\", \"0.2\",\n",
    "                \"--nb_classes\", \"5\",\n",
    "                \"--data_path\", data_path,\n",
    "                \"--task\", retfound_output_dir,\n",
    "                \"--finetune\", \"RETFound_cfp_weights.pth\",\n",
    "                \"--input_size\", \"224\"\n",
    "            ]\n",
    "                \n",
    "            # Run the command inside the RETFound_MAE repository\n",
    "            subprocess.run(command, check=True, cwd=repo_path)\n",
    "            \n",
    "        for data_path in data_paths:\n",
    "            if os.path.exists(data_path):\n",
    "                shutil.rmtree(data_path)\n",
    "                print(f\"Deleted folder: {data_path}\")\n",
    "            else:\n",
    "                print(f\"Folder does not exist: {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eb022e-dd04-4b43-a9c9-47387c140ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finishedd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917d7c0f-ee12-4b20-af29-1aff6a82b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "for ds_bag in ds_bag_list:\n",
    "    source_dir = output_dir / ds_bag.dataset_rid /  \"RETFound_task\"\n",
    "    # print(source_dir)\n",
    "    for item in os.listdir(source_dir):\n",
    "        item_path = Path(source_dir)  / item\n",
    "        suffix = \"uncropped\" if \"Uncropped\" in item else \"cropped\"\n",
    "\n",
    "        if item.endswith(\".pth\"):\n",
    "            new_file_name = f\"{ds_bag.dataset_rid}_{suffix}.pth\"\n",
    "            destination_path = asset_path_models / new_file_name  \n",
    "            shutil.move(item_path, destination_path)\n",
    "        elif  \"test\" in item and (item.endswith(\".csv\") or item.endswith(\".jpg\")):\n",
    "            new_file_name = f\"{ds_bag.dataset_rid}_{suffix}_metrics_test.csv\" if item.endswith(\".csv\") else f\"{ds_bag.dataset_rid}_{suffix}_conf_matrix.jpg\"\n",
    "            destination_path = asset_path_output / new_file_name \n",
    "            shutil.move(item_path, destination_path)\n",
    "        elif \"val\" in item:\n",
    "              new_file_name = f\"{ds_bag.dataset_rid}_{suffix}_metrics_val.csv\"\n",
    "              destination_path = asset_path_logs / new_file_name  \n",
    "              shutil.move(item_path, destination_path)\n",
    "        elif item_path.is_dir():  \n",
    "              for sub_item in item_path.iterdir():\n",
    "                   if \"roc_\" in sub_item.name:\n",
    "                        new_file_name = f\"{ds_bag.dataset_rid}_{suffix}_{sub_item.name}.csv\"\n",
    "                        destination_path = asset_path_output / new_file_name\n",
    "                        shutil.move(sub_item, destination_path)\n",
    "        \n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5089267-5445-41ae-abfb-0f5120040e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.upload_execution_outputs(clean_folder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02654f3b-adac-4db0-9696-21009a1b129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"omega done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b5b2eb-c208-4dab-a602-796ebe88c341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c941b7e2-5729-4b76-8a72-e94a5e752e78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My TensorFlow (Conda)",
   "language": "python",
   "name": "my-tensorflow-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
