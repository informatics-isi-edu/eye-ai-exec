{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch==2.3.1 in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (2.3.1+cu121)\n",
      "Requirement already satisfied: torchvision==0.18.1 in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (0.18.1+cu121)\n",
      "Requirement already satisfied: torchaudio==2.3.1 in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (2.3.1+cu121)\n",
      "Requirement already satisfied: filelock in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch==2.3.1) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch==2.3.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch==2.3.1) (1.13.1)\n",
      "Requirement already satisfied: networkx in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch==2.3.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch==2.3.1) (3.1.3)\n",
      "Requirement already satisfied: fsspec in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch==2.3.1) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch==2.3.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch==2.3.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch==2.3.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch==2.3.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch==2.3.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch==2.3.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch==2.3.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch==2.3.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch==2.3.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch==2.3.1) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch==2.3.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch==2.3.1) (2.3.1)\n",
      "Requirement already satisfied: numpy in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torchvision==0.18.1) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torchvision==0.18.1) (10.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1) (12.1.105)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from jinja2->torch==2.3.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from sympy->torch==2.3.1) (1.3.0)\n",
      "fatal: destination path 'RETFound_MAE' already exists and is not an empty directory.\n",
      "Requirement already satisfied: opencv-python~=4.9.0.80 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (4.9.0.80)\n",
      "Requirement already satisfied: Pillow~=10.2.0 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (10.2.0)\n",
      "Requirement already satisfied: pycm~=4.0 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (4.2)\n",
      "Requirement already satisfied: scikit-learn~=1.4.2 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: timm~=0.9.2 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.9.16)\n",
      "Requirement already satisfied: numpy~=1.26.4 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.26.4)\n",
      "Requirement already satisfied: matplotlib~=3.8.4 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (3.8.4)\n",
      "Requirement already satisfied: scikit-multilearn~=0.2.0 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: huggingface-hub~=0.23.4 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.23.5)\n",
      "Requirement already satisfied: tensorboard~=2.15.0 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (2.15.2)\n",
      "Requirement already satisfied: art>=1.8 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from pycm~=4.0->-r requirements.txt (line 3)) (6.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from scikit-learn~=1.4.2->-r requirements.txt (line 4)) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from scikit-learn~=1.4.2->-r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from scikit-learn~=1.4.2->-r requirements.txt (line 4)) (3.3.0)\n",
      "Requirement already satisfied: torch in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from timm~=0.9.2->-r requirements.txt (line 5)) (2.3.1+cu121)\n",
      "Requirement already satisfied: torchvision in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from timm~=0.9.2->-r requirements.txt (line 5)) (0.18.1+cu121)\n",
      "Requirement already satisfied: pyyaml in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from timm~=0.9.2->-r requirements.txt (line 5)) (6.0.1)\n",
      "Requirement already satisfied: safetensors in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from timm~=0.9.2->-r requirements.txt (line 5)) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from matplotlib~=3.8.4->-r requirements.txt (line 8)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from matplotlib~=3.8.4->-r requirements.txt (line 8)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from matplotlib~=3.8.4->-r requirements.txt (line 8)) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from matplotlib~=3.8.4->-r requirements.txt (line 8)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from matplotlib~=3.8.4->-r requirements.txt (line 8)) (24.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from matplotlib~=3.8.4->-r requirements.txt (line 8)) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from matplotlib~=3.8.4->-r requirements.txt (line 8)) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from huggingface-hub~=0.23.4->-r requirements.txt (line 10)) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from huggingface-hub~=0.23.4->-r requirements.txt (line 10)) (2024.2.0)\n",
      "Requirement already satisfied: requests in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from huggingface-hub~=0.23.4->-r requirements.txt (line 10)) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from huggingface-hub~=0.23.4->-r requirements.txt (line 10)) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from huggingface-hub~=0.23.4->-r requirements.txt (line 10)) (4.12.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from tensorboard~=2.15.0->-r requirements.txt (line 11)) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from tensorboard~=2.15.0->-r requirements.txt (line 11)) (1.62.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from tensorboard~=2.15.0->-r requirements.txt (line 11)) (2.28.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from tensorboard~=2.15.0->-r requirements.txt (line 11)) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from tensorboard~=2.15.0->-r requirements.txt (line 11)) (3.5.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from tensorboard~=2.15.0->-r requirements.txt (line 11)) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from tensorboard~=2.15.0->-r requirements.txt (line 11)) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from tensorboard~=2.15.0->-r requirements.txt (line 11)) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from tensorboard~=2.15.0->-r requirements.txt (line 11)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from tensorboard~=2.15.0->-r requirements.txt (line 11)) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.15.0->-r requirements.txt (line 11)) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.15.0->-r requirements.txt (line 11)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.15.0->-r requirements.txt (line 11)) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard~=2.15.0->-r requirements.txt (line 11)) (1.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from requests->huggingface-hub~=0.23.4->-r requirements.txt (line 10)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from requests->huggingface-hub~=0.23.4->-r requirements.txt (line 10)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from requests->huggingface-hub~=0.23.4->-r requirements.txt (line 10)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from requests->huggingface-hub~=0.23.4->-r requirements.txt (line 10)) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.15.0->-r requirements.txt (line 11)) (2.1.5)\n",
      "Requirement already satisfied: sympy in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from torch->timm~=0.9.2->-r requirements.txt (line 5)) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm~=0.9.2->-r requirements.txt (line 5)) (12.1.105)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.15.0->-r requirements.txt (line 11)) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard~=2.15.0->-r requirements.txt (line 11)) (3.2.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages (from sympy->torch->timm~=0.9.2->-r requirements.txt (line 5)) (1.3.0)\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "!cd Repos && git clone --branch eye-ai-compatible https://github.com/huynguyentran/RETFound_MAE.git \n",
    "!cd Repos/RETFound_MAE && pip install -r requirements.txt\n",
    "!cd Repos/RETFound_MAE && git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "repo_dir = \"Repos\"   # Set this to be where your github repos are located.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Update the load path so python can find modules for the model\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.home() / repo_dir / \"eye-ai-ml\"))\n",
    "sys.path.insert(0, str(Path.home() / repo_dir / \"eye-ai-exec\" / \"models\" / \"vgg19\"))\n",
    "sys.path.insert(0, \"Repos/RETFound_MAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "import json\n",
    "import os\n",
    "from eye_ai.eye_ai import EyeAI\n",
    "\n",
    "from pathlib import Path, PurePath\n",
    "import logging\n",
    "\n",
    "from deriva_ml import DatasetBag, Workflow, ExecutionConfiguration, DatasetVersion\n",
    "from deriva_ml import MLVocab as vc\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are already logged in.\n"
     ]
    }
   ],
   "source": [
    "from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\n",
    "catalog_id = \"eye-ai\" #@param\n",
    "host = 'www.eye-ai.org'\n",
    "\n",
    "\n",
    "gnl = GlobusNativeLogin(host=host)\n",
    "if gnl.is_logged_in([host]):\n",
    "    print(\"You are already logged in.\")\n",
    "else:\n",
    "    gnl.login([host], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n",
    "    print(\"Login Successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 21:27:46,220 - WARNING - nbstripout is not installed in repository. Please run nbstripout --install\n",
      "2025-03-23 21:27:46,221 - INFO - Loading dirty model.  Consider commiting and tagging: 1.1.0.post123+git.84bdec8f.dirty\n"
     ]
    }
   ],
   "source": [
    "cache_dir = '/mnt/c/Users/huyng/Desktop/Multimodal/cache'\n",
    "working_dir = '/mnt/c/Users/huyng/Desktop/Multimodal/working_dir'\n",
    "temp_dir = '/mnt/c/Users/huyng/Desktop/Multimodal/tmp'\n",
    "EA = EyeAI(hostname = host, catalog_id = catalog_id, cache_dir= cache_dir, working_dir=working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['git', 'log', '-n', '1', '--pretty=format:%H--', PosixPath('/home/nguyent8/multimodal_extract_images_template.ipynb')]' returned non-zero exit status 128.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 16\u001b[0m\n\u001b[1;32m      9\u001b[0m     ds_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrid\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset,\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaterialize\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m'\u001b[39m:EA\u001b[38;5;241m.\u001b[39mdataset_version(dataset_rid\u001b[38;5;241m=\u001b[39mdataset),\n\u001b[1;32m     13\u001b[0m     }\n\u001b[1;32m     14\u001b[0m     to_be_download\u001b[38;5;241m.\u001b[39mappend(ds_dict)\n\u001b[0;32m---> 16\u001b[0m workflow_instance \u001b[38;5;241m=\u001b[39m \u001b[43mEA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_workflow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMultimodal workflow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkflow_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMultimodal workflow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m config \u001b[38;5;241m=\u001b[39m ExecutionConfiguration(\n\u001b[1;32m     22\u001b[0m     datasets\u001b[38;5;241m=\u001b[39mto_be_download,\n\u001b[1;32m     23\u001b[0m     assets \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4-QA7T\u001b[39m\u001b[38;5;124m'\u001b[39m,],\n\u001b[1;32m     24\u001b[0m     workflow\u001b[38;5;241m=\u001b[39mworkflow_instance,\n\u001b[1;32m     25\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstance of applying CV modelsto multimodal data. We are attempting to increase the accuracy of prediction by including table values into images prediction.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m execution \u001b[38;5;241m=\u001b[39m EA\u001b[38;5;241m.\u001b[39mcreate_execution(config)\n",
      "File \u001b[0;32m~/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages/deriva_ml/deriva_ml_base.py:1156\u001b[0m, in \u001b[0;36mDerivaML.create_workflow\u001b[0;34m(self, name, workflow_type, description, create)\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;66;03m# Make sure type is correct.\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup_term(MLVocab\u001b[38;5;241m.\u001b[39mworkflow_type, workflow_type)\n\u001b[0;32m-> 1156\u001b[0m github_url, is_dirty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_github_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dirty:\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutable_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has been modified since last commit. Consider commiting before executing\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1161\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages/deriva_ml/deriva_ml_base.py:1229\u001b[0m, in \u001b[0;36mDerivaML._github_url\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1226\u001b[0m     is_dirty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# If Git command fails, assume no changes\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get SHA-1 hash of latest commit of the file in the repository\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1229\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-n\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--pretty=format:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH--\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecutable_path\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecutable_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1236\u001b[0m sha \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m   1237\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgithub_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/blob/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msha\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutable_path\u001b[38;5;241m.\u001b[39mrelative_to(repo_root)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/my-tensorflow-conda/lib/python3.10/subprocess.py:526\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 526\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    527\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['git', 'log', '-n', '1', '--pretty=format:%H--', PosixPath('/home/nguyent8/multimodal_extract_images_template.ipynb')]' returned non-zero exit status 128."
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "    '4-4116', # Selected images for training\n",
    "    '4-411G', # Selected images for testing\n",
    "    '2-7P5P', # Full multimodal dataset\n",
    "    ]\n",
    "\n",
    "to_be_download = []\n",
    "for dataset in datasets:\n",
    "    ds_dict = {\n",
    "        'rid': dataset,\n",
    "        'materialize':True,\n",
    "        'version':EA.dataset_version(dataset_rid=dataset),\n",
    "    }\n",
    "    to_be_download.append(ds_dict)\n",
    "\n",
    "workflow_instance = EA.create_workflow(\n",
    "    name=\"Multimodal workflow\",\n",
    "    workflow_type=\"Multimodal workflow\"\n",
    ")\n",
    "\n",
    "config = ExecutionConfiguration(\n",
    "    datasets=to_be_download,\n",
    "    assets = ['4-QA7T',],\n",
    "    workflow=workflow_instance,\n",
    "    description=\"Instance of applying CV modelsto multimodal data. We are attempting to increase the accuracy of prediction by including table values into images prediction.\")\n",
    "\n",
    "execution = EA.create_execution(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ds_bag = execution.datasets[0]\n",
    "testing_ds_bag = execution.datasets[1]\n",
    "\n",
    "multimodal_full_ds_bag = execution.datasets[2]\n",
    "retfound_pretrained_weight = execution.asset_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_from_bag(ds_bag: DatasetBag, multimodal_full_ds_bag: DatasetBag):\n",
    "    observation_table = ds_bag.get_table_as_dataframe('Observation')\n",
    "    image_table = ds_bag.get_table_as_dataframe('Image')\n",
    "    laterality_table = ds_bag.get_table_as_dataframe('Execution_Image_Fundus_Laterality')\n",
    "\n",
    "    image_table_filtered = image_table[['RID', 'Filename', 'Observation']].rename(columns={'RID': 'RID_Image'})\n",
    "    laterality_table_filtered = laterality_table[['Image', 'Image_Side']].rename(columns={'Image': 'RID_Image'})\n",
    "    image_laterality = pd.merge(image_table_filtered, laterality_table_filtered, left_on='RID_Image', right_on='RID_Image', how='inner')\n",
    "    observation_table_filtered = observation_table[['RID',  'Subject']].rename(columns={'RID': 'RID_Observation'})\n",
    "    image_laterality_observation = pd.merge(image_laterality, observation_table_filtered, left_on='Observation', right_on='RID_Observation', how='inner')\n",
    "\n",
    "    wide = EA.multimodal_wide(multimodal_full_ds_bag) \n",
    "\n",
    "    image_observation_laterality_subject_wide = pd.merge(\n",
    "     wide, \n",
    "     image_laterality_observation, \n",
    "     left_on=['RID_Subject', 'Image_Side'], \n",
    "     right_on=['Subject', 'Image_Side'], \n",
    "     how='inner'\n",
    "    )\n",
    "\n",
    "    return image_observation_laterality_subject_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = get_dataframe_from_bag(training_ds_bag, multimodal_full_ds_bag)\n",
    "test_df= get_dataframe_from_bag(testing_ds_bag, multimodal_full_ds_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train_df = train_df[['RID_Image', 'Filename','Condition_Label', 'Condition_Display']]\n",
    "filtered_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_test_df = test_df[['RID_Image', 'Filename','Condition_Label', 'Condition_Display']]\n",
    "filtered_test_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = execution._working_dir\n",
    "working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\"Glaucoma_Suspect\": 0 ,\n",
    "            \"Glaucoma\": 1, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil \n",
    "\n",
    "def create_dataset_folder(df, output_path, output_name):\n",
    "    output_path =  output_path / output_name\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    output_path_suspected = output_path / \"Glaucoma_Suspect\"\n",
    "    output_path_glaucoma = output_path / \"Glaucoma\"\n",
    "    \n",
    "    output_path_suspected.mkdir(parents=True, exist_ok=True)\n",
    "    output_path_glaucoma.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    \n",
    "    for index, row in df.iterrows():     \n",
    "        src_path = row[\"Filename\"]\n",
    "        dest_name = row[\"RID_Image\"] + \".jpg\"\n",
    "        label = row['Condition_Label']\n",
    "        if label == \"GS\":\n",
    "            dest_path = os.path.join(output_path_suspected, dest_name)\n",
    "        elif label == \"POAG\" or label == \"PACG\":\n",
    "            dest_path = os.path.join(output_path_glaucoma, dest_name)\n",
    "        else: \n",
    "            continue    \n",
    "        shutil.copy2(src_path, dest_path)\n",
    "        \n",
    "    return output_path \n",
    "\n",
    "train_dir = create_dataset_folder(filtered_train_df, working_dir, \"train\")\n",
    "test_dir = create_dataset_folder(filtered_test_df, working_dir, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_set(train_dir, val_dir, split_ratio=0.15):\n",
    "     os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "     for class_name in os.listdir(train_dir):\n",
    "          class_train_path = os.path.join(train_dir, class_name)\n",
    "          class_val_path = os.path.join(val_dir, class_name)\n",
    "\n",
    "          if os.path.isdir(class_train_path):  \n",
    "               os.makedirs(class_val_path, exist_ok=True)\n",
    "\n",
    "               images = [f for f in os.listdir(class_train_path) if os.path.isfile(os.path.join(class_train_path, f))]\n",
    "               num_val = int(len(images) * split_ratio)\n",
    "\n",
    "               val_images = random.sample(images, num_val)\n",
    "               for img in val_images:\n",
    "                    shutil.move(os.path.join(class_train_path, img), os.path.join(class_val_path, img))\n",
    "\n",
    "val_dir = working_dir / \"val\"\n",
    "create_validation_set(train_dir, val_dir, split_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def count_images_per_class(directory):\n",
    "     class_counts = {}\n",
    "     for class_name in os.listdir(directory):\n",
    "          class_path = os.path.join(directory, class_name)\n",
    "          if os.path.isdir(class_path): \n",
    "               num_images = len([f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))])\n",
    "               class_counts[class_name] = num_images\n",
    "     return class_counts\n",
    "\n",
    "\n",
    "train_counts = count_images_per_class(train_dir)\n",
    "test_counts = count_images_per_class(test_dir)\n",
    "val_counts = count_images_per_class(val_dir)\n",
    "\n",
    "print(\"Training Set:\")\n",
    "for class_name, count in train_counts.items():\n",
    "     print(f\"  {class_name}: {count} images\")\n",
    "\n",
    "print(\"\\nValidation Set:\")\n",
    "for class_name, count in test_counts.items():\n",
    "     print(f\"  {class_name}: {count} images\")\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "for class_name, count in test_counts.items():\n",
    "     print(f\"  {class_name}: {count} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rid_images_in_folder\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Example usage for the 'train_dir', 'val_dir', and 'test_dir' folders:\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# For train directory\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m train_glaucoma_suspect_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mtrain_dir\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGlaucoma_Suspect\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m train_glaucoma_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(train_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGlaucoma\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m train_glaucoma_suspect_rid_images \u001b[38;5;241m=\u001b[39m get_rid_images_from_folder(train_glaucoma_suspect_folder, filtered_train_df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dir' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_rid_images_from_folder(folder_path, df):\n",
    "    # Get all the filenames in the folder (excluding the path)\n",
    "    filenames = os.listdir(folder_path)\n",
    "    \n",
    "    # Extract the 'RID_Image' values from the DataFrame that match the filenames (without the extension)\n",
    "    rid_images_in_folder = df[df['RID_Image'].isin([os.path.splitext(f)[0] for f in filenames])]\n",
    "    \n",
    "    return rid_images_in_folder\n",
    "\n",
    "# Example usage for the 'train_dir', 'val_dir', and 'test_dir' folders:\n",
    "\n",
    "# For train directory\n",
    "train_glaucoma_suspect_folder = os.path.join(train_dir, \"Glaucoma_Suspect\")\n",
    "train_glaucoma_folder = os.path.join(train_dir, \"Glaucoma\")\n",
    "\n",
    "train_glaucoma_suspect_rid_images = get_rid_images_from_folder(train_glaucoma_suspect_folder, filtered_train_df)\n",
    "train_glaucoma_rid_images = get_rid_images_from_folder(train_glaucoma_folder, filtered_train_df)\n",
    "\n",
    "# For validation directory\n",
    "val_glaucoma_suspect_folder = os.path.join(val_dir, \"Glaucoma_Suspect\")\n",
    "val_glaucoma_folder = os.path.join(val_dir, \"Glaucoma\")\n",
    "\n",
    "val_glaucoma_suspect_rid_images = get_rid_images_from_folder(val_glaucoma_suspect_folder, filtered_train_df)\n",
    "val_glaucoma_rid_images = get_rid_images_from_folder(val_glaucoma_folder, filtered_train_df)\n",
    "\n",
    "# For test directory\n",
    "test_glaucoma_suspect_folder = os.path.join(test_dir, \"Glaucoma_Suspect\")\n",
    "test_glaucoma_folder = os.path.join(test_dir, \"Glaucoma\")\n",
    "\n",
    "test_glaucoma_suspect_rid_images = get_rid_images_from_folder(test_glaucoma_suspect_folder, filtered_test_df)\n",
    "test_glaucoma_rid_images = get_rid_images_from_folder(test_glaucoma_folder, filtered_test_df)\n",
    "\n",
    "# Convert 'RID_Image' column to lists\n",
    "train_glaucoma_suspect_rid_images_list = train_glaucoma_suspect_rid_images['RID_Image'].tolist()\n",
    "train_glaucoma_rid_images_list = train_glaucoma_rid_images['RID_Image'].tolist()\n",
    "\n",
    "val_glaucoma_suspect_rid_images_list = val_glaucoma_suspect_rid_images['RID_Image'].tolist()\n",
    "val_glaucoma_rid_images_list = val_glaucoma_rid_images['RID_Image'].tolist()\n",
    "\n",
    "test_glaucoma_suspect_rid_images_list = test_glaucoma_suspect_rid_images['RID_Image'].tolist()\n",
    "test_glaucoma_rid_images_list = test_glaucoma_rid_images['RID_Image'].tolist()\n",
    "\n",
    "\n",
    "def save_to_text_file(file_path, data_list):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for item in data_list:\n",
    "            file.write(f\"{item}\\n\")\n",
    "\n",
    "# Save 'RID_Image' lists to text files\n",
    "save_to_text_file(\"train_glaucoma_suspect_rid_images.txt\", train_glaucoma_suspect_rid_images_list)\n",
    "save_to_text_file(\"train_glaucoma_rid_images.txt\", train_glaucoma_rid_images_list)\n",
    "\n",
    "save_to_text_file(\"val_glaucoma_suspect_rid_images.txt\", val_glaucoma_suspect_rid_images_list)\n",
    "save_to_text_file(\"val_glaucoma_rid_images.txt\", val_glaucoma_rid_images_list)\n",
    "\n",
    "save_to_text_file(\"test_glaucoma_suspect_rid_images.txt\", test_glaucoma_suspect_rid_images_list)\n",
    "save_to_text_file(\"test_glaucoma_rid_images.txt\", test_glaucoma_rid_images_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_path_models = execution.execution_asset_path(\"Diagnosis_Model\")\n",
    "asset_path_output = execution.execution_asset_path(\"Model_Prediction\")\n",
    "asset_path_logs = execution.execution_asset_path(\"Training_Log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_date = datetime.now().strftime(\"%b_%d_%Y\") \n",
    "print(current_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir, val_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_finetune import main, get_args_parser \n",
    "import torch\n",
    "\n",
    "with execution.execute() as exec:\n",
    "    args_list = [\n",
    "        \"--model\", \"RETFound_mae\",\n",
    "        \"--savemodel\",\n",
    "        \"--global_pool\",\n",
    "        \"--batch_size\", \"16\",\n",
    "        \"--world_size\", \"1\",\n",
    "        \"--epochs\", \"100\",\n",
    "        \"--blr\", \"5e-3\", \"--layer_decay\", \"0.65\",\n",
    "        \"--weight_decay\", \"0.05\", \"--drop_path\", \"0.2\",\n",
    "        \"--nb_classes\", \"2\",\n",
    "        \"--data_path\", \"/data/nguyent8/EyeAI_working/\",\n",
    "        \"--input_size\", \"224\",\n",
    "        \"--task\", str(asset_path_output),\n",
    "        \"--output_dir\", str(asset_path_output),\n",
    "        \"--finetune\", str(retfound_pretrained_weight),\n",
    "    ]\n",
    "\n",
    "    args = get_args_parser().parse_args(args_list)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    if args.output_dir:\n",
    "        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    main(args, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vgg19_diagnosis_train import train_and_evaluate\n",
    "with execution.execute() as exec:\n",
    "        predictions_results, metrics_summary, model_save_path, training_history_csv = train_and_evaluate(\n",
    "            train_path=train_dir,\n",
    "            valid_path=val_dir, \n",
    "            test_path=test_dir, \n",
    "            model_path=asset_path_models,\n",
    "            log_path=asset_path_logs,\n",
    "            eval_path=asset_path_output,\n",
    "            model_name = f\"VGG19_Multimodal_{current_date}\",\n",
    "            classes = classes,\n",
    "            )\n",
    "        print(\"Execution Results:\")\n",
    "        print(predictions_results, metrics_summary, model_save_path, training_history_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_results, metrics_summary, model_save_path, training_history_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.upload_execution_outputs(clean_folder=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My TensorFlow (Conda)",
   "language": "python",
   "name": "my-tensorflow-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
