{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29948bf-af19-4328-8e4b-0216621f6c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = \"Repos\"   # Set this to be where your github repos are located.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Update the load path so python can find modules for the model\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.home() / repo_dir / \"eye-ai-ml\"))\n",
    "sys.path.append('Repos/eye-ai-exec/models/vgg19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc74b1f-337b-4835-b866-2a631d274f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "import json\n",
    "import os\n",
    "from eye_ai.eye_ai import EyeAI\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path, PurePath\n",
    "import logging\n",
    "\n",
    "from deriva_ml import DatasetBag, Workflow, ExecutionConfiguration, DatasetVersion\n",
    "from deriva_ml import MLVocab as vc\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ff9030-2b6e-4188-85fa-c17002d8423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\n",
    "catalog_id = \"eye-ai\" #@param\n",
    "host = 'www.eye-ai.org'\n",
    "\n",
    "\n",
    "gnl = GlobusNativeLogin(host=host)\n",
    "if gnl.is_logged_in([host]):\n",
    "    print(\"You are already logged in.\")\n",
    "else:\n",
    "    gnl.login([host], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n",
    "    print(\"Login Successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74699969-e90f-4e0f-8500-e30dcb26c7ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cache_dir = '/data'\n",
    "working_dir = '/data'\n",
    "EA = EyeAI(hostname = host, catalog_id = catalog_id, cache_dir= cache_dir, working_dir=working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e716415e-5986-4c3d-8aa9-c1f5960314f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RID of source dataset, if any.\n",
    "datasets = [\n",
    "                 '4-PV06',\n",
    "                  '2-39FY', \n",
    "                  '2-277M',\n",
    "]\n",
    "\n",
    "to_be_download = []\n",
    "for dataset in datasets:\n",
    "    ds_dict = {\n",
    "        'rid': dataset,\n",
    "        'materialize':True,\n",
    "        'version':EA.dataset_version(dataset_rid=dataset),\n",
    "    }\n",
    "    to_be_download.append(ds_dict)\n",
    "# EA.add_term(vc.workflow_type, \"VGG19 Model Train\", description=\"A workflow to train VGG19 model\")\n",
    "# Workflow instance\n",
    "workflow_rid = EA.add_workflow(Workflow(\n",
    "    name=\"VGG19 Model train - 3000 images\",\n",
    "    url=\"https://github.com/informatics-isi-edu/eye-ai-exec/blob/main/notebooks/VGG19_Huy/VGG19_TRAIN_3000.ipynb\",\n",
    "    workflow_type=\"VGG19 Model Train\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Set to False if you only need the metadata from the bag, and not the assets.\n",
    "download_assets = True\n",
    "\n",
    "config = ExecutionConfiguration(\n",
    "    # Comment out the following line if you don't need the assets.\n",
    "    datasets=to_be_download if download_assets else [],\n",
    "    workflow=workflow_rid,\n",
    "    description=\"Instance of training VGG19 model - 3000 images\",\n",
    ")\n",
    "    \n",
    "\n",
    "# Initialize execution\n",
    "execution = EA.create_execution(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066ab0e6-120d-43ee-97a8-d3e672266fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43fa59a-4089-4a54-9416-8eb3495f976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_bag_0 = execution.datasets[0]\n",
    "\n",
    "\n",
    "ds_bag_val = execution.datasets[1]\n",
    "ds_bag_test = execution.datasets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d0ddd-d7cc-4f95-aa9a-2c26b76b3d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_bag_list = [ds_bag_0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a929a007-ffb1-4ffe-a257-46ad9dd27f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_excluded_df = pd.read_csv(\"valid_no_optic_disc_image_ids.csv\")\n",
    "val_excluded = val_excluded_df[\"ID\"].tolist()\n",
    "\n",
    "train_excluded_df = pd.read_csv(\"train_no_optic_disc_image_ids.csv\")\n",
    "train_excluded = train_excluded_df[\"ID\"].tolist()\n",
    "\n",
    "test_included_df = pd.read_csv(\"Graded_Test_Dataset_2-277M_With_Demographics_CDR_Diagnosis_Image_Quality_Model_Diagnosis_Predicitons_with_Jiun_Do_June8_2024_with_Catalog_model_predictions.csv\")\n",
    "test_included = test_included_df[\"Image_cd\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7111f3-3746-4f04-996d-0985a8708abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = execution._working_dir\n",
    "validation_image_path_cropped, validation_csv_cropped = EA.create_cropped_images(ds_bag_val,\n",
    "                                                                                 output_dir = output_dir ,\n",
    "                                                                                 crop_to_eye=True,\n",
    "                                                                                exclude_list= val_excluded)\n",
    "\n",
    "validation_image_path_uncropped, validation_csv_uncropped = EA.create_cropped_images(ds_bag_val,\n",
    "                                                                                 output_dir = output_dir,\n",
    "                                                                                 crop_to_eye=False,\n",
    "                                                                                    exclude_list= val_excluded)\n",
    "\n",
    "test_image_path_cropped, test_csv_cropped = EA.create_cropped_images(ds_bag_test,\n",
    "                                                                     output_dir = output_dir,\n",
    "                                                                     crop_to_eye=True,\n",
    "                                                                     include_only_list = test_included)\n",
    "\n",
    "test_image_path_uncropped, test_csv_uncropped = EA.create_cropped_images(ds_bag_test,\n",
    "                                                                         output_dir = output_dir ,\n",
    "                                                                         crop_to_eye=False,\n",
    "                                                                         include_only_list = test_included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a86ce-8797-44c1-bf50-3ad87030f7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyper_parameters_json_path = \"best_hyperparameters_exluding_no_optic_disc_images_june_24_2024.json\"\n",
    "best_hyper_parameters_json_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f28035b-ad2e-4857-9903-7f026ac53260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crete asset path\n",
    "asset_path_models = execution.execution_asset_path(\"Diagnosis_Model\")\n",
    "asset_path_output = execution.execution_asset_path(\"Model_Prediction\")\n",
    "asset_path_logs = execution.execution_asset_path(\"Training_Log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1bdaec-20f1-4e4d-823c-fbae77d23c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_path_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e71d766-99e8-4b71-9ea5-a593bf3081af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_date = datetime.now().strftime(\"%b_%d_%Y\") \n",
    "print(current_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172870d0-e038-4029-81b6-e7308a4a1c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vgg19_diagnosis_train import train_and_evaluate\n",
    "with execution.execute() as exec:\n",
    "    for ds_bag in ds_bag_list:\n",
    "        image_path_ds_bag_path_cropped, csv_ds_bag_cropped = EA.create_cropped_images(\n",
    "                                                   ds_bag, \n",
    "                                                   output_dir, \n",
    "                                                   crop_to_eye=True,\n",
    "                                                     exclude_list=train_excluded)\n",
    "        image_path_ds_bag_path_uncropped, csv_ds_bag_uncropped = EA.create_cropped_images(\n",
    "                                                   ds_bag, \n",
    "                                                   output_dir, \n",
    "                                                   crop_to_eye=False,\n",
    "                                                 exclude_list=train_excluded)\n",
    "        print(\"Dataset: \", ds_bag.dataset_rid)\n",
    "        \n",
    "        predictions_results_cropped, metrics_summary_cropped, model_save_path_cropped, training_history_csv_cropped=train_and_evaluate(\n",
    "            train_path=image_path_ds_bag_path_cropped,\n",
    "            valid_path=validation_image_path_cropped, \n",
    "            test_path=test_image_path_cropped, \n",
    "            model_path=asset_path_models,\n",
    "            log_path=asset_path_logs,\n",
    "            eval_path=asset_path_output,\n",
    "            best_hyperparameters_json_path = best_hyper_parameters_json_path,\n",
    "            model_name = f\"VGG19_3000_Images_Cropped_{ds_bag.dataset_rid}_{current_date}\"\n",
    "           )\n",
    "\n",
    "        predictions_results, metrics_summary, model_save_path, training_history_csv = train_and_evaluate(\n",
    "            train_path=image_path_ds_bag_path_uncropped,\n",
    "            valid_path=validation_image_path_uncropped, \n",
    "            test_path=test_image_path_uncropped, \n",
    "            model_path=asset_path_models,\n",
    "            log_path=asset_path_logs,\n",
    "            eval_path=asset_path_output,\n",
    "            best_hyperparameters_json_path = best_hyper_parameters_json_path,\n",
    "            model_name = f\"VGG19_3000_Images_Uncropped_{ds_bag.dataset_rid}_{current_date}\"\n",
    "           )\n",
    "        print(\"Uncropped\")\n",
    "        print(predictions_results, metrics_summary, model_save_path, training_history_csv)\n",
    "        print(\"Cropped\")\n",
    "        print(predictions_results_cropped, metrics_summary_cropped, model_save_path_cropped, training_history_csv_cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2a3c1e-6ab2-4a64-ae71-dd6b1ec4eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.upload_execution_outputs(clean_folder=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My TensorFlow (Conda)",
   "language": "python",
   "name": "my-tensorflow-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
