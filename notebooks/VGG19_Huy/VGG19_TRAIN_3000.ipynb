{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e194946c-2a5b-4484-8095-4e244234c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = \"Repos\"   # Set this to be where your github repos are located.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Update the load path so python can find modules for the model\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.home() / repo_dir / \"eye-ai-ml\"))\n",
    "sys.path.append('Repos/eye-ai-exec/models/vgg19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "869b0e25-fde2-4f59-8a27-43703d5d6dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 20:59:36.335234: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738040376.353809   13586 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738040376.359502   13586 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-27 20:59:36.378320: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Prerequisites\n",
    "import json\n",
    "import os\n",
    "from eye_ai.eye_ai import EyeAI\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path, PurePath\n",
    "import logging\n",
    "\n",
    "from deriva_ml import DatasetBag, Workflow, ExecutionConfiguration\n",
    "from deriva_ml import MLVocab as vc\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c50942c-8d25-46b7-aade-1155ea24eed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 20:59:37,841 - INFO - Creating client of type <class 'globus_sdk.services.auth.client.native_client.NativeAppAuthClient'> for service \"auth\"\n",
      "2025-01-27 20:59:37,842 - INFO - Finished initializing AuthLoginClient. client_id='8ef15ba9-2b4a-469c-a163-7fd910c9d111', type(authorizer)=<class 'globus_sdk.authorizers.base.NullAuthorizer'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are already logged in.\n"
     ]
    }
   ],
   "source": [
    "from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\n",
    "catalog_id = \"eye-ai\" #@param\n",
    "host = 'www.eye-ai.org'\n",
    "\n",
    "\n",
    "gnl = GlobusNativeLogin(host=host)\n",
    "if gnl.is_logged_in([host]):\n",
    "    print(\"You are already logged in.\")\n",
    "else:\n",
    "    gnl.login([host], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n",
    "    print(\"Login Successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5b8ad83-ebec-48cf-9996-c50e441cf5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 20:59:39,571 - INFO - Creating client of type <class 'globus_sdk.services.auth.client.native_client.NativeAppAuthClient'> for service \"auth\"\n",
      "2025-01-27 20:59:39,572 - INFO - Finished initializing AuthLoginClient. client_id='8ef15ba9-2b4a-469c-a163-7fd910c9d111', type(authorizer)=<class 'globus_sdk.authorizers.base.NullAuthorizer'>\n"
     ]
    }
   ],
   "source": [
    "cache_dir = '/data'\n",
    "working_dir = '/data'\n",
    "EA = EyeAI(hostname = host, catalog_id = catalog_id, cache_dir= cache_dir, working_dir=working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca7baa5-ee72-4583-93fe-36acd8411325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RID of source dataset, if any.\n",
    "source_dataset = ['4-JBTJ', \n",
    "                  '4-JBW2', \n",
    "                  '4-JBXJ', \n",
    "                  '4-JBZ2', \n",
    "                  '4-JC0J', \n",
    "                  '4-JC22', \n",
    "                  '4-JC3J', \n",
    "                  '4-JC52', \n",
    "                  '4-JC6J', \n",
    "                  '4-JC82', \n",
    "                  '2-39FY', \n",
    "                  '2-277M']\n",
    "\n",
    "# EA.add_term(vc.workflow_type, \"VGG19 Model Train\", description=\"A workflow to train VGG19 model\")\n",
    "# Workflow instance\n",
    "workflow_instance = Workflow(\n",
    "    name=\"VGG19 Model train - 10 images\",\n",
    "    url=\"https://github.com/informatics-isi-edu/eye-ai-exec/blob/main/notebooks/VGG19_Huy/VGG19_TRAIN_10.ipynb\",\n",
    "    workflow_type=\"VGG19 Model Train\",\n",
    ")\n",
    "# Configuration instance.\n",
    "\n",
    "# Set to False if you only need the metadata from the bag, and not the assets.\n",
    "download_assets = True\n",
    "\n",
    "config = ExecutionConfiguration(\n",
    "    # Comment out the following line if you don't need the assets.\n",
    "    # datasets=[source_dataset] if download_assets else [],\n",
    "    datasets=source_dataset,\n",
    "    workflow=workflow_instance,\n",
    "    description=\"Instance of training VGG19 model - 10 images\")\n",
    "\n",
    "# Initialize execution\n",
    "execution = EA.create_execution(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bdf4e4e-0298-4e56-b2b5-b83cd3271b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caching_dir: /data\n",
      "working_dir: /data/nguyent8/EyeAI_working\n",
      "execution_rid: 4-M48M\n",
      "workflow_rid: 4-53WJ\n",
      "dataset_paths: [PosixPath('/data/4-JBTJ_3135bb6182f08f3200bf7f00ff26cbf98b93e3239f45f079d8338d5f9d93ad71/Dataset_4-JBTJ'), PosixPath('/data/4-JBW2_3cf40177e92525b639f7db77cc2106a576caff9f2c35e11d0d2e6d0372ee5f23/Dataset_4-JBW2'), PosixPath('/data/4-JBXJ_6aca9955d13051e7cb5177ca12607291e5c2ffcbacb51c52147a9e5967dbca9d/Dataset_4-JBXJ'), PosixPath('/data/4-JBZ2_648fbd820eee1f5367dc3c7b9ec53a70cae91daed14e329be7dc33be56022506/Dataset_4-JBZ2'), PosixPath('/data/4-JC0J_51dd716671af784e0382e7608e70b478feca99f740b3920df7943dbeb0b8a654/Dataset_4-JC0J'), PosixPath('/data/4-JC22_43f0ecbf4df85dc39a00af608fc24bff68e228f37d5a9649d65e99f9c76460bc/Dataset_4-JC22'), PosixPath('/data/4-JC3J_3473b6d8b2d657ac6eef781e4eec45d5d76da3d0a1d2c23aad3a4d83878fdaa6/Dataset_4-JC3J'), PosixPath('/data/4-JC52_ebd489d3026fe7a9133b1cccb6f384d0f4259d8551d174baf87fc33858ba7c41/Dataset_4-JC52'), PosixPath('/data/4-JC6J_ea00b129d0229eee09d4840a1ebf60e3fb06c387c4dbd683a51fc9869c37943f/Dataset_4-JC6J'), PosixPath('/data/4-JC82_48c12f6ee1a2a9a95675f03f2ec6cb5536b8d3e241777cc31bc545c197449573/Dataset_4-JC82'), PosixPath('/data/2-39FY_b9659b218f7e341ad2b880e988e56b9a2a7bf9acf3cb90e956271129287b99dd/Dataset_2-39FY'), PosixPath('/data/2-277M_6a784dc8c439efc72dca0487e59ee20e010ef843c499fbdfbc5f453909ff5388/Dataset_2-277M')]\n",
      "asset_paths: []\n",
      "configuration: datasets=['4-JBTJ', '4-JBW2', '4-JBXJ', '4-JBZ2', '4-JC0J', '4-JC22', '4-JC3J', '4-JC52', '4-JC6J', '4-JC82', '2-39FY', '2-277M'] assets=[] workflow=Workflow(name='VGG19 Model train - 10 images', url='https://github.com/informatics-isi-edu/eye-ai-exec/blob/main/notebooks/VGG19_Huy/VGG19_TRAIN_10.ipynb', workflow_type='VGG19 Model Train', version=None, description=None) description='Instance of training VGG19 model - 10 images'\n"
     ]
    }
   ],
   "source": [
    "print(execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84f5e0b3-05c5-4d80-bf15-64e9cf40eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_bag_0 = DatasetBag(execution.dataset_paths[0])\n",
    "ds_bag_1 = DatasetBag(execution.dataset_paths[1])\n",
    "ds_bag_2 = DatasetBag(execution.dataset_paths[2])\n",
    "ds_bag_3 = DatasetBag(execution.dataset_paths[3])\n",
    "ds_bag_4 = DatasetBag(execution.dataset_paths[4])\n",
    "ds_bag_5 = DatasetBag(execution.dataset_paths[5])\n",
    "ds_bag_6 = DatasetBag(execution.dataset_paths[6])\n",
    "ds_bag_7 = DatasetBag(execution.dataset_paths[7])\n",
    "ds_bag_8 = DatasetBag(execution.dataset_paths[8])\n",
    "ds_bag_9 = DatasetBag(execution.dataset_paths[9])\n",
    "ds_bag_val = DatasetBag(execution.dataset_paths[10])\n",
    "ds_bag_test = DatasetBag(execution.dataset_paths[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbfa78a6-5e7b-4334-b809-792954686e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_bag_list = [ds_bag_0, ds_bag_1, ds_bag_2, ds_bag_3, ds_bag_4,\n",
    "               ds_bag_5, ds_bag_6, ds_bag_7, ds_bag_8, ds_bag_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1aa19c9-0d59-4cce-a0a4-44095287805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_bag_dataset_paths = {\n",
    "    0: execution.dataset_paths[0],\n",
    "    1: execution.dataset_paths[1],\n",
    "    2: execution.dataset_paths[2],\n",
    "    3: execution.dataset_paths[3],\n",
    "    4: execution.dataset_paths[4],\n",
    "    5: execution.dataset_paths[5],\n",
    "    6: execution.dataset_paths[6],\n",
    "    7: execution.dataset_paths[7],\n",
    "    8: execution.dataset_paths[8],\n",
    "    9: execution.dataset_paths[9]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c433d15c-2cce-4479-9ab9-be45a8751c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = execution.working_dir\n",
    "validation_image_path_cropped, validation_csv_cropped = EA.create_images_directory(execution.dataset_paths[10],\n",
    "                                                                                    ds_bag_val,\n",
    "                                                                                 output_dir = output_dir / 'valid_cropped',\n",
    "                                                                                 crop_to_eye=True)\n",
    "\n",
    "test_image_path_cropped, test_csv_cropped = EA.create_images_directory(execution.dataset_paths[11],\n",
    "                                                                       ds_bag_test,\n",
    "                                                                     output_dir = output_dir / 'test_cropped',\n",
    "                                                                     crop_to_eye=True)\n",
    "\n",
    "validation_image_path_uncropped, validation_csv_cropped = EA.create_images_directory(execution.dataset_paths[10],\n",
    "                                                                                    ds_bag_val,\n",
    "                                                                                 output_dir = output_dir / 'valid_uncropped',\n",
    "                                                                                 crop_to_eye=False)\n",
    "test_image_path_uncropped, test_csv_uncropped = EA.create_images_directory(execution.dataset_paths[11],\n",
    "                                                                       ds_bag_test,\n",
    "                                                                     output_dir = output_dir / 'test_uncropped',\n",
    "                                                                     crop_to_eye=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e42fde8b-86ea-4213-8737-8aedc07cf607",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyper_parameters_json_path = \"best_hyperparameters_exluding_no_optic_disc_images_june_24_2024.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "593bcc82-bfc0-4f08-88ed-3d400c9aac06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best_hyperparameters_exluding_no_optic_disc_images_june_24_2024.json'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyper_parameters_json_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeed2db-43af-4d3f-a445-c96336241a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  4-JBTJ\n",
      "Found 20 images belonging to 2 classes.\n",
      "Found 905 images belonging to 2 classes.\n",
      "Found 1130 images belonging to 2 classes.\n",
      "train path:  /data/nguyent8/EyeAI_working/4-JBTJ/Images_Cropped\n",
      "validation path:  /data/nguyent8/EyeAI_working/valid_cropped/2-39FY/Images_Cropped\n",
      "test path:  /data/nguyent8/EyeAI_working/test_cropped/2-277M/Images_Cropped\n",
      "train_generator.class_indices :  {'No_Glaucoma': 0, 'Suspected_Glaucoma': 1}\n",
      "validation_generator.class_indices :  {'No_Glaucoma': 0, 'Suspected_Glaucoma': 1}\n",
      "test_generator.class_indices :  {'No_Glaucoma': 0, 'Suspected_Glaucoma': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1738043195.963502   13586 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/home/nguyent8/.conda/envs/my-tensorflow-conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 21:46:40.448619: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 256901120 exceeds 10% of free system memory.\n",
      "2025-01-27 21:46:40.573311: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 256901120 exceeds 10% of free system memory.\n",
      "2025-01-27 21:46:41.027564: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 128450560 exceeds 10% of free system memory.\n",
      "2025-01-27 21:46:41.278936: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 128450560 exceeds 10% of free system memory.\n",
      "2025-01-27 21:46:52.525687: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 128450560 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 219s/step - accuracy_score: 0.4500 - f1_score_normal: 8.2353 - loss: 0.9541 - roc_auc_score: 0.3600 - val_accuracy_score: 0.5094 - val_f1_score_normal: 1.2531 - val_loss: 0.7007 - val_roc_auc_score: 0.5487\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 215s/step - accuracy_score: 0.6000 - f1_score_normal: 10.0000 - loss: 0.8033 - roc_auc_score: 0.5700 - val_accuracy_score: 0.5127 - val_f1_score_normal: 1.5312 - val_loss: 0.6980 - val_roc_auc_score: 0.5707\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16s/step - accuracy_score: 0.6500 - f1_score_normal: 8.2353 - loss: 0.6247 - roc_auc_score: 0.6850"
     ]
    }
   ],
   "source": [
    "from vgg19_diagnosis_train import main \n",
    "with execution.execute() as exec:\n",
    "    for index, ds_bag in enumerate(ds_bag_list):\n",
    "        image_path_ds_bag_path_cropped, csv_ds_bag_cropped = EA.create_images_directory(ds_bag_dataset_paths[index],\n",
    "                                                   ds_bag, \n",
    "                                                   output_dir, \n",
    "                                                   crop_to_eye=True)\n",
    "        \n",
    "        image_path_ds_bag_path_uncropped, csv_ds_bag_uncropped = EA.create_images_directory(ds_bag_dataset_paths[index],\n",
    "                                                   ds_bag, \n",
    "                                                   output_dir, \n",
    "                                                   crop_to_eye=False)\n",
    "        print(\"Dataset: \", ds_bag.dataset_rid)\n",
    "\n",
    "        \n",
    "        output_path_cropped = output_dir / ds_bag.dataset_rid / \"output_cropped\"\n",
    "        output_path_uncropped = output_dir / ds_bag.dataset_rid / \"output_uncropped\"\n",
    "        output_path_cropped.mkdir(parents=True, exist_ok=True)\n",
    "        output_path_uncropped.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    \n",
    "        main(train_path=image_path_ds_bag_path_cropped,\n",
    "           valid_path=validation_image_path_cropped, \n",
    "           test_path=test_image_path_cropped, \n",
    "           output_path = output_path_cropped,\n",
    "           best_hyperparameters_json_path = best_hyper_parameters_json_path,\n",
    "           model_name = f\"VGG19_10_Images_Cropped_No_{index+1}_Jan_27_2025\"\n",
    "           )\n",
    "        \n",
    "        main(train_path=image_path_ds_bag_path_uncropped,\n",
    "           valid_path=validation_image_path_uncropped, \n",
    "           test_path=test_image_path_uncropped, \n",
    "           output_path = output_path_uncropped,\n",
    "           best_hyperparameters_json_path = best_hyper_parameters_json_path,\n",
    "           model_name = f\"VGG19_10_Images_Uncropped_No_{index+1}_Jan_27_2025\"\n",
    "           )\n",
    "\n",
    "    \n",
    "print(\"Finished\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d85f2c9-34dd-4066-91cc-087795136a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567e3236-68ab-48d3-b265-be55f00a3bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ca0b5-204a-461f-bf4a-480ca42212cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crete asset path\n",
    "asset_type_name = \"Model results\"\n",
    "asset_path = execution.execution_asset_path(asset_type_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40623748-c89a-4f81-a9fd-69a0da1fe782",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.upload_execution_outputs(clean_folder=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My TensorFlow (Conda)",
   "language": "python",
   "name": "my-tensorflow-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
