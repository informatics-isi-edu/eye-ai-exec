{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/informatics-isi-edu/eye-ai-exec/blob/main/notebooks/VGG19_Diagnosis_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVq_jMdfx7Ni"
   },
   "source": [
    "# Multimodal analyses: linear regression predicting MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# if IN_COLAB:\n",
    "#     !pip install deriva\n",
    "#     !pip install bdbag\n",
    "#     !pip install --upgrade --force pydantic\n",
    "#     !pip install git+https://github.com/informatics-isi-edu/deriva-ml git+https://github.com/informatics-isi-edu/eye-ai-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = \"Repos\"   # Set this to be where your github repos are located.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Update the load path so python can find modules for the model\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.home() / repo_dir / \"eye-ai-ml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "\n",
    "import json\n",
    "import os\n",
    "from eye_ai.eye_ai import EyeAI\n",
    "import pandas as pd\n",
    "from pathlib import Path, PurePath\n",
    "import logging\n",
    "# import torch\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import the class\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "# Import label encoder \n",
    "from sklearn import preprocessing \n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\n",
    "catalog_id = \"eye-ai\" #@param\n",
    "host = 'www.eye-ai.org'\n",
    "\n",
    "\n",
    "gnl = GlobusNativeLogin(host=host)\n",
    "if gnl.is_logged_in([host]):\n",
    "    print(\"You are already logged in.\")\n",
    "else:\n",
    "    gnl.login([host], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n",
    "    print(\"Login Successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imD3DJ4lx7Nm"
   },
   "source": [
    "Connect to Eye-AI catalog.  Configure to store data local cache and working directories.  Initialize Eye-AI for pending execution based on the provided configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to configure the rest of the notebook.\n",
    "\n",
    "cache_dir = '/data'        # Directory in which to cache materialized BDBags for datasets\n",
    "working_dir = '/data'    # Directory in which to place output files for later upload.\n",
    "\n",
    "configuration_rid= \"2-CCD4\" # rid I created with my config containing minid for both train and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EA = EyeAI(hostname = host, catalog_id = catalog_id, cache_dir= None, working_dir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Initiate an Execution\n",
    "configuration_records = EA.initialize_execution(configuration_rid=configuration_rid)\n",
    "configuration_records.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate multimodal wide table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN: configuration_records.bag_paths[0]\n",
    "wide_train_raw = EA.severity_analysis(configuration_records.bag_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: configuration_records.bag_paths[1]\n",
    "wide_test_raw = EA.severity_analysis(configuration_records.bag_paths[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add age to table\n",
    "age_path = \"/data/yukim3003/EyeAI_working/Execution_Assets/Multimodal_Analysis/multimodal_subject_age.csv\"\n",
    "age_df = pd.read_csv(age_path)\n",
    "age_df.rename(columns={'RID': 'RID_Subject'}, inplace=True)\n",
    "wide_train_raw = wide_train_raw.merge(age_df, on='RID_Subject', how='left')\n",
    "wide_test_raw = wide_test_raw.merge(age_df, on='RID_Subject', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new table with only more severe eye for each patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eye_ai.py:     def pick_severe_eye(self, df, rnfl_threshold, md_threshold):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_train_nothresh = EA.pick_severe_eye(wide_train_raw, 0, 0)\n",
    "wide_test_nothresh = EA.pick_severe_eye(wide_test_raw, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnfl_thresh = 0\n",
    "md_thresh = 0\n",
    "wide_train = EA.pick_severe_eye(wide_train_raw, rnfl_thresh, md_thresh)\n",
    "wide_test = EA.pick_severe_eye(wide_test_raw, rnfl_thresh, md_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show which subjects changed eyes by adding thresholds\n",
    "diff_values = wide_train.compare(wide_train_nothresh, align_axis=0, keep_shape=True, keep_equal=True) #keep_equal=False --> values that are equal are represented as NaN\n",
    "diff_values = diff_values.drop_duplicates(keep=False) # drop rows that have a duplicate\n",
    "print(\"# subjects where eye choice changed: %i\" % (len(diff_values)/2))\n",
    "diff_values[['RID_Subject', 'Side', 'Label', 'Average_RNFL_Thickness(μm)', 'MD', 'CDR']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in features and target variable\n",
    "demographic_fx = ['Gender', 'Ethnicity', 'Age']\n",
    "clinic_fx = ['LogMAR_VA', 'IOP'] # 'Gonioscopy' - mostly NaN, not standardized annotation # CCT - mostly NaN\n",
    "CDR_fx = ['CDR']\n",
    "RNFL_fx = ['Average_RNFL_Thickness(μm)'] # Average_C/D_Ratio - for RNFL-derived CDR\n",
    "RNFL_clockhr_fx = ['Clock_Hours_1', 'Clock_Hours_2', 'Clock_Hours_3', 'Clock_Hours_4', 'Clock_Hours_5', 'Clock_Hours_6', 'Clock_Hours_7', 'Clock_Hours_8', 'Clock_Hours_9', 'Clock_Hours_10', 'Clock_Hours_11', 'Clock_Hours_12'] # if I want to use each clock hour\n",
    "RNFL_quad_fx = ['Quadrants_S', 'Quadrants_N', 'Quadrants_T', 'Quadrants_I']\n",
    "RNFL_IS_fx = ['Quadrants_S', 'Quadrants_I']\n",
    "HVF_fx = ['MD', 'VFI'] # 'PSD' - mostly NaN. I think PSD and PSD.1 columns should be merged to use this column if desired\n",
    "\n",
    "# All Project Fx\n",
    "fx_cols = demographic_fx + clinic_fx + CDR_fx + RNFL_fx + RNFL_IS_fx + HVF_fx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Train and Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transferred to eye_ai.py\n",
    "# def transform_data(multimodal_wide, fx_cols, y_method=\"all_glaucoma\" or \"urgent_glaucoma\"):\n",
    "# Returns: X_transformed, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_keep_missing, _ = EA.transform_data(wide_train, fx_cols)\n",
    "X_test_keep_missing, _ = EA.transform_data(wide_test, fx_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows missing MD\n",
    "X_train_keep_missing.dropna(subset=['MD'], inplace=True)\n",
    "X_test_keep_missing.dropna(subset=['MD'], inplace=True)\n",
    "\n",
    "print(len(X_train_keep_missing))\n",
    "print(len(X_test_keep_missing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Counts / data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train_keep_missing) + len(X_test_keep_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.unique(y_train_keep_missing, return_counts=True)\n",
    "print(counts) # #GS vs #Glaucoma\n",
    "print(\"Percent mild-GS vs mod-severe in TRAIN:\", counts[1] / sum(counts[1])) # percent\n",
    "\n",
    "counts = np.unique(y_test_keep_missing, return_counts=True)\n",
    "print(counts) # #GS vs #Glaucoma\n",
    "print(\"Percent mild-GS vs mod-severe in TEST:\", counts[1] / sum(counts[1])) # percent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = sum(X_train_keep_missing['Gender_M']) + sum(X_test_keep_missing['Gender_M'])\n",
    "print(\"Num male:\", counts)\n",
    "counts = sum(X_train_keep_missing['Gender_F']) + sum(X_test_keep_missing['Gender_F'])\n",
    "print(\"Num female:\", counts)\n",
    "\n",
    "mean_age = (np.sum(X_train_keep_missing['Age']) + np.sum(X_test_keep_missing['Age'])) / (len(X_train_keep_missing) + len(X_test_keep_missing))\n",
    "print(\"Mean age:\", mean_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #NAN\n",
    "### the number of rows with nan in any column will increase if I choose more features\n",
    "\n",
    "# count number / percent of rows with nan value\n",
    "num_rows_with_nan = X_train_keep_missing.isnull().any(axis=1).sum()\n",
    "print (\"Number of train rows with any nan: %i\" % num_rows_with_nan)\n",
    "\n",
    "# Calculate the percentage of rows with NaN values\n",
    "print (\"Percent of train rows with any nan: %f\" % ((num_rows_with_nan / len(X_train_keep_missing)) * 100))\n",
    "\n",
    "# count number / percent of rows with nan value\n",
    "num_rows_with_nan = X_test_keep_missing.isnull().any(axis=1).sum()\n",
    "print (\"Number of test rows with any nan: %i\" % num_rows_with_nan)\n",
    "\n",
    "# Calculate the percentage of rows with NaN values\n",
    "print (\"Percent of test rows with any nan: %f\" % ((num_rows_with_nan / len(X_test_keep_missing)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize Data\n",
    "(NOT REQUIRED FOR LINEAR REGRESSION but makes coefficients easier to interpret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### normalize numeric training data (so that features are on same scale instead of wildly different scales)\n",
    "# not required for typical logistic regression, but do need for regularized regression\n",
    "# I didn't put this in transform_data because I want to use the scaler fitted on train for test too\n",
    "\n",
    "# how? https://datascience.stackexchange.com/questions/54908/data-normalization-before-or-after-train-test-split\n",
    "# why? https://stackoverflow.com/questions/52670012/convergencewarning-liblinear-failed-to-converge-increase-the-number-of-iterati\n",
    "\n",
    "# eye_ai.py: def standardize_data(self, fx_cols, X_train, X_test):\n",
    "\n",
    "X_train_keep_missing, X_test_keep_missing = EA.standardize_data(fx_cols, X_train_keep_missing, X_test_keep_missing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# A) Simple imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat = 'mean'\n",
    "# NOTE: the following code imputes X_test based on the imputer fitted to X_train\n",
    "\n",
    "\"\"\"\n",
    "STRATEGIES\n",
    "If “mean”, then replace missing values using the mean along each column. Can only be used with numeric data.\n",
    "\n",
    "If “median”, then replace missing values using the median along each column. Can only be used with numeric data.\n",
    "\n",
    "If “most_frequent”, then replace missing using the most frequent value along each column. Can be used with strings or numeric data. If there is more than one such value, only the smallest is returned.\n",
    "\n",
    "If “constant”, then replace missing values with fill_value. Can be used with strings or numeric data.\n",
    "\"\"\"\n",
    "\n",
    "# simple imputation fitted to X_train, but also applied to X_test\n",
    "# eye_ai.py: def simple_impute(self, X_train_keep_missing, X_test_keep_missing, strat = \"mean\"):\n",
    "X_train, X_test = EA.simple_impute(X_train_keep_missing, X_test_keep_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# B) Multiple imputations (10 imputations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good article on MCAR vs MAR vs MNAR and how to appropriately handle missing values in each case: https://datascience.stackexchange.com/questions/116622/what-should-you-do-with-nan-values\n",
    "\n",
    "# return list of pandas dataframes, each containing 1 of 10 imputations\n",
    "# eye_ai.py:     def mult_impute_missing(self, X, train_data=None):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputedsets = EA.mult_impute_missing(X_train_keep_missing) # list of 10 imputed X_trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_imputedsets = EA.mult_impute_missing(X_test_keep_missing, train_data=X_test_keep_missing) # Impute test data using model fit with training data, not with test data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C) Drop NA\n",
    "## don't drop until later when choosing specific variables to run, so that you don't drop extra columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_keep_missing\n",
    "X_test = X_test_keep_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transferred to eye_ai.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Linear Regression DROPNA or SIMPLEIMPUTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NORMAL LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_linreg(chosen_fx, X_train, X_test):\n",
    "    # drop NAs (if used simple or multiple imputer, this should also be fine because there shouldn't be anything to drop)\n",
    "    # (do this first so it drops from both X and y)\n",
    "    X_train = X_train.dropna(subset=chosen_fx)\n",
    "    X_test = X_test.dropna(subset=chosen_fx)\n",
    "\n",
    "    y_train = X_train['MD']\n",
    "    y_test = X_test['MD']\n",
    "    X_train = X_train[chosen_fx]\n",
    "    X_test = X_test[chosen_fx]\n",
    "    print(\"X_train length: %i \\n X_test length: %i\" % (len(X_train), len(X_test)))\n",
    "    \n",
    "    # MUST DROP REFERENCE COLUMN FOR ONE-HOT-ENCODED VARIABLES (AVOID DUMMY VARIABLE TRAP)\n",
    "    chosen_ref_labels = ['Gender_M', 'Ethnicity_Other'] \n",
    "    drop_cols = [x for x in X_train.columns if x in chosen_ref_labels]\n",
    "    X_train = X_train.drop(columns=drop_cols)\n",
    "    X_test = X_test.drop(columns=drop_cols)\n",
    "    \n",
    "    linreg = LinearRegression() \n",
    "    \n",
    "    linreg.fit(X_train, y_train)\n",
    "    y_pred = linreg.predict(X_test)\n",
    "\n",
    "    ### Commenting out because statsmodels package prints out same info but more\n",
    "    # Print the coefficients and intercept\n",
    "    #print(\"Coefficients:\", pd.DataFrame({'Coefficient': linreg.coef_}, index=list(X_train.columns)))\n",
    "    #print(\"Intercept:\", linreg.intercept_)\n",
    "    # model evaluation \n",
    "    #print('mean_squared_error : ', metrics.mean_squared_error(y_test, y_pred))\n",
    "    #print('mean_absolute_error : ', metrics.mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "    #print(\"\\nALTERNATIVE STATSMODELS PACKAGE-------------------------\")\n",
    "    import statsmodels.api as sm\n",
    "    X2_train = sm.add_constant(X_train)\n",
    "    model = sm.OLS(y_train, X2_train).fit()\n",
    "    print(model.summary())\n",
    "\n",
    "    # print out of metrics\n",
    "    print(\"\\nR-squared for y_train vs y_pred\", linreg.score(X_train, y_train))\n",
    "    print(\"R-squared for y_test vs y_pred2:\", linreg.score(X_test, y_test)) # because only sklearn computes this\n",
    "    y_pred2 = model.predict(sm.add_constant(X_test))\n",
    "    print('mean_squared_error of test predictions: ', metrics.mean_squared_error(y_test, y_pred2))\n",
    "    print('mean_absolute_error of test predictions: ', metrics.mean_absolute_error(y_test, y_pred2))\n",
    "    print(\"---------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "all_fx = [x for x in X_train.columns if x not in ['MD', 'VFI']] # to choose all features\n",
    "print(\"\\nAll fx\")\n",
    "run_linreg(all_fx, X_train, X_test)\n",
    "all_fx_exceptIS = [x for x in X_train.columns if x not in ['MD', 'VFI']+RNFL_IS_fx] # to choose all features\n",
    "print(\"\\nAll non-redundant fx\")\n",
    "run_linreg(all_fx_exceptIS, X_train, X_test)\n",
    "print(\"\\nDemographics\\n\")\n",
    "demographic_fx_onehot = [x for x in X_train.columns if x not in (['LogMAR_VA', 'IOP', 'CDR','MD', 'VFI']+RNFL_fx+RNFL_IS_fx)]\n",
    "run_linreg(demographic_fx_onehot, X_train, X_test)\n",
    "print(\"\\nCDR + RNFL\\n\")\n",
    "run_linreg(CDR_fx+RNFL_fx, X_train, X_test)\n",
    "print(\"\\nCDR\\n\")\n",
    "run_linreg(CDR_fx, X_train, X_test)\n",
    "print(\"\\nRNFL\\n\")\n",
    "run_linreg(RNFL_fx, X_train, X_test)\n",
    "\n",
    "# Expectation of coefficient signs: IOP-, CDR-, RNFL+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTHING AFTER THIS IS UDPATED #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge and Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Regularization params\n",
    "k_folds = 10 #5-10 standard\n",
    "scoring = 'roc_auc' # 'neg_log_loss', 'neg_brier_score', 'accuracy' (default), 'roc_auc', 'neg_mean_absolute_error' ...options on sklearn.metrics: https://scikit-learn.org/stable/api/sklearn.metrics.html#module-sklearn.metrics\n",
    "max_iter=1000\n",
    "solver='saga'\n",
    "# for elastic net only:\n",
    "lambda_inverse = 20  # of C's (=inverse of lambda) to try; 10 by default\n",
    "alpha_range = np.linspace(0, 1, 20)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Ridge\n",
    "ridge_cv = LogisticRegressionCV(cv=k_folds, scoring=scoring, solver=solver, max_iter=max_iter)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "# Retrieve the best hyperparameters\n",
    "best_C = ridge_cv.C_[0]\n",
    "print(f\"Best C (inverse of regularization strength): {best_C}\")\n",
    "\n",
    "EA.model_summary(ridge_cv, X_train)\n",
    "EA.compute_performance(ridge_cv, X_test, y_test)\n",
    "EA.compute_performance_youden(ridge_cv, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Elastic Net\n",
    "#https://stackoverflow.com/questions/66787845/how-to-perform-elastic-net-for-a-classification-problem\n",
    "# SAGA should be considered more advanced and used over SAG. For more information, see: https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-defintions\n",
    "en_cv = LogisticRegressionCV(cv=k_folds, scoring=scoring, penalty='elasticnet', Cs = lambda_inverse, l1_ratios=alpha_range, solver=solver, max_iter=max_iter)\n",
    "en_cv.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "best_C = en_cv.C_[0]\n",
    "best_l1_ratio = en_cv.l1_ratio_[0]\n",
    "print(f\"Best C (inverse of regularization strength): {best_C}\")\n",
    "print(f\"Best l1_ratio (mixing parameter): {best_l1_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best C (inverse of regularization strength): {best_C}\")\n",
    "print(f\"Best l1_ratio (mixing parameter): {best_l1_ratio}\")\n",
    "EA.model_summary(en_cv, X_train)\n",
    "EA.compute_performance(en_cv, X_test, y_test)\n",
    "EA.compute_performance_youden(en_cv, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Multivariate Logistic Regression MULTIPLE IMPUTATIONS\n",
    "### To check if what I did is best method: used mode of y_pred, and averaged prediction probabilities of each imputed model to determine AUC, and averaged p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eye_ai.py: \n",
    "#     def compute_performance_mice(self, logreg_models, Xtest_finals, y_test):\n",
    "#     def model_summary_mice(self, logreg_models, Xtrain_finals):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to do prediction after multiple imputation:\n",
    "# https://github.com/amices/mice/issues/82\n",
    "# https://stackoverflow.com/questions/68460923/how-to-do-the-prediction-after-multiple-imputation-with-mice-package\n",
    "logreg_models = []\n",
    "Xtrain_finals = []\n",
    "Xtest_finals = []\n",
    "\n",
    "# MUST DROP REFERENCE COLUMN FOR ONE-HOT-ENCODED VARIABLES\n",
    "#chosen_ref_labels = ['GHT_Within Normal Limits', 'Gender_M', 'Ethnicity_Other']\n",
    "chosen_ref_labels = ['GHT_Within Normal Limits','GHT_Borderline', 'Gender_M', 'Ethnicity_Other']\n",
    "penalty=None#'l1', 'l2', 'elasticnet', or None\n",
    "solver='saga' # 'lbfgs', 'saga' (only saga supports l1 and elasticnet)\n",
    "\n",
    "for X_train, X_test in zip(X_train_imputedsets, X_test_imputedsets):\n",
    "    # NORMAL LOGISTIC REGRESSION\n",
    "    drop_cols = [x for x in X_train.columns if x in chosen_ref_labels]\n",
    "    X_train_dropped = X_train.drop(columns=drop_cols)\n",
    "    X_test_dropped = X_test.drop(columns=drop_cols)\n",
    "\n",
    "    logreg = LogisticRegression(random_state=16, solver=solver, max_iter=1000, penalty=penalty)\n",
    "    logreg.fit(X_train_dropped, y_train)\n",
    "    logreg_models.append(logreg)\n",
    "\n",
    "    Xtrain_finals.append(X_train_dropped)\n",
    "    Xtest_finals.append(X_test_dropped)\n",
    "\n",
    "EA.model_summary_mice(logreg_models, Xtrain_finals)\n",
    "EA.compute_performance_mice(logreg_models, Xtest_finals, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Alternative models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't have to onehotencode, but xgboost performs better if does\n",
    "# keep dummy variables, don't drop ref label for decision trees\n",
    "\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "drop_NA=True\n",
    "if drop_NA:\n",
    "    # Drop NA if desired\n",
    "    x = X_train_keep_missing.dropna()\n",
    "    x_t = X_test_keep_missing.dropna()\n",
    "\n",
    "    y = y_train[y_train.index.isin(x.index)]\n",
    "    y_t = y_test[y_test.index.isin(x_t.index)]\n",
    "\n",
    "print(x.columns)\n",
    "\n",
    "#model = BaggingClassifier(estimator=SVC(), n_estimators=10, random_state=0) # bagged SVC\n",
    "#model=BaggingClassifier() # bagged decision trees (bc DecisionTree is default)\n",
    "model=SVC(probability=True) # probability=True to enable predict_proba function (slow)\n",
    "clf = model.fit(x,y)\n",
    "\n",
    "# define cross-validation evaluation procedure\n",
    "k = 10\n",
    "cv = RepeatedStratifiedKFold(n_splits=k, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, x, y, scoring='roc_auc', cv=cv)\n",
    "# summarize performance\n",
    "print('Mean AUC using %i-fold cross-validation: %.3f' % (k, mean(scores)))# AUC from 10-fold cv on TRAINING set, as opposed to AUC on test set computed in compute_performance -- if this better than AUC for test set, then model probably overfit\n",
    "print(\"\")\n",
    "\n",
    "# test performance\n",
    "EA.compute_performance(clf, x_t, y_t)\n",
    "EA.compute_performance_youden(clf, x_t, y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT multiple ROC curves\n",
    "- current version of this code requires running above multiple times for each roc curve I want to plot, then saving them manually and adding to global dictionary before plotting combined ROC curve\n",
    "- X_test and y_test have different #s for drop_NA bc drop_NA may drop diff # rows depending on which variables are included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models = {} # model label name: (model, associated X_test, associated y_test)\n",
    "# start with univarate models dict\n",
    "models ={**models, **models_univariate} ## don't overwrite models just in case already contains stuff\n",
    "# map univariate model names\n",
    "key_mapping = {\n",
    "    'Average_RNFL_Thickness(μm)': 'OCT',\n",
    "    'MD': 'HVF',\n",
    "    'ML Feature Selection (Elastic Net)': 'ML Elastic Net'\n",
    "}\n",
    "# Function to rename keys in a dictionary\n",
    "def rename_keys(d, key_map):\n",
    "    return {key_map.get(k, k): v for k, v in d.items()}\n",
    "# Apply the renaming function to the dictionary\n",
    "models = rename_keys(models, key_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual model additions -- EDIT THE NAME AND MODEL NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Manual model additions\n",
    "name = \"ML Elastic Net\" # \"Demographics\"\n",
    "mod = logreg #en_cv for elastic net model\n",
    "models[name] = (mod, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## how to combine 2 dictionaries\n",
    "#all_models = {**models_univariate, **selected_models}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select which models to plot\n",
    "wanted_keys = ['Demographics', 'LogMAR_VA', 'CDR', 'OCT', 'CDR+OCT', 'All Significant Features', 'ML Elastic Net'] # The keys you want\n",
    "selected_models = dict((k, models[k]) for k in wanted_keys if k in models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 8))\n",
    "for name, (m, xt, yt) in selected_models.items():\n",
    "    print (name)\n",
    "    fpr, tpr, auc, optimal_idx, optimal_threshold = EA.compute_performance_youden(m, xt, yt, plot=False)\n",
    "    #plt.plot(fpr, tpr, label=\"%s (AUC=%s, Youden's=%.3f)\" % (name, auc, (tpr[optimal_idx] - fpr[optimal_idx])))\n",
    "    plt.plot(fpr, tpr, label=\"%s (AUC=%s)\" % (name, auc))\n",
    "    #plt.scatter(fpr[optimal_idx], tpr[optimal_idx], marker='o', color='red')\n",
    "    print (\"\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(\"/home/yukim3003/Figure_1_urgent_glaucoma.png\", format=\"png\", dpi=300)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a high quality plot - nvm this has to be in same cell as original plot creation to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(configuration_records.working_dir/'Execution_Assets/Multimodal_Figures/')\n",
    "fig_path = configuration_records.working_dir/'Execution_Assets/Multimodal_Figures/Figure_1_urgent_glaucoma.png'\n",
    "\n",
    "# Save the plot with higher DPI\n",
    "plt.savefig(fig_path, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workaround\n",
    "plt.savefig(\"/home/yukim3003/Figure_1_urgent_glaucoma.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "cache_path = configuration_records.working_dir/'Execution_Assets/Multimodal_Analysis/models_cache_urgent_glaucoma.pkl'\n",
    "\n",
    "# Cache the models dictionary to a file\n",
    "with open(cache_path, 'wb') as f:\n",
    "    pickle.dump(models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the cached models dictionary later\n",
    "with open(cache_path, 'rb') as f:\n",
    "    cached_models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access a specific saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a specific model that is saved\n",
    "name = \"ML Elastic Net\"\n",
    "m, xt, yt = models[name]\n",
    "EA.compute_performance(m, xt, yt)\n",
    "EA.compute_performance_youden(m, xt, yt, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_assets = EA.execution_upload(configuration_records.execution_rid, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
