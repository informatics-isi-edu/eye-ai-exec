{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/informatics-isi-edu/eye-ai-exec/blob/main/notebooks/VGG19_Diagnosis_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVq_jMdfx7Ni"
   },
   "source": [
    "# Multimodal Initial analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0B5DczZgx7Nl"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# if IN_COLAB:\n",
    "#     !pip install deriva\n",
    "#     !pip install bdbag\n",
    "#     !pip install --upgrade --force pydantic\n",
    "#     !pip install git+https://github.com/informatics-isi-edu/deriva-ml git+https://github.com/informatics-isi-edu/eye-ai-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = \"Repos\"   # Set this to be where your github repos are located.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Update the load path so python can find modules for the model\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.home() / repo_dir / \"eye-ai-ml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgmzhX4Fx7Nm"
   },
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "\n",
    "import json\n",
    "import os\n",
    "from eye_ai.eye_ai import EyeAI\n",
    "import pandas as pd\n",
    "from pathlib import Path, PurePath\n",
    "import logging\n",
    "# import torch\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Import label encoder \n",
    "from sklearn import preprocessing \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qw-bW4bORlqQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\n",
    "catalog_id = \"eye-ai\" #@param\n",
    "host = 'www.eye-ai.org'\n",
    "\n",
    "\n",
    "gnl = GlobusNativeLogin(host=host)\n",
    "if gnl.is_logged_in([host]):\n",
    "    print(\"You are already logged in.\")\n",
    "else:\n",
    "    gnl.login([host], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n",
    "    print(\"Login Successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imD3DJ4lx7Nm"
   },
   "source": [
    "Connect to Eye-AI catalog.  Configure to store data local cache and working directories.  Initialize Eye-AI for pending execution based on the provided configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5U3w6SPx7Nn"
   },
   "outputs": [],
   "source": [
    "# Variables to configure the rest of the notebook.\n",
    "\n",
    "cache_dir = '/data'        # Directory in which to cache materialized BDBags for datasets\n",
    "working_dir = '/data'    # Directory in which to place output files for later upload.\n",
    "\n",
    "configuration_rid=\"2-CC3W\" # rid I created\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KkHsSCJXx7Nn"
   },
   "outputs": [],
   "source": [
    "EA = EyeAI(hostname = host, catalog_id = catalog_id, cache_dir= cache_dir, working_dir=working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "kCIfOvbUXTGB"
   },
   "outputs": [],
   "source": [
    "# @title Initiate an Execution\n",
    "configuration_records = EA.execution_init(configuration_rid=configuration_rid)\n",
    "configuration_records.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyses using multimodal_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old method using local files -- NOT recommended\n",
    "# multimodal_wide_path = \"/data/yukim3003/EyeAI_working/Execution_Assets/Multimodal_Analysis/wide_multimodal_full.csv\"\n",
    "# multimodal_wide = pd.read_csv(multimodal_wide_path)\n",
    "# multimodal_wide\n",
    "\n",
    "### for some reason this file is still the old file Aug 5, not Aug 16 file. I will just work with this table for now to set up the code and can swap it out later once that bug gets fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_wide.loc[multimodal_wide['RID_Subject']==\"2-7KWE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_wide['Average_RNFL_Thickness(μm)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new table with only more severe eye for each patient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in features and target variable\n",
    "demographic_fx = ['Gender', 'Ethnicity']\n",
    "clinic_fx = ['LogMAR_VA', 'IOP', 'CDR'] # 'Gonioscopy' - mostly NaN, not standardized annotation # CCT - mostly NaN\n",
    "HVF_fx = ['MD', 'VFI'] # 'PSD' - mostly NaN. I think PSD and PSD.1 columns should be merged to use this column if desired\n",
    "RNFL_fx = ['Average_RNFL_Thickness(μm)'] # Average_C/D_Ratio - for RNFL-derived CDR\n",
    "RNFL_clockhr_fx = ['Clock_Hours_1', 'Clock_Hours_2', 'Clock_Hours_3', 'Clock_Hours_4', 'Clock_Hours_5', 'Clock_Hours_6', 'Clock_Hours_7', 'Clock_Hours_8', 'Clock_Hours_9', 'Clock_Hours_10', 'Clock_Hours_11', 'Clock_Hours_12'] # if I want to use each clock hour\n",
    "RNFL_quad_fx = ['Quadrants_S', 'Quadrants_N', 'Quadrants_T', 'Quadrants_I']\n",
    "\n",
    "fx_cols = demographic_fx + clinic_fx + HVF_fx + RNFL_fx # selected feature cols from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows missing label\n",
    "multimodal_wide = multimodal_wide.dropna(subset=['Label'])\n",
    "multimodal_wide.Label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = multimodal_wide[fx_cols] # Features\n",
    "y = multimodal_wide.Label # Target variable\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical data: encode using LabelEncoder or OneHotEncoder\n",
    "# label encoder if data ordinal (ie ranked) -- jk nvm This transformer should be used to encode target values, i.e. y, and not the input X!\n",
    "# one-hot if data not ranked; note this will increase dimensionality of data which is bad if >1/3rd of fx are one-hot\n",
    "# https://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor\n",
    "#one_hot_encoder = preprocessing.OneHotEncoder(handle_unknown='ignore')\n",
    "from feature_engine.encoding import OneHotEncoder # this instead of skLearn allows me to one hot encode desired columns only\n",
    "\n",
    "encoder = OneHotEncoder(variables = ['Gender', 'Ethnicity'])\n",
    "X_transformed = encoder.fit_transform(X)\n",
    "X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format numerical data\n",
    "\n",
    "# VFI\n",
    "X_transformed['VFI'] = X_transformed['VFI'].replace('Off', np.nan) # replace \"Off\" with nan\n",
    "def convert_percent(x):\n",
    "    if pd.isnull(x):\n",
    "        return np.nan\n",
    "    return float(x.strip('%'))/100\n",
    "X_transformed['VFI'] = X_transformed['VFI'].map(convert_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format y\n",
    "# combine PACG and POAG as glaucoma\n",
    "y = y.replace(['POAG', 'PACG'], 'Glaucoma')\n",
    "# convert to 0 and 1\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "y[:] = label_encoder.fit_transform(y) # fit_transform combines fit and transform\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# Xu:\n",
    "# - In the past, we’ve used multiple imputation as long as the % of missing values was less than 10% for any given variable. I attached a paper we wrote where we used this technique. \n",
    "# - Balancing can be done by upsampling the minority class, although in this case the two are fairly similar in number.\"\n",
    "# https://scikit-learn.org/stable/modules/impute.html\n",
    "\n",
    "# temp simple imputation method\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer = imputer.fit(X_transformed)\n",
    "X_transformed[:] = imputer.transform(X_transformed) # [:] modifies the df in place\n",
    "X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.25, random_state=16)\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression(random_state=16, solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model performance\n",
    "# https://medium.com/javarevisited/evaluating-the-logistic-regression-ae2decf42d61\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "# evaluate predictions\n",
    "mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "print('MAE: %.3f' % mae)\n",
    "\n",
    "# examine the class distribution of the testing set (using a Pandas Series method)\n",
    "y_test.value_counts()\n",
    "\n",
    "# calculate the percentage of ones\n",
    "# because y_test only contains ones and zeros, we can simply calculate the mean = percentage of ones\n",
    "y_test.mean()\n",
    "\n",
    "# calculate the percentage of zeros\n",
    "1 - y_test.mean()\n",
    "\n",
    "\n",
    "# # Metrics computed from a confusion matrix (before thresholding)\n",
    "\n",
    "# Confusion matrix is used to evaluate the correctness of a classification model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,y_pred)\n",
    "confusion_matrix\n",
    "\n",
    "TP = confusion_matrix[1, 1]\n",
    "TN = confusion_matrix[0, 0]\n",
    "FP = confusion_matrix[0, 1]\n",
    "FN = confusion_matrix[1, 0]\n",
    "\n",
    "# Classification Accuracy: Overall, how often is the classifier correct?\n",
    "# use float to perform true division, not integer division\n",
    "# print((TP + TN) / sum(map(sum, confusion_matrix))) -- this is is the same as the below automatic method\n",
    "print('Accuracy: %.3f' % metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Sensitivity(recall): When the actual value is positive, how often is the prediction correct?\n",
    "sensitivity = TP / float(FN + TP)\n",
    "\n",
    "print('Sensitivity: %.3f' % sensitivity)\n",
    "print('Recall score: %.3f' % metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "# Specificity: When the actual value is negative, how often is the prediction correct?\n",
    "specificity = TN / (TN + FP)\n",
    "print('Specificity: %.3f' % specificity)\n",
    "\n",
    "#from imblearn.metrics import specificity_score\n",
    "#specificity_score(y_test, y_pred)\n",
    "\n",
    "# False Positive Rate: When the actual value is negative, how often is the prediction incorrect?\n",
    "false_positive_rate = FP / float(TN + FP)\n",
    "print('FPR: %.3f' % false_positive_rate)\n",
    "# print(1 - specificity) # same as FPR\n",
    "\n",
    "# Precision: When a positive value is predicted, how often is the prediction correct?\n",
    "precision = TP / float(TP + FP)\n",
    "print('Precision: %.3f' % precision)\n",
    "# print('Precision score: %.3f' % metrics.precision_score(y_test, y_pred))\n",
    "\n",
    "# F score\n",
    "f_score = 2*TP / (2*TP + FP + FN)\n",
    "# print('F score: %.3f' % f_score)\n",
    "print('F1 score: %.3f' % metrics.f1_score(y_test,y_pred))\n",
    "\n",
    "#Evaluate the model using other performance metrics\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "from sklearn import metrics\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = None)\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = logreg.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# template stuff I haven't deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View data\n",
    "\n",
    "# subject = pd.read_csv(configuration_records.bag_paths[0]/'data/Subject.csv')\n",
    "# subject\n",
    "\n",
    "# observation = pd.read_csv(configuration_records.bag_paths[0]/'data/Observation.csv')\n",
    "# observation\n",
    "\n",
    "# clinic = pd.read_csv(configuration_records.bag_paths[0]/'data/Clinical_Records.csv')\n",
    "# clinic\n",
    "\n",
    "# observation_clinic_asso = pd.read_csv(configuration_records.bag_paths[0]/'data/Observation_Clinic_Asso.csv')\n",
    "# observation_clinic_asso # association table between observation table and clinic record table\n",
    "\n",
    "# icd10 = pd.read_csv(configuration_records.bag_paths[0]/'data/Clinic_ICD10.csv')\n",
    "# icd10\n",
    "\n",
    "# icd10_asso = pd.read_csv(configuration_records.bag_paths[0]/'data/Clinic_ICD_Asso.csv')\n",
    "# icd10_asso # association table between clinic record table and ICD10 code\n",
    "\n",
    "# report = pd.read_csv(configuration_records.bag_paths[0]/'data/Report.csv')\n",
    "# report\n",
    "\n",
    "# RNFL_OCR = pd.read_csv(configuration_records.bag_paths[0]/'data/RNFL_OCR.csv')\n",
    "# RNFL_OCR\n",
    "\n",
    "HVF_OCR = pd.read_csv(configuration_records.bag_paths[0]/'data/HVF_OCR.csv')\n",
    "HVF_OCR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIJj6Uj73sOm"
   },
   "outputs": [],
   "source": [
    "# @title Execute Training algorithm\n",
    "from eye_ai.models.vgg19_hyper_parameter_tuning import main #import the new logistic module.\n",
    "with EA.execution(execution_rid=configuration_records.execution_rid) as exec:\n",
    "  main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHqtgNpxrISs"
   },
   "outputs": [],
   "source": [
    "# @title Save Execution Assets (model) and Metadata\n",
    "uploaded_assets = EA.execution_upload(configuration_records.execution_rid, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "My TensorFlow (Conda)",
   "language": "python",
   "name": "my-tensorflow-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
