{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/informatics-isi-edu/eye-ai-exec/blob/main/notebooks/VGG19_Diagnosis_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVq_jMdfx7Ni"
   },
   "source": [
    "# Multimodal Initial analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# if IN_COLAB:\n",
    "#     !pip install deriva\n",
    "#     !pip install bdbag\n",
    "#     !pip install --upgrade --force pydantic\n",
    "#     !pip install git+https://github.com/informatics-isi-edu/deriva-ml git+https://github.com/informatics-isi-edu/eye-ai-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = \"Repos\"   # Set this to be where your github repos are located.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Update the load path so python can find modules for the model\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.home() / repo_dir / \"eye-ai-ml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "\n",
    "import json\n",
    "import os\n",
    "from eye_ai.eye_ai import EyeAI\n",
    "import pandas as pd\n",
    "from pathlib import Path, PurePath\n",
    "import logging\n",
    "# import torch\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Import label encoder \n",
    "from sklearn import preprocessing \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\n",
    "catalog_id = \"eye-ai\" #@param\n",
    "host = 'www.eye-ai.org'\n",
    "\n",
    "\n",
    "gnl = GlobusNativeLogin(host=host)\n",
    "if gnl.is_logged_in([host]):\n",
    "    print(\"You are already logged in.\")\n",
    "else:\n",
    "    gnl.login([host], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n",
    "    print(\"Login Successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imD3DJ4lx7Nm"
   },
   "source": [
    "Connect to Eye-AI catalog.  Configure to store data local cache and working directories.  Initialize Eye-AI for pending execution based on the provided configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to configure the rest of the notebook.\n",
    "\n",
    "cache_dir = '/data'        # Directory in which to cache materialized BDBags for datasets\n",
    "working_dir = '/data'    # Directory in which to place output files for later upload.\n",
    "\n",
    "configuration_rid=\"2-CC3W\" # rid I created\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EA = EyeAI(hostname = host, catalog_id = catalog_id, cache_dir= cache_dir, working_dir=working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Initiate an Execution\n",
    "configuration_records = EA.execution_init(configuration_rid=configuration_rid)\n",
    "configuration_records.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate multimodal wide table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old method using local files -- NOT recommended\n",
    "# multimodal_wide_path = \"/data/yukim3003/EyeAI_working/Execution_Assets/Multimodal_Analysis/wide_multimodal_full.csv\"\n",
    "# multimodal_wide = pd.read_csv(multimodal_wide_path)\n",
    "# multimodal_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN: configuration_records.bag_paths[0]\n",
    "wide_train_raw = EA.severity_analysis(configuration_records.bag_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: configuration_records.bag_paths[1]\n",
    "# TRAIN: configuration_records.bag_paths[0]\n",
    "wide_test_raw = EA.severity_analysis(configuration_records.bag_paths[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new table with only more severe eye for each patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old method with bugs:\n",
    "#1. if eye1 is GS and eye2 is NaN, then eye2 also becomes GS; and similarly if eye1 has CDR 0.9 but eye2 is NaN, then eye2 also gets CDR 0.9. Fixed this by adding skipna=False to first()\n",
    "#2. if eye1 has an RNFL but eye2 RNFL is NaN, then this method will consider eye1 to be more severe, whereas it's better to move on to assessing MD in that case. Fixing this fixed 11 eyes\n",
    "\n",
    "#def pick_severe_eye(df):    \n",
    "#    # Sort by RNFL, HVF, CDR whereby first row is most severe\n",
    "#    df = df.sort_values(by=['Average_RNFL_Thickness(μm)', 'MD', 'CDR'], ascending=[True, True, False])\n",
    "#    \n",
    "#    # Group by subject and get the first row in each group. If all tied, will just pick the first eye - ie the right eye\n",
    "#    return df.groupby('RID_Subject').first(skipna=False).reset_index() # first computes the first entry of each column within each group, but NaN's dont count as a value; so if one eye has NaN for any random column, then the value for the other eye is transferred to that eye\n",
    "\n",
    "#wide_train = pick_severe_eye(wide_train_raw)\n",
    "#wide_test = pick_severe_eye(wide_test_raw)\n",
    "\n",
    "# the row that made me realize the bug requiring skipna in first() # if I did want to apply the label of any eye to both eyes, this could be useful\n",
    "# wide_train[wide_train['RID_Subject']=='2-7KVA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current severity rule: prioritize RNFL > HVF > CDR\n",
    "# if don't want thresholds, just make threshold 0\n",
    "# just return the first eye if RNFL, MD, CDR all NaN\n",
    "def pick_severe_eye(df, rnfl_threshold, md_threshold):\n",
    "    # Sort by 'Average_RNFL_Thickness(μm)', 'MD', and 'CDR' in descending order\n",
    "    df = df.sort_values(by=['Average_RNFL_Thickness(μm)', 'MD', 'CDR'], ascending=[True, True, False])\n",
    "    \n",
    "    # Custom function to select the row with the most severe value within the thresholds\n",
    "    def select_row(group):\n",
    "        max_value = group['Average_RNFL_Thickness(μm)'].min() # min is more severe for RNFL\n",
    "        within_value_threshold = group[np.abs(group['Average_RNFL_Thickness(μm)'] - max_value) <= rnfl_threshold] # identify eyes within threshold\n",
    "\n",
    "        if len(within_value_threshold) > 1 or len(within_value_threshold) == 0: # if both eyes \"equal\" rnfl OR if RNFL is NaN, then try MD\n",
    "            max_other_column = within_value_threshold['MD'].min() # min is more severe for MD\n",
    "            within_other_column_threshold = within_value_threshold[np.abs(within_value_threshold['MD'] - max_other_column) <= md_threshold]\n",
    "\n",
    "            if len(within_other_column_threshold) > 1 or len(within_other_column_threshold) == 0: # if both eyes \"equal\" MD OR if MD is NaN, then try CDR\n",
    "                return group.sort_values(by=['CDR'], ascending=[False]).iloc[0] # since i didn't set CDR threshold, this will always pick something (even if NaN)\n",
    "            else:\n",
    "                return within_other_column_threshold.iloc[0]\n",
    "        else:\n",
    "            return within_value_threshold.iloc[0]\n",
    "                \n",
    "    # Apply the custom function to each group\n",
    "    return df.groupby('RID_Subject').apply(select_row).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_train_nothresh = pick_severe_eye(wide_train_raw, 0, 0)\n",
    "wide_test_nothresh = pick_severe_eye(wide_test_raw, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnfl_thresh = 4\n",
    "md_thresh = 2\n",
    "wide_train = pick_severe_eye(wide_train_raw, rnfl_thresh, md_thresh)\n",
    "wide_test = pick_severe_eye(wide_test_raw, rnfl_thresh, md_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show which subjects changed eyes by adding thresholds\n",
    "diff_values = wide_train.compare(wide_train_nothresh, align_axis=0, keep_shape=True, keep_equal=True) #keep_equal=False --> values that are equal are represented as NaN\n",
    "diff_values = diff_values.drop_duplicates(keep=False) # drop rows that have a duplicate\n",
    "print(\"# subjects where eye choice changed: %i\" % (len(diff_values)/2))\n",
    "diff_values[['RID_Subject', 'Side', 'Label', 'Average_RNFL_Thickness(μm)', 'MD', 'CDR']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Train and Test Data and Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in features and target variable\n",
    "demographic_fx = ['Gender', 'Ethnicity']\n",
    "clinic_fx = ['LogMAR_VA', 'IOP', 'CDR'] # 'Gonioscopy' - mostly NaN, not standardized annotation # CCT - mostly NaN\n",
    "HVF_fx = ['MD', 'VFI'] # 'PSD' - mostly NaN. I think PSD and PSD.1 columns should be merged to use this column if desired\n",
    "RNFL_fx = ['Average_RNFL_Thickness(μm)'] # Average_C/D_Ratio - for RNFL-derived CDR\n",
    "RNFL_clockhr_fx = ['Clock_Hours_1', 'Clock_Hours_2', 'Clock_Hours_3', 'Clock_Hours_4', 'Clock_Hours_5', 'Clock_Hours_6', 'Clock_Hours_7', 'Clock_Hours_8', 'Clock_Hours_9', 'Clock_Hours_10', 'Clock_Hours_11', 'Clock_Hours_12'] # if I want to use each clock hour\n",
    "RNFL_quad_fx = ['Quadrants_S', 'Quadrants_N', 'Quadrants_T', 'Quadrants_I']\n",
    "\n",
    "fx_cols = demographic_fx + clinic_fx + HVF_fx + RNFL_fx # selected feature cols from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to transfer data, to apply to wide_train and wide_test\n",
    "def transform_data(multimodal_wide):\n",
    "    ### drop rows missing label (ie no label for POAG vs PACG vs GS)\n",
    "    multimodal_wide = multimodal_wide.dropna(subset=['Label'])\n",
    "    # drop rows where label is \"Other\" (should only be PACG, POAG, or GS)\n",
    "    allowed_labels = [\"PACG\", \"POAG\", \"GS\"]\n",
    "    multimodal_wide = multimodal_wide[multimodal_wide['Label'].isin(allowed_labels)]\n",
    "\n",
    "    X = multimodal_wide[fx_cols] # Features\n",
    "    y = multimodal_wide.Label # Target variable\n",
    "\n",
    "    ### categorical data: encode using LabelEncoder or OneHotEncoder\n",
    "    # label encoder if data ordinal (ie ranked) -- jk nvm this transformer should be used to encode target values, i.e. y, and not the input X!\n",
    "    # one-hot if data not ranked; note this will increase dimensionality of data which is bad if >1/3rd of fx are one-hot\n",
    "    # https://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor\n",
    "    #one_hot_encoder = preprocessing.OneHotEncoder(handle_unknown='ignore')\n",
    "    from feature_engine.encoding import OneHotEncoder # this instead of skLearn allows me to one hot encode desired columns only\n",
    "    categorical_vars = ['Gender', 'Ethnicity']\n",
    "    encoder = OneHotEncoder(variables = categorical_vars)\n",
    "    X_transformed = encoder.fit_transform(X)\n",
    "\n",
    "    ### sort categorical encoded columns so that they're in alphabetical order\n",
    "    def sort_cols(X, var):\n",
    "        # Select the subset of columns to sort\n",
    "        subset_columns = [col for col in X.columns if col.startswith(var)]\n",
    "        # Sort the subset of columns alphabetically\n",
    "        sorted_columns = sorted(subset_columns)\n",
    "        # Reorder the DataFrame based on the sorted columns\n",
    "        sorted_df = X[[col for col in X.columns if col not in subset_columns] + sorted_columns]\n",
    "        return sorted_df\n",
    "    for var in categorical_vars:\n",
    "        X_transformed = sort_cols(X_transformed, var)\n",
    "\n",
    "    ### format numerical data\n",
    "    # VFI\n",
    "    X_transformed['VFI'] = X_transformed['VFI'].replace('Off', np.nan) # replace \"Off\" with nan\n",
    "    def convert_percent(x):\n",
    "        if pd.isnull(x):\n",
    "            return np.nan\n",
    "        return float(x.strip('%'))/100\n",
    "    X_transformed['VFI'] = X_transformed['VFI'].map(convert_percent)\n",
    "\n",
    "    ### format y\n",
    "    # combine PACG and POAG as glaucoma\n",
    "    y = y.replace(['POAG', 'PACG'], 'Glaucoma')\n",
    "    # convert to 0 and 1\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    y[:] = label_encoder.fit_transform(y) # fit_transform combines fit and transform\n",
    "    y = y.astype(int)\n",
    "\n",
    "    return X_transformed, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_keep_missing, y_train = transform_data(wide_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing(X_transformed):\n",
    "    ### Handle missing values\n",
    "    # Xu:\n",
    "    # - In the past, we’ve used multiple imputation as long as the % of missing values was less than 10% for any given variable. I attached a paper we wrote where we used this technique. \n",
    "    # - Balancing can be done by upsampling the minority class, although in this case the two are fairly similar in number.\"\n",
    "    # https://scikit-learn.org/stable/modules/impute.html\n",
    "    \n",
    "    ## temp simple imputation method\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imputer = imputer.fit(X_transformed)\n",
    "    X_transformed[:] = imputer.transform(X_transformed) # [:] modifies the df in place\n",
    "\n",
    "    return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = handle_missing(X_train_keep_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat transform and handle missing for test\n",
    "X_test_keep_missing, y_test = transform_data(wide_test)\n",
    "X_test = handle_missing(X_test_keep_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression(random_state=16, solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model performance\n",
    "# https://medium.com/javarevisited/evaluating-the-logistic-regression-ae2decf42d61\n",
    "\n",
    "print('Training set count: %i' % len(X_train))\n",
    "print('Test set count: %i' % len(X_test))\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "# evaluate predictions\n",
    "mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "print('MAE: %.3f' % mae)\n",
    "\n",
    "# examine the class distribution of the testing set (using a Pandas Series method)\n",
    "y_test.value_counts()\n",
    "\n",
    "# calculate the percentage of ones\n",
    "# because y_test only contains ones and zeros, we can simply calculate the mean = percentage of ones\n",
    "y_test.mean()\n",
    "\n",
    "# calculate the percentage of zeros\n",
    "1 - y_test.mean()\n",
    "\n",
    "\n",
    "# # Metrics computed from a confusion matrix (before thresholding)\n",
    "\n",
    "# Confusion matrix is used to evaluate the correctness of a classification model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,y_pred)\n",
    "confusion_matrix\n",
    "\n",
    "TP = confusion_matrix[1, 1]\n",
    "TN = confusion_matrix[0, 0]\n",
    "FP = confusion_matrix[0, 1]\n",
    "FN = confusion_matrix[1, 0]\n",
    "\n",
    "# Classification Accuracy: Overall, how often is the classifier correct?\n",
    "# use float to perform true division, not integer division\n",
    "# print((TP + TN) / sum(map(sum, confusion_matrix))) -- this is is the same as the below automatic method\n",
    "print('Accuracy: %.3f' % metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Sensitivity(recall): When the actual value is positive, how often is the prediction correct?\n",
    "sensitivity = TP / float(FN + TP)\n",
    "\n",
    "print('Sensitivity: %.3f' % sensitivity)\n",
    "# print('Recall score: %.3f' % metrics.recall_score(y_test, y_pred)) # same thing as sensitivity, but recall term used in ML\n",
    "\n",
    "# Specificity: When the actual value is negative, how often is the prediction correct?\n",
    "specificity = TN / (TN + FP)\n",
    "print('Specificity: %.3f' % specificity)\n",
    "\n",
    "#from imblearn.metrics import specificity_score\n",
    "#specificity_score(y_test, y_pred)\n",
    "\n",
    "# False Positive Rate: When the actual value is negative, how often is the prediction incorrect?\n",
    "false_positive_rate = FP / float(TN + FP)\n",
    "print('FPR: %.3f' % false_positive_rate)\n",
    "# print(1 - specificity) # same as FPR\n",
    "\n",
    "# Precision: When a positive value is predicted, how often is the prediction correct?\n",
    "precision = TP / float(TP + FP)\n",
    "#print('Precision: %.3f' % precision)\n",
    "print('Precision: %.3f' % metrics.precision_score(y_test, y_pred))\n",
    "\n",
    "# F score\n",
    "f_score = 2*TP / (2*TP + FP + FN)\n",
    "#print('F score: %.3f' % f_score)\n",
    "print('F1 score: %.3f' % metrics.f1_score(y_test,y_pred))\n",
    "\n",
    "#Evaluate the model using other performance metrics\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "from sklearn import metrics\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = None)\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = logreg.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# template stuff I haven't deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View data\n",
    "\n",
    "# subject = pd.read_csv(configuration_records.bag_paths[0]/'data/Subject.csv')\n",
    "# subject\n",
    "\n",
    "# observation = pd.read_csv(configuration_records.bag_paths[0]/'data/Observation.csv')\n",
    "# observation\n",
    "\n",
    "# clinic = pd.read_csv(configuration_records.bag_paths[0]/'data/Clinical_Records.csv')\n",
    "# clinic\n",
    "\n",
    "# observation_clinic_asso = pd.read_csv(configuration_records.bag_paths[0]/'data/Observation_Clinic_Asso.csv')\n",
    "# observation_clinic_asso # association table between observation table and clinic record table\n",
    "\n",
    "# icd10 = pd.read_csv(configuration_records.bag_paths[0]/'data/Clinic_ICD10.csv')\n",
    "# icd10\n",
    "\n",
    "# icd10_asso = pd.read_csv(configuration_records.bag_paths[0]/'data/Clinic_ICD_Asso.csv')\n",
    "# icd10_asso # association table between clinic record table and ICD10 code\n",
    "\n",
    "# report = pd.read_csv(configuration_records.bag_paths[0]/'data/Report.csv')\n",
    "# report\n",
    "\n",
    "# RNFL_OCR = pd.read_csv(configuration_records.bag_paths[0]/'data/RNFL_OCR.csv')\n",
    "# RNFL_OCR\n",
    "\n",
    "HVF_OCR = pd.read_csv(configuration_records.bag_paths[0]/'data/HVF_OCR.csv')\n",
    "HVF_OCR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Execute Training algorithm\n",
    "from eye_ai.models.vgg19_hyper_parameter_tuning import main #import the new logistic module.\n",
    "with EA.execution(execution_rid=configuration_records.execution_rid) as exec:\n",
    "  main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Save Execution Assets (model) and Metadata\n",
    "uploaded_assets = EA.execution_upload(configuration_records.execution_rid, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
