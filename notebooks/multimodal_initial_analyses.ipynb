{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/informatics-isi-edu/eye-ai-exec/blob/main/notebooks/VGG19_Diagnosis_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVq_jMdfx7Ni"
   },
   "source": [
    "# Multimodal Initial analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# if IN_COLAB:\n",
    "#     !pip install deriva\n",
    "#     !pip install bdbag\n",
    "#     !pip install --upgrade --force pydantic\n",
    "#     !pip install git+https://github.com/informatics-isi-edu/deriva-ml git+https://github.com/informatics-isi-edu/eye-ai-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = \"Repos\"   # Set this to be where your github repos are located.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Update the load path so python can find modules for the model\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.home() / repo_dir / \"eye-ai-ml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "\n",
    "import json\n",
    "import os\n",
    "from eye_ai.eye_ai import EyeAI\n",
    "import pandas as pd\n",
    "from pathlib import Path, PurePath\n",
    "import logging\n",
    "# import torch\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Import label encoder \n",
    "from sklearn import preprocessing \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\n",
    "catalog_id = \"eye-ai\" #@param\n",
    "host = 'www.eye-ai.org'\n",
    "\n",
    "\n",
    "gnl = GlobusNativeLogin(host=host)\n",
    "if gnl.is_logged_in([host]):\n",
    "    print(\"You are already logged in.\")\n",
    "else:\n",
    "    gnl.login([host], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n",
    "    print(\"Login Successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imD3DJ4lx7Nm"
   },
   "source": [
    "Connect to Eye-AI catalog.  Configure to store data local cache and working directories.  Initialize Eye-AI for pending execution based on the provided configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to configure the rest of the notebook.\n",
    "\n",
    "cache_dir = '/data'        # Directory in which to cache materialized BDBags for datasets\n",
    "working_dir = '/data'    # Directory in which to place output files for later upload.\n",
    "\n",
    "configuration_rid= \"2-CCD4\" # rid I created with my config containing minid for both train and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EA = EyeAI(hostname = host, catalog_id = catalog_id, cache_dir= cache_dir, working_dir=working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Initiate an Execution\n",
    "configuration_records = EA.execution_init(configuration_rid=configuration_rid)\n",
    "configuration_records.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate multimodal wide table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN: configuration_records.bag_paths[0]\n",
    "wide_train_raw = EA.severity_analysis(configuration_records.bag_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: configuration_records.bag_paths[1]\n",
    "# TRAIN: configuration_records.bag_paths[0]\n",
    "wide_test_raw = EA.severity_analysis(configuration_records.bag_paths[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add age to table\n",
    "age_path = \"/data/yukim3003/EyeAI_working/Execution_Assets/Multimodal_Analysis/multimodal_subject_age.csv\"\n",
    "age_df = pd.read_csv(age_path)\n",
    "age_df.rename(columns={'RID': 'RID_Subject'}, inplace=True)\n",
    "wide_train_raw = wide_train_raw.merge(age_df, on='RID_Subject', how='left')\n",
    "wide_test_raw = wide_test_raw.merge(age_df, on='RID_Subject', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new table with only more severe eye for each patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current severity rule: prioritize RNFL > HVF > CDR\n",
    "# if don't want thresholds, just make threshold 0\n",
    "# just return the first eye if RNFL, MD, CDR all NaN\n",
    "def pick_severe_eye(df, rnfl_threshold, md_threshold):\n",
    "    # Sort by 'Average_RNFL_Thickness(μm)', 'MD', and 'CDR' in descending order\n",
    "    df = df.sort_values(by=['Average_RNFL_Thickness(μm)', 'MD', 'CDR'], ascending=[True, True, False])\n",
    "\n",
    "    ### 1. if only 1 eye has a label, just pick that eye as more severe eye (for Dr. Song's patients)\n",
    "    df = df.groupby('RID_Subject').apply(lambda group: group[group['Label'].notna()]).reset_index(drop=True)\n",
    "    \n",
    "    # 2. Select the row/eye with most severe value within the thresholds\n",
    "    def select_row(group):\n",
    "        max_value = group['Average_RNFL_Thickness(μm)'].min() # min is more severe for RNFL\n",
    "        within_value_threshold = group[np.abs(group['Average_RNFL_Thickness(μm)'] - max_value) <= rnfl_threshold] # identify eyes within threshold\n",
    "\n",
    "        if len(within_value_threshold) > 1 or len(within_value_threshold) == 0: # if both eyes \"equal\" RNFL OR if RNFL is NaN, then try MD\n",
    "            max_other_column = within_value_threshold['MD'].min() # min is more severe for MD\n",
    "            within_other_column_threshold = within_value_threshold[np.abs(within_value_threshold['MD'] - max_other_column) <= md_threshold]\n",
    "\n",
    "            if len(within_other_column_threshold) > 1 or len(within_other_column_threshold) == 0: # if both eyes \"equal\" MD OR if MD is NaN, then try CDR\n",
    "                return group.sort_values(by=['CDR'], ascending=[False]).iloc[0] # since i didn't set CDR threshold, this will always pick something (even if NaN)\n",
    "            else:\n",
    "                return within_other_column_threshold.iloc[0]\n",
    "        else:\n",
    "            return within_value_threshold.iloc[0]\n",
    "    return df.groupby('RID_Subject').apply(select_row).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_train_nothresh = pick_severe_eye(wide_train_raw, 0, 0)\n",
    "wide_test_nothresh = pick_severe_eye(wide_test_raw, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnfl_thresh = 0\n",
    "md_thresh = 0\n",
    "wide_train = pick_severe_eye(wide_train_raw, rnfl_thresh, md_thresh)\n",
    "wide_test = pick_severe_eye(wide_test_raw, rnfl_thresh, md_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show which subjects changed eyes by adding thresholds\n",
    "diff_values = wide_train.compare(wide_train_nothresh, align_axis=0, keep_shape=True, keep_equal=True) #keep_equal=False --> values that are equal are represented as NaN\n",
    "diff_values = diff_values.drop_duplicates(keep=False) # drop rows that have a duplicate\n",
    "print(\"# subjects where eye choice changed: %i\" % (len(diff_values)/2))\n",
    "diff_values[['RID_Subject', 'Side', 'Label', 'Average_RNFL_Thickness(μm)', 'MD', 'CDR']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in features and target variable\n",
    "demographic_fx = ['Gender', 'Ethnicity', 'Age']\n",
    "clinic_fx = ['LogMAR_VA', 'IOP', 'CDR'] # 'Gonioscopy' - mostly NaN, not standardized annotation # CCT - mostly NaN\n",
    "RNFL_fx = ['Average_RNFL_Thickness(μm)'] # Average_C/D_Ratio - for RNFL-derived CDR\n",
    "RNFL_clockhr_fx = ['Clock_Hours_1', 'Clock_Hours_2', 'Clock_Hours_3', 'Clock_Hours_4', 'Clock_Hours_5', 'Clock_Hours_6', 'Clock_Hours_7', 'Clock_Hours_8', 'Clock_Hours_9', 'Clock_Hours_10', 'Clock_Hours_11', 'Clock_Hours_12'] # if I want to use each clock hour\n",
    "RNFL_quad_fx = ['Quadrants_S', 'Quadrants_N', 'Quadrants_T', 'Quadrants_I']\n",
    "RNFL_IS_fx = ['Quadrants_S', 'Quadrants_I']\n",
    "HVF_fx = ['MD', 'VFI'] # 'PSD' - mostly NaN. I think PSD and PSD.1 columns should be merged to use this column if desired\n",
    "GHT = ['GHT']\n",
    "\n",
    "## Adjust fx_cols depending on what variables I want to include in model\n",
    "fx_cols = demographic_fx + clinic_fx + HVF_fx + RNFL_fx + RNFL_IS_fx + GHT # selected feature cols from above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to transform data, to apply to wide_train and wide_test\n",
    "def transform_data(multimodal_wide, fx_cols):\n",
    "    ### drop rows missing label (ie no label for POAG vs PACG vs GS)\n",
    "    multimodal_wide = multimodal_wide.dropna(subset=['Label'])\n",
    "    # drop rows where label is \"Other\" (should only be PACG, POAG, or GS)\n",
    "    allowed_labels = [\"PACG\", \"POAG\", \"GS\"]\n",
    "    multimodal_wide = multimodal_wide[multimodal_wide['Label'].isin(allowed_labels)]\n",
    "\n",
    "    X = multimodal_wide[fx_cols] # Features\n",
    "    y = multimodal_wide.Label # Target variable\n",
    "\n",
    "    ### GHT: reformat as \"Outside Normal Limits\", \"Within Normal Limits\", \"Borderline\", \"Other\"\n",
    "    if \"GHT\" in fx_cols:\n",
    "        GHT_categories = [\"Outside Normal Limits\", \"Within Normal Limits\", \"Borderline\"]\n",
    "        X.loc[~X['GHT'].isin(GHT_categories), 'GHT'] = np.nan # alt: 'Other'; I did np.nan bc I feel like it makes more sense to drop this variable\n",
    "\n",
    "    ### Ethnicity: reformat so that Multi-racial, Other, and ethnicity not specified are combined as Other\n",
    "    if \"Ethnicity\" in fx_cols:\n",
    "        eth_categories = [\"African Descent\", \"Asian\", \"Caucasian\", \"Latin American\"]\n",
    "        X.loc[~X['Ethnicity'].isin(eth_categories), 'Ethnicity'] = 'Other'\n",
    "        \n",
    "    ### categorical data: encode using OneHotEncoder\n",
    "    from feature_engine.encoding import OneHotEncoder\n",
    "    categorical_vars = list(set(fx_cols) & set(['Gender', 'Ethnicity', 'GHT']))  # cateogorical vars that exist\n",
    "\n",
    "    if len(categorical_vars)>0: \n",
    "        # replace NaN with category \"Unknown\", then delete this column from one-hot encoding later\n",
    "        for var in categorical_vars:\n",
    "            X[var] = X[var].fillna(\"Unknown\")\n",
    "        \n",
    "        encoder = OneHotEncoder(variables = categorical_vars)\n",
    "        X_transformed = encoder.fit_transform(X)\n",
    "\n",
    "        # delete Unknown columns\n",
    "        X_transformed.drop(list(X_transformed.filter(regex='Unknown')), axis=1, inplace=True)\n",
    "\n",
    "        ### sort categorical encoded columns so that they're in alphabetical order\n",
    "        def sort_cols(X, var):\n",
    "            # Select the subset of columns to sort\n",
    "            subset_columns = [col for col in X.columns if col.startswith(var)]\n",
    "            # Sort the subset of columns alphabetically\n",
    "            sorted_columns = sorted(subset_columns)\n",
    "            # Reorder the DataFrame based on the sorted columns\n",
    "            sorted_df = X[[col for col in X.columns if col not in subset_columns] + sorted_columns]\n",
    "            return sorted_df\n",
    "        for var in categorical_vars:\n",
    "            X_transformed = sort_cols(X_transformed, var)\n",
    "\n",
    "    else:\n",
    "        print(\"No categorical variables\")\n",
    "        X_transformed=X\n",
    "\n",
    "    ### format numerical data\n",
    "    # VFI\n",
    "    if 'VFI' in fx_cols:\n",
    "        X_transformed['VFI'] = X_transformed['VFI'].replace('Off', np.nan) # replace \"Off\" with nan\n",
    "        def convert_percent(x):\n",
    "            if pd.isnull(x):\n",
    "                return np.nan\n",
    "            return float(x.strip('%'))/100\n",
    "        X_transformed['VFI'] = X_transformed['VFI'].map(convert_percent)\n",
    "\n",
    "    ### format y\n",
    "    # combine PACG and POAG as glaucoma\n",
    "    y = y.replace(['POAG', 'PACG'], 'Glaucoma')\n",
    "    # convert to 0 and 1\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    y[:] = label_encoder.fit_transform(y) # fit_transform combines fit and transform\n",
    "    y = y.astype(int)\n",
    "\n",
    "    return X_transformed, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_keep_missing, y_train_keep_missing = transform_data(wide_train, fx_cols)\n",
    "X_test_keep_missing, y_test_keep_missing = transform_data(wide_test, fx_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize Data\n",
    "#### may not be required for univariate analysis to improve interpretability (but still good to center/scale to improve Gaussian-ness of distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### normalize numeric training data (so that features are on same scale instead of wildly different scales)\n",
    "# not required for typical logistic regression, but do need for regularized regression\n",
    "# I didn't put this in transform_data because I want to use the scaler fitted on train for test too\n",
    "\n",
    "# how? https://datascience.stackexchange.com/questions/54908/data-normalization-before-or-after-train-test-split\n",
    "# why? https://stackoverflow.com/questions/52670012/convergencewarning-liblinear-failed-to-converge-increase-the-number-of-iterati\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "categorical_vars = ['Gender', 'Ethnicity', 'GHT']\n",
    "numeric_vars = sorted(set(fx_cols) - set(categorical_vars), key=fx_cols.index)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "normalized_numeric_X_train = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train_keep_missing[numeric_vars]),\n",
    "    columns = numeric_vars\n",
    ")\n",
    "cat_df = X_train_keep_missing.drop(numeric_vars, axis=1)\n",
    "X_train_keep_missing = pd.concat([normalized_numeric_X_train.set_index(cat_df.index), cat_df], axis=1)\n",
    "\n",
    "# normalize test data, but using scaler fitted to training data to prevent data leakage\n",
    "normalized_numeric_X_test = pd.DataFrame(\n",
    "    scaler.transform(X_test_keep_missing[numeric_vars]),\n",
    "    columns = numeric_vars\n",
    ")\n",
    "cat_df = X_test_keep_missing.drop(numeric_vars, axis=1)\n",
    "X_test_keep_missing = pd.concat([normalized_numeric_X_test.set_index(cat_df.index), cat_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counts / data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.unique(y_train_keep_missing, return_counts=True)\n",
    "print(counts) # #GS vs #Glaucoma\n",
    "print(\"Percent GS vs Glaucoma in TRAIN:\", counts[1] / sum(counts[1])) # percent\n",
    "\n",
    "counts = np.unique(y_test_keep_missing, return_counts=True)\n",
    "print(counts) # #GS vs #Glaucoma\n",
    "print(\"Percent GS vs Glaucoma in TEST:\", counts[1] / sum(counts[1])) # percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #NAN\n",
    "### the number of rows with nan in any column will increase if I choose more features\n",
    "\n",
    "# count number / percent of rows with nan value\n",
    "num_rows_with_nan = X_train_keep_missing.isnull().any(axis=1).sum()\n",
    "print (\"Number of train rows with any nan: %i\" % num_rows_with_nan)\n",
    "\n",
    "# Calculate the percentage of rows with NaN values\n",
    "print (\"Percent of train rows with any nan: %f\" % ((num_rows_with_nan / len(X_train_keep_missing)) * 100))\n",
    "\n",
    "# count number / percent of rows with nan value\n",
    "num_rows_with_nan = X_test_keep_missing.isnull().any(axis=1).sum()\n",
    "print (\"Number of test rows with any nan: %i\" % num_rows_with_nan)\n",
    "\n",
    "# Calculate the percentage of rows with NaN values\n",
    "print (\"Percent of test rows with any nan: %f\" % ((num_rows_with_nan / len(X_test_keep_missing)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A) Simple imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat = 'mean'\n",
    "# NOTE: the following code imputes X_test based on the imputer fitted to X_train\n",
    "\n",
    "\"\"\"\n",
    "STRATEGIES\n",
    "If “mean”, then replace missing values using the mean along each column. Can only be used with numeric data.\n",
    "\n",
    "If “median”, then replace missing values using the median along each column. Can only be used with numeric data.\n",
    "\n",
    "If “most_frequent”, then replace missing using the most frequent value along each column. Can be used with strings or numeric data. If there is more than one such value, only the smallest is returned.\n",
    "\n",
    "If “constant”, then replace missing values with fill_value. Can be used with strings or numeric data.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy=strat)\n",
    "imputer = imputer.fit(X_train_keep_missing)\n",
    "X_train_imputed = imputer.transform(X_train_keep_missing)\n",
    "X_test_imputed = imputer.transform(X_test_keep_missing)\n",
    "# convert into pandas dataframe instead of np array\n",
    "X_train = pd.DataFrame(X_train_imputed, columns=X_train_keep_missing.columns)\n",
    "X_test = pd.DataFrame(X_test_imputed, columns=X_test_keep_missing.columns)\n",
    "\n",
    "y_train = y_train_keep_missing\n",
    "y_test = y_test_keep_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Multiple imputations (10 imputations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good article on MCAR vs MAR vs MNAR and how to appropriately handle missing values in each case: https://datascience.stackexchange.com/questions/116622/what-should-you-do-with-nan-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return list of pandas dataframes, each containing 1 of 10 imputations\n",
    "def mult_impute_missing(X, train_data=None):\n",
    "    if train_data is None:\n",
    "        train_data = X\n",
    "\n",
    "    ### multiple imputation method using IterativeImputer from sklearn \n",
    "    from sklearn.experimental import enable_iterative_imputer\n",
    "    from sklearn.impute import IterativeImputer\n",
    "\n",
    "    imp = IterativeImputer(max_iter=10, random_state=0, sample_posterior=True)\n",
    "\n",
    "    imputed_datasets = []\n",
    "    for i in range(10): # 3-10 imputations standard\n",
    "        imp.random_state = i\n",
    "        imp.fit(train_data)\n",
    "        X_imputed = imp.transform(X)\n",
    "        imputed_datasets.append(pd.DataFrame(X_imputed, columns=X.columns))\n",
    "\n",
    "    # ALTERNATIVE\n",
    "    #from statsmodels.imputation import mice.MICEData # alternative package for MICE imputation\n",
    "    # official docs: https://www.statsmodels.org/dev/generated/statsmodels.imputation.mice.MICE.html#statsmodels.imputation.mice.MICE\n",
    "    # multiple imputation example using statsmodels: https://github.com/kshedden/mice_workshop\n",
    "    #imp = mice.MICEData(data)\n",
    "    #fml = 'y ~ x1 + x2 + x3 + x4' # variables used in multiple imputation model\n",
    "    #mice = mice.MICE(fml, sm.OLS, imp) # OLS chosen; can change this up\n",
    "    #results = mice.fit(10, 10) # 10 burn-in cycles to skip, 10 imputations\n",
    "    #print(results.summary())\n",
    "    \n",
    "    return imputed_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputedsets = mult_impute_missing(X_train_keep_missing) # list of 10 imputed X_trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_imputedsets = mult_impute_missing(X_test_keep_missing, train_data=X_test_keep_missing) # Impute test data using model fit with training data, not with test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train_keep_missing\n",
    "y_test = y_test_keep_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C) Drop NA\n",
    "### DON'T use this with the univariate loop -- drops too many rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with nan\n",
    "X_train = X_train_keep_missing.dropna()\n",
    "X_test = X_test_keep_missing.dropna()\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "\n",
    "y_train = y_train_keep_missing[y_train_keep_missing.index.isin(X_train.index)]\n",
    "y_test = y_test_keep_missing[y_test_keep_missing.index.isin(X_test.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "### 2 ways to calculate p-values; NOTE THAT P VALUES MAY NOT MAKE SENSE FOR REGULARIZED MODELS\n",
    "# https://stackoverflow.com/questions/25122999/scikit-learn-how-to-check-coefficients-significance\n",
    "def logit_pvalue(model, x):\n",
    "    \"\"\" Calculate z-scores for scikit-learn LogisticRegression.\n",
    "    parameters:\n",
    "        model: fitted sklearn.linear_model.LogisticRegression with intercept and large C\n",
    "        x:     matrix on which the model was fit\n",
    "    This function uses asymtptics for maximum likelihood estimates.\n",
    "    \"\"\"\n",
    "    p = model.predict_proba(x)\n",
    "    n = len(p)\n",
    "    m = len(model.coef_[0]) + 1\n",
    "    coefs = np.concatenate([model.intercept_, model.coef_[0]])\n",
    "    x_full = np.matrix(np.insert(np.array(x), 0, 1, axis = 1))\n",
    "    ans = np.zeros((m, m))\n",
    "    for i in range(n):\n",
    "        ans = ans + np.dot(np.transpose(x_full[i, :]), x_full[i, :]) * p[i,1] * p[i, 0]\n",
    "    vcov = np.linalg.inv(np.matrix(ans))\n",
    "    se = np.sqrt(np.diag(vcov))\n",
    "    t =  coefs/se  \n",
    "    p = (1 - norm.cdf(abs(t))) * 2\n",
    "    return p\n",
    "\n",
    "def format_dec(decimals):\n",
    "    f = [\"<.001\" if x<0.001 else \"%.3f\"%x for x in decimals]\n",
    "    return f\n",
    "\n",
    "# print model coefficients, ORs, p-values\n",
    "def model_summary(model, X_train):\n",
    "    print(\"Training set: %i\" % len(X_train))\n",
    "    coefs = model.coef_[0]\n",
    "    # odd ratios = e^coef\n",
    "    ors = np.exp(coefs)\n",
    "    intercept = model.intercept_[0]\n",
    "\n",
    "\n",
    "    p_values = logit_pvalue(model, X_train)\n",
    "\n",
    "    # compare with statsmodels ### RESULT: produces same result except gives nan instead of 1.00 for insignficant p-values\n",
    "    #import statsmodels.api as sm\n",
    "    #sm_model = sm.Logit(y_train.reset_index(drop=True), sm.add_constant(X_train)).fit(disp=0) ### this uses y_train from outside this function so not really valid but oh well I just want it for testing purposes\n",
    "    #p_values=sm_model.pvalues\n",
    "    #print(format_dec(pvalues))\n",
    "    #sm_model.summary()\n",
    "\n",
    "    # print results\n",
    "    results = pd.DataFrame({\n",
    "        'Coefficient': format_dec(np.append(intercept, coefs)),\n",
    "        'Odds Ratio': format_dec(np.append(np.exp(intercept), ors)),\n",
    "        'P-value': format_dec(p_values)\n",
    "    }, index=['Intercept'] + list(X_train.columns))\n",
    "    print(results)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model performance\n",
    "# https://medium.com/javarevisited/evaluating-the-logistic-regression-ae2decf42d61\n",
    "\n",
    "def compute_performance(model, X_test, y_test):\n",
    "    print(\"Test set: %i\" % len(X_test))\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    import sklearn.metrics as metrics\n",
    "    # evaluate predictions\n",
    "    mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "    print('MAE: %.3f' % mae)\n",
    "    \n",
    "    # examine the class distribution of the testing set (using a Pandas Series method)\n",
    "    y_test.value_counts()\n",
    "    \n",
    "    # calculate the percentage of ones\n",
    "    # because y_test only contains ones and zeros, we can simply calculate the mean = percentage of ones\n",
    "    y_test.mean()\n",
    "    \n",
    "    # calculate the percentage of zeros\n",
    "    1 - y_test.mean()\n",
    "    \n",
    "    # # Metrics computed from a confusion matrix (before thresholding)\n",
    "    \n",
    "    # Confusion matrix is used to evaluate the correctness of a classification model\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    confusion_matrix = confusion_matrix(y_test,y_pred)\n",
    "    confusion_matrix\n",
    "    \n",
    "    TP = confusion_matrix[1, 1]\n",
    "    TN = confusion_matrix[0, 0]\n",
    "    FP = confusion_matrix[0, 1]\n",
    "    FN = confusion_matrix[1, 0]\n",
    "    \n",
    "    # Classification Accuracy: Overall, how often is the classifier correct?\n",
    "    # use float to perform true division, not integer division\n",
    "    # print((TP + TN) / sum(map(sum, confusion_matrix))) -- this is is the same as the below automatic method\n",
    "    print('Accuracy: %.3f' % metrics.accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    # Sensitivity(recall): When the actual value is positive, how often is the prediction correct?\n",
    "    sensitivity = TP / float(FN + TP)\n",
    "    \n",
    "    print('Sensitivity: %.3f' % sensitivity)\n",
    "    # print('Recall score: %.3f' % metrics.recall_score(y_test, y_pred)) # same thing as sensitivity, but recall term used in ML\n",
    "    \n",
    "    # Specificity: When the actual value is negative, how often is the prediction correct?\n",
    "    specificity = TN / float(TN + FP)\n",
    "    print('Specificity: %.3f' % specificity)\n",
    "    \n",
    "    #from imblearn.metrics import specificity_score\n",
    "    #specificity_score(y_test, y_pred)\n",
    "    \n",
    "    # Precision: When a positive value is predicted, how often is the prediction correct?\n",
    "    precision = TP / float(TP + FP)\n",
    "    #print('Precision: %.3f' % precision)\n",
    "    print('Precision: %.3f' % metrics.precision_score(y_test, y_pred))\n",
    "    \n",
    "    # F score\n",
    "    f_score = 2*TP / float(2*TP + FP + FN)\n",
    "    #print('F score: %.3f' % f_score)\n",
    "    print('F1 score: %.3f' % metrics.f1_score(y_test,y_pred))\n",
    "    \n",
    "    #Evaluate the model using other performance metrics - REDUNDANT, COMMENTED OUT FOR NOW\n",
    "    # from sklearn.metrics import classification_report\n",
    "    # print(classification_report(y_test,y_pred))\n",
    "\n",
    "    # AUC\n",
    "    y_pred_proba = model.predict_proba(X_test)[::,1]\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "    print('AUC: %.3f' % auc)\n",
    "\n",
    "    # CM matrix plot\n",
    "    from sklearn import metrics\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = None)\n",
    "    \n",
    "    cm_display.plot()\n",
    "    plt.show()\n",
    "    # 0 = GS, 1 = POAG\n",
    "    \n",
    "    # ROC curve plot\n",
    "    plt.plot(fpr,tpr,label=\"auc=\"+str(auc))\n",
    "    plt.xlabel(\"False positive rate (1-specificity)\")\n",
    "    plt.ylabel(\"True positive rate (sensitivity)\")\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Simple Logistic Regression loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_NA=True # change if do simple imputation instead\n",
    "if drop_NA:\n",
    "    X_train = X_train_keep_missing\n",
    "    X_test = X_test_keep_missing\n",
    "    y_train = y_train_keep_missing\n",
    "    y_test = y_test_keep_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all feature columns\n",
    "# (code isn't all that different from the simple one below, I just wanted to use loop instead of doing by hand)\n",
    "\n",
    "# MUST DROP REFERENCE COLUMN FOR ONE-HOT-ENCODED VARIABLES\n",
    "chosen_ref_labels = ['GHT_Within Normal Limits', 'Gender_M', 'Ethnicity_Other']\n",
    "X_train_dropped = X_train.drop(columns=chosen_ref_labels)\n",
    "X_test_dropped = X_test.drop(columns=chosen_ref_labels)\n",
    "\n",
    "penalty=None#'l1', 'l2', 'elasticnet', or None\n",
    "solver='saga' # 'lbfgs', 'saga' (only saga supports l1 and elasticnet)\n",
    "\n",
    "logreg = LogisticRegression(random_state=16, solver=solver, max_iter=1000, penalty=penalty)    \n",
    "\n",
    "def process_fx(fx, X, X_t, Y, Y_t):\n",
    "    print(fx)\n",
    "    # select all columns that contain fx (because of categorical vars)\n",
    "    cols = [col for col in X.columns if fx in col]\n",
    "    x = X[cols]\n",
    "    x_t = X_t[cols]\n",
    "\n",
    "    if drop_NA:\n",
    "        # Drop NA if desired\n",
    "        x = x.dropna()\n",
    "        x_t = x_t.dropna()\n",
    "\n",
    "        y = Y[Y.index.isin(x.index)]\n",
    "        y_t = Y_t[Y_t.index.isin(x_t.index)]\n",
    "    \n",
    "    # fit the model with data\n",
    "    logreg.fit(x, y)\n",
    "    model_summary(logreg, x)\n",
    "    compute_performance(logreg, x_t, y_t)\n",
    "    print(\"\")\n",
    "\n",
    "for fx in fx_cols:\n",
    "    process_fx(fx, X_train_dropped, X_test_dropped, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Logistic Regression DROPNA or SIMPLEIMPUTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLE LOGISTIC REGRESSION\n",
    "# MUST DROP REFERENCE COLUMN FOR ONE-HOT-ENCODED VARIABLES\n",
    "#chosen_ref_labels = ['GHT_Within Normal Limits', 'Gender_M', 'Ethnicity_Other']\n",
    "chosen_ref_labels = ['GHT_Within Normal Limits','GHT_Borderline', 'Gender_M', 'Ethnicity_Other']\n",
    "drop_cols = [x for x in X_train.columns if x in chosen_ref_labels]\n",
    "X_train_dropped = X_train.drop(columns=drop_cols)\n",
    "X_test_dropped = X_test.drop(columns=drop_cols)\n",
    "\n",
    "penalty=None#'l1', 'l2', 'elasticnet', or None\n",
    "solver='saga' # 'lbfgs', 'saga' (only saga supports l1 and elasticnet)\n",
    "\n",
    "logreg = LogisticRegression(random_state=16, solver=solver, max_iter=1000, penalty=penalty) \n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(X_train_dropped, y_train)\n",
    "model_summary(logreg, X_train_dropped)\n",
    "compute_performance(logreg, X_test_dropped, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Regularization params\n",
    "k_folds = 10 #5-10 standard\n",
    "scoring = 'roc_auc' # 'neg_log_loss', 'neg_brier_score', 'accuracy' (default), 'roc_auc', 'neg_mean_absolute_error' ...options on sklearn.metrics: https://scikit-learn.org/stable/api/sklearn.metrics.html#module-sklearn.metrics\n",
    "max_iter=1000\n",
    "solver='saga'\n",
    "# for elastic net only:\n",
    "lambda_inverse = 20  # of C's (=inverse of lambda) to try; 10 by default\n",
    "alpha_range = np.linspace(0, 1, 20)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Ridge\n",
    "ridge_cv = LogisticRegressionCV(cv=k_folds, scoring=scoring, solver=solver, max_iter=max_iter)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "# Retrieve the best hyperparameters\n",
    "best_C = ridge_cv.C_[0]\n",
    "print(f\"Best C (inverse of regularization strength): {best_C}\")\n",
    "\n",
    "model_summary(ridge_cv, X_train)\n",
    "compute_performance(ridge_cv, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Elastic Net\n",
    "#https://stackoverflow.com/questions/66787845/how-to-perform-elastic-net-for-a-classification-problem\n",
    "# SAGA should be considered more advanced and used over SAG. For more information, see: https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-defintions\n",
    "en_cv = LogisticRegressionCV(cv=k_folds, scoring=scoring, penalty='elasticnet', Cs = lambda_inverse, l1_ratios=alpha_range, solver=solver, max_iter=max_iter)\n",
    "en_cv.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "best_C = en_cv.C_[0]\n",
    "best_l1_ratio = en_cv.l1_ratio_[0]\n",
    "print(f\"Best C (inverse of regularization strength): {best_C}\")\n",
    "print(f\"Best l1_ratio (mixing parameter): {best_l1_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best C (inverse of regularization strength): {best_C}\")\n",
    "print(f\"Best l1_ratio (mixing parameter): {best_l1_ratio}\")\n",
    "model_summary(en_cv, X_train)\n",
    "compute_performance(en_cv, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Logistic Regression MULTIPLE IMPUTATIONS\n",
    "### To check if what I did is best method: used mode of y_pred, and averaged prediction probabilities of each imputed model to determine AUC, and averaged p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### After performing logistic regression on each imputed dataset, pool the results using Rubin’s rules to obtain a single set of estimates.\n",
    "\n",
    "# print model coefficients, ORs, p-values\n",
    "def model_summary_mice(logreg_models, Xtrain_finals):\n",
    "    print(\"Training set: %i\" % len(Xtrain_finals[0]))\n",
    "    \n",
    "    # Extract coefficients and standard errors\n",
    "    coefs = np.array([model.coef_[0] for model in logreg_models])\n",
    "    ors = np.exp(coefs)\n",
    "    intercepts = np.array([model.intercept_[0] for model in logreg_models])\n",
    "    p_values = np.array([logit_pvalue(model, Xtrain_finals[i]) for i, model in enumerate(logreg_models)])\n",
    "    \n",
    "    # Calculate pooled estimates\n",
    "    pooled_coefs = np.mean(coefs, axis=0)\n",
    "    pooled_ors = np.mean(ors, axis=0)\n",
    "    pooled_intercept = np.mean(intercepts)\n",
    "    # I think this calculates SES between the imputed datasets\n",
    "    pooled_ses = np.sqrt(np.mean(coefs**2, axis=0) + np.var(coefs, axis=0, ddof=1) * (1 + 1/len(logreg_models)))\n",
    "\n",
    "    pooled_p_values = np.mean(p_values, axis=0)\n",
    "    \n",
    "    # Display pooled results\n",
    "    results = pd.DataFrame({\n",
    "        'Coefficient': format_dec(np.append(pooled_intercept, pooled_coefs)),\n",
    "        'Odds Ratio': format_dec(np.append(np.exp(intercept), pooled_ors)),\n",
    "        'Standard Error': format_dec(np.append(np.nan, pooled_ses)),  # Intercept SE is not calculated here\n",
    "        'P-value': format_dec(pooled_p_values)\n",
    "    }, index=['Intercept'] + list(Xtrain_finals[0].columns))\n",
    "    print(results)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model performance\n",
    "# https://medium.com/javarevisited/evaluating-the-logistic-regression-ae2decf42d61\n",
    "def compute_performance_mice(logreg_models, Xtest_finals, y_test):\n",
    "    print(\"Test set: %i\" % len(Xtest_finals[0]))\n",
    "    \n",
    "    y_pred_results = []\n",
    "    y_pred_proba_results = []\n",
    "    for model, X_test in zip(logreg_models, Xtest_finals):\n",
    "        y_pred_results.append(model.predict(X_test))\n",
    "        \n",
    "        y_pred_proba_results.append(model.predict_proba(X_test)[::,1])\n",
    "\n",
    "    ypred_df = pd.DataFrame(np.row_stack(y_pred_results))\n",
    "    y_pred = np.array(ypred_df.mode(axis=0).loc[0].astype(int)) ##### used the mode of y_pred across the 10 imputations\n",
    "    y_pred_proba = np.mean(y_pred_proba_results, axis=0)\n",
    "    \n",
    "    \n",
    "    import sklearn.metrics as metrics\n",
    "    # evaluate predictions\n",
    "    mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "    print('MAE: %.3f' % mae)\n",
    "    \n",
    "    # examine the class distribution of the testing set (using a Pandas Series method)\n",
    "    y_test.value_counts()\n",
    "    \n",
    "    # calculate the percentage of ones\n",
    "    # because y_test only contains ones and zeros, we can simply calculate the mean = percentage of ones\n",
    "    y_test.mean()\n",
    "    \n",
    "    # calculate the percentage of zeros\n",
    "    1 - y_test.mean()\n",
    "    \n",
    "    # # Metrics computed from a confusion matrix (before thresholding)\n",
    "    \n",
    "    # Confusion matrix is used to evaluate the correctness of a classification model\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    confusion_matrix = confusion_matrix(y_test,y_pred)\n",
    "    confusion_matrix\n",
    "    \n",
    "    TP = confusion_matrix[1, 1]\n",
    "    TN = confusion_matrix[0, 0]\n",
    "    FP = confusion_matrix[0, 1]\n",
    "    FN = confusion_matrix[1, 0]\n",
    "    \n",
    "    # Classification Accuracy: Overall, how often is the classifier correct?\n",
    "    # use float to perform true division, not integer division\n",
    "    # print((TP + TN) / sum(map(sum, confusion_matrix))) -- this is is the same as the below automatic method\n",
    "    print('Accuracy: %.3f' % metrics.accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    # Sensitivity(recall): When the actual value is positive, how often is the prediction correct?\n",
    "    sensitivity = TP / float(FN + TP)\n",
    "    \n",
    "    print('Sensitivity: %.3f' % sensitivity)\n",
    "    # print('Recall score: %.3f' % metrics.recall_score(y_test, y_pred)) # same thing as sensitivity, but recall term used in ML\n",
    "    \n",
    "    # Specificity: When the actual value is negative, how often is the prediction correct?\n",
    "    specificity = TN / float(TN + FP)\n",
    "    print('Specificity: %.3f' % specificity)\n",
    "    \n",
    "    #from imblearn.metrics import specificity_score\n",
    "    #specificity_score(y_test, y_pred)\n",
    "    \n",
    "    # Precision: When a positive value is predicted, how often is the prediction correct?\n",
    "    precision = TP / float(TP + FP)\n",
    "    #print('Precision: %.3f' % precision)\n",
    "    print('Precision: %.3f' % metrics.precision_score(y_test, y_pred))\n",
    "    \n",
    "    # F score\n",
    "    f_score = 2*TP / float(2*TP + FP + FN)\n",
    "    #print('F score: %.3f' % f_score)\n",
    "    print('F1 score: %.3f' % metrics.f1_score(y_test,y_pred))\n",
    "    \n",
    "    #Evaluate the model using other performance metrics - REDUNDANT, COMMENTED OUT FOR NOW\n",
    "    # from sklearn.metrics import classification_report\n",
    "    # print(classification_report(y_test,y_pred))\n",
    "\n",
    "    # AUC\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "    print('AUC: %.3f' % auc)\n",
    "\n",
    "    # CM matrix plot\n",
    "    from sklearn import metrics\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = None)\n",
    "    \n",
    "    cm_display.plot()\n",
    "    plt.show()\n",
    "    # 0 = GS, 1 = POAG\n",
    "    \n",
    "    # ROC curve plot\n",
    "    plt.plot(fpr,tpr,label=\"auc=\"+str(auc))\n",
    "    plt.xlabel(\"False positive rate (1-specificity)\")\n",
    "    plt.ylabel(\"True positive rate (sensitivity)\")\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to do prediction after multiple imputation:\n",
    "# https://github.com/amices/mice/issues/82\n",
    "# https://stackoverflow.com/questions/68460923/how-to-do-the-prediction-after-multiple-imputation-with-mice-package\n",
    "logreg_models = []\n",
    "Xtrain_finals = []\n",
    "Xtest_finals = []\n",
    "\n",
    "# MUST DROP REFERENCE COLUMN FOR ONE-HOT-ENCODED VARIABLES\n",
    "#chosen_ref_labels = ['GHT_Within Normal Limits', 'Gender_M', 'Ethnicity_Other']\n",
    "chosen_ref_labels = ['GHT_Within Normal Limits','GHT_Borderline', 'Gender_M', 'Ethnicity_Other']\n",
    "penalty=None#'l1', 'l2', 'elasticnet', or None\n",
    "solver='saga' # 'lbfgs', 'saga' (only saga supports l1 and elasticnet)\n",
    "\n",
    "for X_train, X_test in zip(X_train_imputedsets, X_test_imputedsets):\n",
    "    # SIMPLE LOGISTIC REGRESSION\n",
    "    drop_cols = [x for x in X_train.columns if x in chosen_ref_labels]\n",
    "    X_train_dropped = X_train.drop(columns=drop_cols)\n",
    "    X_test_dropped = X_test.drop(columns=drop_cols)\n",
    "\n",
    "    logreg = LogisticRegression(random_state=16, solver=solver, max_iter=1000, penalty=penalty)\n",
    "    logreg.fit(X_train_dropped, y_train)\n",
    "    logreg_models.append(logreg)\n",
    "\n",
    "    Xtrain_finals.append(X_train_dropped)\n",
    "    Xtest_finals.append(X_test_dropped)\n",
    "\n",
    "model_summary_mice(logreg_models, Xtrain_finals)\n",
    "compute_performance_mice(logreg_models, Xtest_finals, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't have to onehotencode, but xgboost performs better if does\n",
    "# keep dummy variables, don't drop ref label for decision trees\n",
    "\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "drop_NA=True\n",
    "if drop_NA:\n",
    "    # Drop NA if desired\n",
    "    x = X_train_keep_missing.dropna()\n",
    "    x_t = X_test_keep_missing.dropna()\n",
    "\n",
    "    y = y_train[y_train.index.isin(x.index)]\n",
    "    y_t = y_test[y_test.index.isin(x_t.index)]\n",
    "\n",
    "print(x.columns)\n",
    "\n",
    "#model = BaggingClassifier(estimator=SVC(), n_estimators=10, random_state=0) # bagged SVC\n",
    "#model=BaggingClassifier() # bagged decision trees (bc DecisionTree is default)\n",
    "model=SVC(probability=True) # probability=True to enable predict_proba function (slow)\n",
    "clf = model.fit(x,y)\n",
    "\n",
    "# define cross-validation evaluation procedure\n",
    "k = 10\n",
    "cv = RepeatedStratifiedKFold(n_splits=k, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, x, y, scoring='roc_auc', cv=cv)\n",
    "# summarize performance\n",
    "print('Mean AUC using %i-fold cross-validation: %.3f' % (k, mean(scores)))# AUC from 10-fold cv on TRAINING set, as opposed to AUC on test set computed in compute_performance -- if this better than AUC for test set, then model probably overfit\n",
    "print(\"\")\n",
    "\n",
    "# test performance\n",
    "compute_performance(clf, x_t, y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
