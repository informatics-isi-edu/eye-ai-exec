{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/informatics-isi-edu/eye-ai-exec/blob/main/notebooks/VGG19_Diagnosis_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVq_jMdfx7Ni"
   },
   "source": [
    "# Multimodal Initial analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# if IN_COLAB:\n",
    "#     !pip install deriva\n",
    "#     !pip install bdbag\n",
    "#     !pip install --upgrade --force pydantic\n",
    "#     !pip install git+https://github.com/informatics-isi-edu/deriva-ml git+https://github.com/informatics-isi-edu/eye-ai-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = \"Repos\"   # Set this to be where your github repos are located.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Update the load path so python can find modules for the model\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.home() / repo_dir / \"eye-ai-ml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "\n",
    "import json\n",
    "import os\n",
    "from eye_ai.eye_ai import EyeAI\n",
    "import pandas as pd\n",
    "from pathlib import Path, PurePath\n",
    "import logging\n",
    "# import torch\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Import label encoder \n",
    "from sklearn import preprocessing \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\n",
    "catalog_id = \"eye-ai\" #@param\n",
    "host = 'www.eye-ai.org'\n",
    "\n",
    "\n",
    "gnl = GlobusNativeLogin(host=host)\n",
    "if gnl.is_logged_in([host]):\n",
    "    print(\"You are already logged in.\")\n",
    "else:\n",
    "    gnl.login([host], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n",
    "    print(\"Login Successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imD3DJ4lx7Nm"
   },
   "source": [
    "Connect to Eye-AI catalog.  Configure to store data local cache and working directories.  Initialize Eye-AI for pending execution based on the provided configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to configure the rest of the notebook.\n",
    "\n",
    "cache_dir = '/data'        # Directory in which to cache materialized BDBags for datasets\n",
    "working_dir = '/data'    # Directory in which to place output files for later upload.\n",
    "\n",
    "# CCD4 uses the new minid's, 2-CC3W is old; but I'm getting error with CCD4\n",
    "configuration_rid= \"2-CCD4\" # rid I created with my config containing minid for both train and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EA = EyeAI(hostname = host, catalog_id = catalog_id, cache_dir= cache_dir, working_dir=working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Initiate an Execution\n",
    "configuration_records = EA.execution_init(configuration_rid=configuration_rid)\n",
    "configuration_records.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate multimodal wide table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old method using local files -- NOT recommended\n",
    "# multimodal_wide_path = \"/data/yukim3003/EyeAI_working/Execution_Assets/Multimodal_Analysis/wide_multimodal_full.csv\"\n",
    "# multimodal_wide = pd.read_csv(multimodal_wide_path)\n",
    "# multimodal_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN: configuration_records.bag_paths[0]\n",
    "wide_train_raw = EA.severity_analysis(configuration_records.bag_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: configuration_records.bag_paths[1]\n",
    "# TRAIN: configuration_records.bag_paths[0]\n",
    "wide_test_raw = EA.severity_analysis(configuration_records.bag_paths[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing duplicate patients where one should have been removed\n",
    "#wide_train_raw.loc[wide_train_raw['RID_Subject']==\"2-7KWW\"]\n",
    "#wide_test_raw.loc[wide_test_raw['RID_Subject']==\"2-7MJ4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add age to table\n",
    "age_path = \"/data/yukim3003/EyeAI_working/Execution_Assets/Multimodal_Analysis/multimodal_subject_age.csv\"\n",
    "age_df = pd.read_csv(age_path)\n",
    "age_df.rename(columns={'RID': 'RID_Subject'}, inplace=True)\n",
    "wide_train_raw = wide_train_raw.merge(age_df, on='RID_Subject', how='left')\n",
    "wide_test_raw = wide_test_raw.merge(age_df, on='RID_Subject', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new table with only more severe eye for each patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old method with bugs:\n",
    "#1. if eye1 is GS and eye2 is NaN, then eye2 also becomes GS; and similarly if eye1 has CDR 0.9 but eye2 is NaN, then eye2 also gets CDR 0.9. Fixed this by adding skipna=False to first()\n",
    "#2. if eye1 has an RNFL but eye2 RNFL is NaN, then this method will consider eye1 to be more severe, whereas it's better to move on to assessing MD in that case. Fixing this fixed 11 eyes\n",
    "\n",
    "#def pick_severe_eye(df):    \n",
    "#    # Sort by RNFL, HVF, CDR whereby first row is most severe\n",
    "#    df = df.sort_values(by=['Average_RNFL_Thickness(μm)', 'MD', 'CDR'], ascending=[True, True, False])\n",
    "#    \n",
    "#    # Group by subject and get the first row in each group. If all tied, will just pick the first eye - ie the right eye\n",
    "#    return df.groupby('RID_Subject').first(skipna=False).reset_index() # first computes the first entry of each column within each group, but NaN's dont count as a value; so if one eye has NaN for any random column, then the value for the other eye is transferred to that eye\n",
    "\n",
    "#wide_train = pick_severe_eye(wide_train_raw)\n",
    "#wide_test = pick_severe_eye(wide_test_raw)\n",
    "\n",
    "# the row that made me realize the bug requiring skipna in first() # if I did want to apply the label of any eye to both eyes, this could be useful\n",
    "# wide_train[wide_train['RID_Subject']=='2-7KVA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current severity rule: prioritize RNFL > HVF > CDR\n",
    "# if don't want thresholds, just make threshold 0\n",
    "# just return the first eye if RNFL, MD, CDR all NaN\n",
    "def pick_severe_eye(df, rnfl_threshold, md_threshold):\n",
    "    # Sort by 'Average_RNFL_Thickness(μm)', 'MD', and 'CDR' in descending order\n",
    "    df = df.sort_values(by=['Average_RNFL_Thickness(μm)', 'MD', 'CDR'], ascending=[True, True, False])\n",
    "\n",
    "    ### 1. if only 1 eye has a label, just pick that eye as more severe eye (for Dr. Song's patients)\n",
    "    df = df.groupby('RID_Subject').apply(lambda group: group[group['Label'].notna()]).reset_index(drop=True)\n",
    "    \n",
    "    # 2. Select the row/eye with most severe value within the thresholds\n",
    "    def select_row(group):\n",
    "        max_value = group['Average_RNFL_Thickness(μm)'].min() # min is more severe for RNFL\n",
    "        within_value_threshold = group[np.abs(group['Average_RNFL_Thickness(μm)'] - max_value) <= rnfl_threshold] # identify eyes within threshold\n",
    "\n",
    "        if len(within_value_threshold) > 1 or len(within_value_threshold) == 0: # if both eyes \"equal\" RNFL OR if RNFL is NaN, then try MD\n",
    "            max_other_column = within_value_threshold['MD'].min() # min is more severe for MD\n",
    "            within_other_column_threshold = within_value_threshold[np.abs(within_value_threshold['MD'] - max_other_column) <= md_threshold]\n",
    "\n",
    "            if len(within_other_column_threshold) > 1 or len(within_other_column_threshold) == 0: # if both eyes \"equal\" MD OR if MD is NaN, then try CDR\n",
    "                return group.sort_values(by=['CDR'], ascending=[False]).iloc[0] # since i didn't set CDR threshold, this will always pick something (even if NaN)\n",
    "            else:\n",
    "                return within_other_column_threshold.iloc[0]\n",
    "        else:\n",
    "            return within_value_threshold.iloc[0]\n",
    "    return df.groupby('RID_Subject').apply(select_row).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_train_nothresh = pick_severe_eye(wide_train_raw, 0, 0)\n",
    "wide_test_nothresh = pick_severe_eye(wide_test_raw, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnfl_thresh = 0\n",
    "md_thresh = 0\n",
    "wide_train = pick_severe_eye(wide_train_raw, rnfl_thresh, md_thresh)\n",
    "wide_test = pick_severe_eye(wide_test_raw, rnfl_thresh, md_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_train\n",
    "wide_train.loc[wide_train['RID_Subject']==\"2-7M2E\"] # duplicate subject in orig database, same as 2-7NGA\n",
    "#wide_train.loc[wide_train['RID_Subject']==\"2-7KTT\"] # example subject where more severe eye Left should be returned\n",
    "#wide_train.loc[wide_train['RID_Subject']==\"2-7KVA\"] # example subject where only eye with label Left should be returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show which subjects changed eyes by adding thresholds\n",
    "diff_values = wide_train.compare(wide_train_nothresh, align_axis=0, keep_shape=True, keep_equal=True) #keep_equal=False --> values that are equal are represented as NaN\n",
    "diff_values = diff_values.drop_duplicates(keep=False) # drop rows that have a duplicate\n",
    "print(\"# subjects where eye choice changed: %i\" % (len(diff_values)/2))\n",
    "diff_values[['RID_Subject', 'Side', 'Label', 'Average_RNFL_Thickness(μm)', 'MD', 'CDR']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in features and target variable\n",
    "demographic_fx = ['Gender', 'Ethnicity', 'Age']\n",
    "clinic_fx = ['LogMAR_VA', 'IOP', 'CDR'] # 'Gonioscopy' - mostly NaN, not standardized annotation # CCT - mostly NaN\n",
    "HVF_fx = ['MD', 'VFI'] # 'PSD' - mostly NaN. I think PSD and PSD.1 columns should be merged to use this column if desired\n",
    "RNFL_fx = ['Average_RNFL_Thickness(μm)'] # Average_C/D_Ratio - for RNFL-derived CDR\n",
    "RNFL_clockhr_fx = ['Clock_Hours_1', 'Clock_Hours_2', 'Clock_Hours_3', 'Clock_Hours_4', 'Clock_Hours_5', 'Clock_Hours_6', 'Clock_Hours_7', 'Clock_Hours_8', 'Clock_Hours_9', 'Clock_Hours_10', 'Clock_Hours_11', 'Clock_Hours_12'] # if I want to use each clock hour\n",
    "RNFL_quad_fx = ['Quadrants_S', 'Quadrants_N', 'Quadrants_T', 'Quadrants_I']\n",
    "RNFL_IS_fx = ['Quadrants_S', 'Quadrants_I']\n",
    "GHT = ['GHT']\n",
    "\n",
    "fx_cols = demographic_fx + clinic_fx + HVF_fx + RNFL_fx + RNFL_IS_fx + GHT # selected feature cols from above\n",
    "# fx_cols = ['CDR', 'MD'] + RNFL_IS_fx + GHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to transform data, to apply to wide_train and wide_test\n",
    "def transform_data(multimodal_wide, fx_cols):\n",
    "    ### drop rows missing label (ie no label for POAG vs PACG vs GS)\n",
    "    multimodal_wide = multimodal_wide.dropna(subset=['Label'])\n",
    "    # drop rows where label is \"Other\" (should only be PACG, POAG, or GS)\n",
    "    allowed_labels = [\"PACG\", \"POAG\", \"GS\"]\n",
    "    multimodal_wide = multimodal_wide[multimodal_wide['Label'].isin(allowed_labels)]\n",
    "\n",
    "    X = multimodal_wide[fx_cols] # Features\n",
    "    y = multimodal_wide.Label # Target variable\n",
    "\n",
    "    ### GHT: reformat as \"Outside Normal Limits\", \"Within Normal Limits\", \"Borderline\", \"Other\"\n",
    "    if \"GHT\" in fx_cols:\n",
    "        GHT_categories = [\"Outside Normal Limits\", \"Within Normal Limits\", \"Borderline\"]\n",
    "        X.loc[~X['GHT'].isin(GHT_categories), 'GHT'] = np.nan # alt: 'Other'; I did np.nan bc I feel like it makes more sense to drop this variable\n",
    "\n",
    "    ### categorical data: encode using LabelEncoder or OneHotEncoder\n",
    "    # label encoder if data ordinal (ie ranked) -- jk nvm this transformer should be used to encode target values, i.e. y, and not the input X!\n",
    "    # one-hot if data not ranked; note this will increase dimensionality of data which is bad if >1/3rd of fx are one-hot\n",
    "    # https://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor\n",
    "    #one_hot_encoder = preprocessing.OneHotEncoder(handle_unknown='ignore')\n",
    "    from feature_engine.encoding import OneHotEncoder # this instead of skLearn allows me to one hot encode desired columns only\n",
    "    categorical_vars = list(set(fx_cols) & set(['Gender', 'Ethnicity', 'GHT']))  # cateogorical vars that exist\n",
    "\n",
    "    if len(categorical_vars)>0: \n",
    "        # replace NaN with category \"Unknown\", then delete this column from one-hot encoding later\n",
    "        for var in categorical_vars:\n",
    "            X[var] = X[var].fillna(\"Unknown\")\n",
    "        \n",
    "        encoder = OneHotEncoder(variables = categorical_vars)\n",
    "        X_transformed = encoder.fit_transform(X)\n",
    "\n",
    "        # delete Unknown columns\n",
    "        X_transformed.drop(list(X_transformed.filter(regex='Unknown')), axis=1, inplace=True)\n",
    "\n",
    "        ### sort categorical encoded columns so that they're in alphabetical order\n",
    "        def sort_cols(X, var):\n",
    "            # Select the subset of columns to sort\n",
    "            subset_columns = [col for col in X.columns if col.startswith(var)]\n",
    "            # Sort the subset of columns alphabetically\n",
    "            sorted_columns = sorted(subset_columns)\n",
    "            # Reorder the DataFrame based on the sorted columns\n",
    "            sorted_df = X[[col for col in X.columns if col not in subset_columns] + sorted_columns]\n",
    "            return sorted_df\n",
    "        for var in categorical_vars:\n",
    "            X_transformed = sort_cols(X_transformed, var)\n",
    "\n",
    "    else:\n",
    "        print(\"No categorical variables\")\n",
    "        X_transformed=X\n",
    "\n",
    "    ### format numerical data\n",
    "    # VFI\n",
    "    if 'VFI' in fx_cols:\n",
    "        X_transformed['VFI'] = X_transformed['VFI'].replace('Off', np.nan) # replace \"Off\" with nan\n",
    "        def convert_percent(x):\n",
    "            if pd.isnull(x):\n",
    "                return np.nan\n",
    "            return float(x.strip('%'))/100\n",
    "        X_transformed['VFI'] = X_transformed['VFI'].map(convert_percent)\n",
    "\n",
    "    ### format y\n",
    "    # combine PACG and POAG as glaucoma\n",
    "    y = y.replace(['POAG', 'PACG'], 'Glaucoma')\n",
    "    # convert to 0 and 1\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    y[:] = label_encoder.fit_transform(y) # fit_transform combines fit and transform\n",
    "    y = y.astype(int)\n",
    "\n",
    "    return X_transformed, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed, y_train_keep_missing = transform_data(wide_train, fx_cols)\n",
    "X_test_transformed, y_test_keep_missing = transform_data(wide_test, fx_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### normalize numeric training data (so that features are on same scale instead of wildly different scales)\n",
    "# not required for typical logistic regression, but do need for regularized regression\n",
    "# I didn't put this in transform_data because I want to use the scaler fitted on train for test too\n",
    "\n",
    "# how? https://datascience.stackexchange.com/questions/54908/data-normalization-before-or-after-train-test-split\n",
    "# why? https://stackoverflow.com/questions/52670012/convergencewarning-liblinear-failed-to-converge-increase-the-number-of-iterati\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "categorical_vars = ['Gender', 'Ethnicity', 'GHT']\n",
    "numeric_vars = sorted(set(fx_cols) - set(categorical_vars), key=fx_cols.index)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "normalized_numeric_X_train = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train_transformed[numeric_vars]),\n",
    "    columns = numeric_vars\n",
    ")\n",
    "cat_df = X_train_transformed.drop(numeric_vars, axis=1)\n",
    "X_train_keep_missing = pd.concat([normalized_numeric_X_train.set_index(cat_df.index), cat_df], axis=1)\n",
    "\n",
    "# normalize test data, but using scaler fitted to training data to prevent data leakage\n",
    "normalized_numeric_X_test = pd.DataFrame(\n",
    "    scaler.transform(X_test_transformed[numeric_vars]),\n",
    "    columns = numeric_vars\n",
    ")\n",
    "cat_df = X_test_transformed.drop(numeric_vars, axis=1)\n",
    "X_test_keep_missing = pd.concat([normalized_numeric_X_test.set_index(cat_df.index), cat_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counts / data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.unique(y_train_keep_missing, return_counts=True)\n",
    "print(counts) # #GS vs #Glaucoma\n",
    "print(\"Percent GS vs Glaucoma in TRAIN:\", counts[1] / sum(counts[1])) # percent\n",
    "\n",
    "counts = np.unique(y_test_keep_missing, return_counts=True)\n",
    "print(counts) # #GS vs #Glaucoma\n",
    "print(\"Percent GS vs Glaucoma in TEST:\", counts[1] / sum(counts[1])) # percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #NAN\n",
    "### the number of rows with nan in any column will increase if I choose more features\n",
    "\n",
    "# count number / percent of rows with nan value\n",
    "num_rows_with_nan = X_train_keep_missing.isnull().any(axis=1).sum()\n",
    "print (\"Number of train rows with any nan: %i\" % num_rows_with_nan)\n",
    "\n",
    "# Calculate the percentage of rows with NaN values\n",
    "print (\"Percent of train rows with any nan: %f\" % ((num_rows_with_nan / len(X_train_keep_missing)) * 100))\n",
    "\n",
    "# count number / percent of rows with nan value\n",
    "num_rows_with_nan = X_test_keep_missing.isnull().any(axis=1).sum()\n",
    "print (\"Number of test rows with any nan: %i\" % num_rows_with_nan)\n",
    "\n",
    "# Calculate the percentage of rows with NaN values\n",
    "print (\"Percent of test rows with any nan: %f\" % ((num_rows_with_nan / len(X_test_keep_missing)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A) Simple imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat = 'mean'\n",
    "# NOTE: the following code imputes X_test based on the imputer fitted to X_train\n",
    "\n",
    "\"\"\"\n",
    "STRATEGIES\n",
    "If “mean”, then replace missing values using the mean along each column. Can only be used with numeric data.\n",
    "\n",
    "If “median”, then replace missing values using the median along each column. Can only be used with numeric data.\n",
    "\n",
    "If “most_frequent”, then replace missing using the most frequent value along each column. Can be used with strings or numeric data. If there is more than one such value, only the smallest is returned.\n",
    "\n",
    "If “constant”, then replace missing values with fill_value. Can be used with strings or numeric data.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy=strat)\n",
    "imputer = imputer.fit(X_train_keep_missing)\n",
    "X_train_imputed = imputer.transform(X_train_keep_missing)\n",
    "X_test_imputed = imputer.transform(X_test_keep_missing)\n",
    "# convert into pandas dataframe instead of np array\n",
    "X_train = pd.DataFrame(X_train_imputed, columns=X_train_keep_missing.columns)\n",
    "X_test = pd.DataFrame(X_test_imputed, columns=X_test_keep_missing.columns)\n",
    "\n",
    "y_train = y_train_keep_missing\n",
    "y_test = y_test_keep_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# B) Multiple imputations (skip this section if not doing imputations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good article on MCAR vs MAR vs MNAR and how to appropriately handle missing values in each case: https://datascience.stackexchange.com/questions/116622/what-should-you-do-with-nan-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def handle_missing(X_transformed):\n",
    "    ### Handle missing values\n",
    "    # Xu:\n",
    "    # - In the past, we’ve used multiple imputation as long as the % of missing values was less than 10% for any given variable. I attached a paper we wrote where we used this technique. \n",
    "    # - Balancing can be done by upsampling the minority class, although in this case the two are fairly similar in number.\"\n",
    "    # https://scikit-learn.org/stable/modules/impute.html\n",
    "    \n",
    "    ### old simple imputation method\n",
    "    #from sklearn.impute import SimpleImputer\n",
    "    #imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    #imputer = imputer.fit(X_transformed)\n",
    "    #X_transformed[:] = imputer.transform(X_transformed) # [:] modifies the df in place\n",
    "\n",
    "# return list of pandas dataframes, each containing 1 of 10 imputations\n",
    "def mult_impute_missing(X, train_data=None):\n",
    "    if train_data is None:\n",
    "        train_data = X\n",
    "\n",
    "    ### multiple imputation method\n",
    "    # IterativeImputer from sklearn is an alternative multivariate simple imputation method, that can be changed to be a multiple imputation method by iterating through multiple random states. However, this \n",
    "    from sklearn.experimental import enable_iterative_imputer\n",
    "    from sklearn.impute import IterativeImputer\n",
    "\n",
    "    imp = IterativeImputer(max_iter=10, random_state=0, sample_posterior=True)\n",
    "\n",
    "    imputed_datasets = []\n",
    "    for i in range(10): # 3-10 imputations standard\n",
    "        imp.random_state = i\n",
    "        imp.fit(train_data)\n",
    "        X_imputed = imp.transform(X)\n",
    "        imputed_datasets.append(pd.DataFrame(X_imputed, columns=X.columns))\n",
    "\n",
    "    # ALTERNATIVE\n",
    "    #from statsmodels.imputation import mice.MICEData # alternative package for MICE imputation\n",
    "    # official docs: https://www.statsmodels.org/dev/generated/statsmodels.imputation.mice.MICE.html#statsmodels.imputation.mice.MICE\n",
    "    # multiple imputation example using statsmodels: https://github.com/kshedden/mice_workshop\n",
    "    #imp = mice.MICEData(data)\n",
    "    #fml = 'y ~ x1 + x2 + x3 + x4' # variables used in multiple imputation model\n",
    "    #mice = mice.MICE(fml, sm.OLS, imp) # OLS chosen; can change this up\n",
    "    #results = mice.fit(10, 10) # 10 burn-in cycles to skip, 10 imputations\n",
    "    #print(results.summary())\n",
    "    \n",
    "    return imputed_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputedsets = mult_impute_missing(X_train_keep_missing) # list of 10 imputed X_trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_imputedsets = mult_impute_missing(X_test_keep_missing, train_data=X_test_keep_missing) # Impute test data using model fit with training data, not with test data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# C) Drop NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with nan\n",
    "X_train = X_train_keep_missing.dropna()\n",
    "X_test = X_test_keep_missing.dropna()\n",
    "\n",
    "y_train = y_train_keep_missing[y_train_keep_missing.index.isin(X_train.index)]\n",
    "y_test = y_test_keep_missing[y_test_keep_missing.index.isin(X_test.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# print model coefficients, ORs, p-values\n",
    "def model_summary(model, X_train):\n",
    "    coefs = model.coef_[0]\n",
    "    # odd ratios = e^coef\n",
    "    ors = np.exp(coefs)\n",
    "    intercept = model.intercept_[0]\n",
    "\n",
    "    ### 2 ways to calculate p-values; NOTE THAT P VALUES MAY NOT MAKE SENSE FOR REGULARIZED MODELS\n",
    "    # https://stackoverflow.com/questions/25122999/scikit-learn-how-to-check-coefficients-significance\n",
    "    def logit_pvalue(model, x):\n",
    "        \"\"\" Calculate z-scores for scikit-learn LogisticRegression.\n",
    "        parameters:\n",
    "            model: fitted sklearn.linear_model.LogisticRegression with intercept and large C\n",
    "            x:     matrix on which the model was fit\n",
    "        This function uses asymtptics for maximum likelihood estimates.\n",
    "        \"\"\"\n",
    "        p = model.predict_proba(x)\n",
    "        n = len(p)\n",
    "        m = len(model.coef_[0]) + 1\n",
    "        coefs = np.concatenate([model.intercept_, model.coef_[0]])\n",
    "        x_full = np.matrix(np.insert(np.array(x), 0, 1, axis = 1))\n",
    "        ans = np.zeros((m, m))\n",
    "        for i in range(n):\n",
    "            ans = ans + np.dot(np.transpose(x_full[i, :]), x_full[i, :]) * p[i,1] * p[i, 0]\n",
    "        vcov = np.linalg.inv(np.matrix(ans))\n",
    "        se = np.sqrt(np.diag(vcov))\n",
    "        t =  coefs/se  \n",
    "        p = (1 - norm.cdf(abs(t))) * 2\n",
    "        return p\n",
    "    p_values = logit_pvalue(model, X_train)\n",
    "    def format_pvalue(p_values):\n",
    "        f = [\"<.001\" if x<0.001 else \"%.3f\"%x for x in p_values]\n",
    "        return f\n",
    "    #print(format_pvalue(pvalues))\n",
    "    \n",
    "    # compare with statsmodels ### RESULT: produces same result except gives nan instead of 1.00 for insignficant p-values\n",
    "    #import statsmodels.api as sm\n",
    "    #sm_model = sm.Logit(y_train, sm.add_constant(X_train)).fit(disp=0) ### these uses y_train from outside this function so not really valid but oh well I just want it for testing purposes\n",
    "    #p_values=sm_model.pvalues\n",
    "    #print(format_pvalue(pvalues))\n",
    "    #sm_model.summary()\n",
    "\n",
    "    # print results\n",
    "    results = pd.DataFrame({\n",
    "        'Coefficient': np.append(intercept, coefs),\n",
    "        'Odds Ratio': np.append(np.exp(intercept), ors),\n",
    "        'P-value': format_pvalue(p_values)\n",
    "    }, index=['Intercept'] + list(X_train.columns))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model performance\n",
    "# https://medium.com/javarevisited/evaluating-the-logistic-regression-ae2decf42d61\n",
    "\n",
    "print('Training set count: %i' % len(X_train))\n",
    "print('Test set count: %i' % len(X_test))\n",
    "\n",
    "def compute_performance(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    import sklearn.metrics as metrics\n",
    "    # evaluate predictions\n",
    "    mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "    print('MAE: %.3f' % mae)\n",
    "    \n",
    "    # examine the class distribution of the testing set (using a Pandas Series method)\n",
    "    y_test.value_counts()\n",
    "    \n",
    "    # calculate the percentage of ones\n",
    "    # because y_test only contains ones and zeros, we can simply calculate the mean = percentage of ones\n",
    "    y_test.mean()\n",
    "    \n",
    "    # calculate the percentage of zeros\n",
    "    1 - y_test.mean()\n",
    "    \n",
    "    \n",
    "    # # Metrics computed from a confusion matrix (before thresholding)\n",
    "    \n",
    "    # Confusion matrix is used to evaluate the correctness of a classification model\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    confusion_matrix = confusion_matrix(y_test,y_pred)\n",
    "    confusion_matrix\n",
    "    \n",
    "    TP = confusion_matrix[1, 1]\n",
    "    TN = confusion_matrix[0, 0]\n",
    "    FP = confusion_matrix[0, 1]\n",
    "    FN = confusion_matrix[1, 0]\n",
    "    \n",
    "    # Classification Accuracy: Overall, how often is the classifier correct?\n",
    "    # use float to perform true division, not integer division\n",
    "    # print((TP + TN) / sum(map(sum, confusion_matrix))) -- this is is the same as the below automatic method\n",
    "    print('Accuracy: %.3f' % metrics.accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    # Sensitivity(recall): When the actual value is positive, how often is the prediction correct?\n",
    "    sensitivity = TP / float(FN + TP)\n",
    "    \n",
    "    print('Sensitivity: %.3f' % sensitivity)\n",
    "    # print('Recall score: %.3f' % metrics.recall_score(y_test, y_pred)) # same thing as sensitivity, but recall term used in ML\n",
    "    \n",
    "    # Specificity: When the actual value is negative, how often is the prediction correct?\n",
    "    specificity = TN / (TN + FP)\n",
    "    print('Specificity: %.3f' % specificity)\n",
    "    \n",
    "    #from imblearn.metrics import specificity_score\n",
    "    #specificity_score(y_test, y_pred)\n",
    "\n",
    "    # False Positive Rate: When the actual value is negative, how often is the prediction incorrect?\n",
    "    false_positive_rate = FP / float(TN + FP)\n",
    "    print('FPR: %.3f' % false_positive_rate)\n",
    "    # print(1 - specificity) # same as FPR\n",
    "    \n",
    "    # Precision: When a positive value is predicted, how often is the prediction correct?\n",
    "    precision = TP / float(TP + FP)\n",
    "    #print('Precision: %.3f' % precision)\n",
    "    print('Precision: %.3f' % metrics.precision_score(y_test, y_pred))\n",
    "    \n",
    "    # F score\n",
    "    f_score = 2*TP / (2*TP + FP + FN)\n",
    "    #print('F score: %.3f' % f_score)\n",
    "    print('F1 score: %.3f' % metrics.f1_score(y_test,y_pred))\n",
    "    \n",
    "    #Evaluate the model using other performance metrics - A BIT REDUNDANT, COMMENTED OUT FOR NOW\n",
    "    # from sklearn.metrics import classification_report\n",
    "    # print(classification_report(y_test,y_pred))\n",
    "\n",
    "    # AUC\n",
    "    y_pred_proba = model.predict_proba(X_test)[::,1]\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "    print('AUC: %.3f' % auc)\n",
    "\n",
    "    # CM matrix plot\n",
    "    from sklearn import metrics\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = None)\n",
    "    \n",
    "    cm_display.plot()\n",
    "    plt.show()\n",
    "    # 0 = GS, 1 = POAG\n",
    "    \n",
    "    # ROC curve plot\n",
    "    plt.plot(fpr,tpr,label=\"auc=\"+str(auc))\n",
    "    plt.xlabel(\"False positive rate (1-specificity)\")\n",
    "    plt.ylabel(\"True positive rate (sensitivity)\")\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression DROPNA or SIMPLEIMPUTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty=None#'l1', 'l2', 'elasticnet', or None\n",
    "solver='saga' # 'lbfgs', 'saga' (only saga supports l1 and elasticnet)\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression(random_state=16, solver=solver, max_iter=1000, penalty=penalty)\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(X_train, y_train)\n",
    "model_summary(logreg, X_train)\n",
    "compute_performance(logreg, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using statsmodels instead -- IGNORE THIS CELL, I FIGURED OUT THAT P-VALUES AREN'T REALLY APPROPRIATE FOR REGULARIZED REGRESSION --> JUST USE SKLEARN PACKAGE\n",
    "\"\"\"\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Adding a constant term for the intercept\n",
    "X = sm.add_constant(X_train)\n",
    "model = sm.Logit(list(y_train), X)\n",
    "\n",
    "# normal logistic regression\n",
    "result = model.fit()\n",
    "result.summary()\n",
    "\n",
    "# Lasso (L1) logistic regression\n",
    "# i think the statsmodels fit_regularized method for logistic regression only has the lasso method?\n",
    "lasso_logit = model.fit_regularized(method='l1', alpha=0.1)\n",
    "lasso_logit.summary()\n",
    "                                    \n",
    "# Elastic Net (L1 and L2) logistic regression\n",
    "# has to be implemented using GLM, whereby choosing binomial family is essentially logistic regression\n",
    "# model = sm.GLM(list(y_train), X, family=sm.families.Binomial())\n",
    "# elastic_net_logit = model.fit_regularized(method='elastic_net', alpha=0.1) # https://www.statsmodels.org/devel/generated/statsmodels.genmod.generalized_linear_model.GLM.fit_regularized.html\n",
    "#print(dir(elastic_net_logit))\n",
    "#elastic_net_logit.params\n",
    "#elastic_net_logit.summary() # this is not implemented\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Regularization params\n",
    "k_folds = 10 #5-10 standard\n",
    "scoring = 'roc_auc' # 'accuracy' (default), 'roc_auc', 'neg_mean_absolute_error' ...options on sklearn.metrics: https://scikit-learn.org/stable/api/sklearn.metrics.html#module-sklearn.metrics\n",
    "max_iter=1000\n",
    "solver='saga'\n",
    "# for elastic net only:\n",
    "lambda_inverse = 100  # of C's (=inverse of lambda) to try; 10 by default\n",
    "alpha_range = np.linspace(0, 1, 100)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Ridge\n",
    "ridge_cv = LogisticRegressionCV(cv=k_folds, scoring=scoring, solver=solver, max_iter=max_iter)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "# Retrieve the best hyperparameters\n",
    "best_C = ridge_cv.C_[0]\n",
    "print(f\"Best C (inverse of regularization strength): {best_C}\")\n",
    "\n",
    "model_summary(ridge_cv, X_train)\n",
    "compute_performance(ridge_cv, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Elastic Net\n",
    "#https://stackoverflow.com/questions/66787845/how-to-perform-elastic-net-for-a-classification-problem\n",
    "# SAGA should be considered more advanced and used over SAG. For more information, see: https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-defintions\n",
    "en_cv = LogisticRegressionCV(cv=k_folds, scoring=scoring, penalty='elasticnet', Cs = lambda_inverse, l1_ratios=alpha_range, solver=solver, max_iter=max_iter)\n",
    "en_cv.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "best_C = en_cv.C_[0]\n",
    "best_l1_ratio = en_cv.l1_ratio_[0]\n",
    "print(f\"Best C (inverse of regularization strength): {best_C}\")\n",
    "print(f\"Best l1_ratio (mixing parameter): {best_l1_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best C (inverse of regularization strength): {best_C}\")\n",
    "print(f\"Best l1_ratio (mixing parameter): {best_l1_ratio}\")\n",
    "model_summary(en_cv, X_train)\n",
    "compute_performance(en_cv, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Logistic Regression MULTIPLE IMPUTATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to do prediction after multiple imputation:\n",
    "# https://github.com/amices/mice/issues/82\n",
    "# https://stackoverflow.com/questions/68460923/how-to-do-the-prediction-after-multiple-imputation-with-mice-package\n",
    "logreg_models = []\n",
    "ypred_results = []\n",
    "\n",
    "for X_train in X_train_imputedsets:\n",
    "    # instantiate the model (using the default parameters)\n",
    "    logreg = LogisticRegression(random_state=16, solver='lbfgs', max_iter=1000)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    logreg_models.append(logreg)\n",
    "\n",
    "    for X_test in X_test_imputedsets:\n",
    "        y_pred = logreg.predict(X_test)\n",
    "        ypred_results.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just took the mode for predictions from my 10 imputations, each fitted on 1 of 10 train imputations\n",
    "# figure out if there's a better method for this\n",
    "from scipy.stats import mode\n",
    "y_pred = mode(ypred_results, axis=0).mode\n",
    "\n",
    "# does the same thing:\n",
    "#ypred_df = pd.DataFrame(np.row_stack(ypred_results)) # 10x10 df\n",
    "#y_pred = ypred_df.mode(axis=0).loc[0].astype(int) # get mode of each column\n",
    "#np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### After performing logistic regression on each imputed dataset, pool the results using Rubin’s rules to obtain a single set of estimates.\n",
    "\n",
    "# Extract coefficients and standard errors\n",
    "coefs = np.array([model.coef_[0] for model in logreg_models])\n",
    "intercepts = np.array([model.intercept_[0] for model in logreg_models])\n",
    "\n",
    "# Calculate pooled estimates\n",
    "pooled_coefs = np.mean(coefs, axis=0)\n",
    "pooled_intercept = np.mean(intercepts)\n",
    "# I think this calculates SES between the imputed datasets\n",
    "pooled_ses = np.sqrt(np.mean(coefs**2, axis=0) + np.var(coefs, axis=0, ddof=1) * (1 + 1/len(logreg_models)))\n",
    "\n",
    "# Display pooled results\n",
    "results = pd.DataFrame({\n",
    "    'Coefficient': np.append(pooled_intercept, pooled_coefs),\n",
    "    'Standard Error': np.append(np.nan, pooled_ses)  # Intercept SE is not calculated here\n",
    "}, index=['Intercept'] + list(X_train.columns))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# template stuff I haven't deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View data\n",
    "\n",
    "# subject = pd.read_csv(configuration_records.bag_paths[0]/'data/Subject.csv')\n",
    "# subject\n",
    "\n",
    "# observation = pd.read_csv(configuration_records.bag_paths[0]/'data/Observation.csv')\n",
    "# observation\n",
    "\n",
    "# clinic = pd.read_csv(configuration_records.bag_paths[0]/'data/Clinical_Records.csv')\n",
    "# clinic\n",
    "\n",
    "# observation_clinic_asso = pd.read_csv(configuration_records.bag_paths[0]/'data/Observation_Clinic_Asso.csv')\n",
    "# observation_clinic_asso # association table between observation table and clinic record table\n",
    "\n",
    "# icd10 = pd.read_csv(configuration_records.bag_paths[0]/'data/Clinic_ICD10.csv')\n",
    "# icd10\n",
    "\n",
    "# icd10_asso = pd.read_csv(configuration_records.bag_paths[0]/'data/Clinic_ICD_Asso.csv')\n",
    "# icd10_asso # association table between clinic record table and ICD10 code\n",
    "\n",
    "# report = pd.read_csv(configuration_records.bag_paths[0]/'data/Report.csv')\n",
    "# report\n",
    "\n",
    "# RNFL_OCR = pd.read_csv(configuration_records.bag_paths[0]/'data/RNFL_OCR.csv')\n",
    "# RNFL_OCR\n",
    "\n",
    "HVF_OCR = pd.read_csv(configuration_records.bag_paths[0]/'data/HVF_OCR.csv')\n",
    "HVF_OCR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Execute Training algorithm\n",
    "from eye_ai.models.vgg19_hyper_parameter_tuning import main #import the new logistic module.\n",
    "with EA.execution(execution_rid=configuration_records.execution_rid) as exec:\n",
    "  main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Save Execution Assets (model) and Metadata\n",
    "uploaded_assets = EA.execution_upload(configuration_records.execution_rid, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
