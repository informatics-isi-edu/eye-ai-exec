{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/informatics-isi-edu/eye-ai-exec/blob/main/notebooks/VGG19_Diagnosis_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVq_jMdfx7Ni"
   },
   "source": [
    "# Multimodal Initial analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# if IN_COLAB:\n",
    "#     !pip install deriva\n",
    "#     !pip install bdbag\n",
    "#     !pip install --upgrade --force pydantic\n",
    "#     !pip install git+https://github.com/informatics-isi-edu/deriva-ml git+https://github.com/informatics-isi-edu/eye-ai-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = \"Repos\"   # Set this to be where your github repos are located.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Update the load path so python can find modules for the model\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.home() / repo_dir / \"eye-ai-ml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "\n",
    "import json\n",
    "import os\n",
    "from eye_ai.eye_ai import EyeAI\n",
    "import pandas as pd\n",
    "from pathlib import Path, PurePath\n",
    "import logging\n",
    "# import torch\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Import label encoder \n",
    "from sklearn import preprocessing \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from deriva.core.utils.globus_auth_utils import GlobusNativeLogin\n",
    "catalog_id = \"eye-ai\" #@param\n",
    "host = 'www.eye-ai.org'\n",
    "\n",
    "\n",
    "gnl = GlobusNativeLogin(host=host)\n",
    "if gnl.is_logged_in([host]):\n",
    "    print(\"You are already logged in.\")\n",
    "else:\n",
    "    gnl.login([host], no_local_server=True, no_browser=True, refresh_tokens=True, update_bdbag_keychain=True)\n",
    "    print(\"Login Successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imD3DJ4lx7Nm"
   },
   "source": [
    "Connect to Eye-AI catalog.  Configure to store data local cache and working directories.  Initialize Eye-AI for pending execution based on the provided configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to configure the rest of the notebook.\n",
    "\n",
    "cache_dir = '/data'        # Directory in which to cache materialized BDBags for datasets\n",
    "working_dir = '/data'    # Directory in which to place output files for later upload.\n",
    "\n",
    "configuration_rid= \"2-CCD4\" # rid I created with my config containing minid for both train and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EA = EyeAI(hostname = host, catalog_id = catalog_id, cache_dir= cache_dir, working_dir=working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Initiate an Execution\n",
    "configuration_records = EA.execution_init(configuration_rid=configuration_rid)\n",
    "configuration_records.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate multimodal wide table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN: configuration_records.bag_paths[0]\n",
    "wide_train_raw = EA.severity_analysis(configuration_records.bag_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: configuration_records.bag_paths[1]\n",
    "wide_test_raw = EA.severity_analysis(configuration_records.bag_paths[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add age to table\n",
    "age_path = \"/data/yukim3003/EyeAI_working/Execution_Assets/Multimodal_Analysis/multimodal_subject_age.csv\"\n",
    "age_df = pd.read_csv(age_path)\n",
    "age_df.rename(columns={'RID': 'RID_Subject'}, inplace=True)\n",
    "wide_train_raw = wide_train_raw.merge(age_df, on='RID_Subject', how='left')\n",
    "wide_test_raw = wide_test_raw.merge(age_df, on='RID_Subject', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new table with only more severe eye for each patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current severity rule: prioritize RNFL > HVF > CDR\n",
    "# if don't want thresholds, just make threshold 0\n",
    "# just return the first eye if RNFL, MD, CDR all NaN\n",
    "def pick_severe_eye(df, rnfl_threshold, md_threshold):\n",
    "    # Sort by 'Average_RNFL_Thickness(μm)', 'MD', and 'CDR' in descending order\n",
    "    df = df.sort_values(by=['Average_RNFL_Thickness(μm)', 'MD', 'CDR'], ascending=[True, True, False])\n",
    "\n",
    "    ### 1. if only 1 eye has a label, just pick that eye as more severe eye (for Dr. Song's patients)\n",
    "    df = df.groupby('RID_Subject').apply(lambda group: group[group['Label'].notna()]).reset_index(drop=True)\n",
    "    \n",
    "    # 2. Select the row/eye with most severe value within the thresholds\n",
    "    def select_row(group):\n",
    "        max_value = group['Average_RNFL_Thickness(μm)'].min() # min is more severe for RNFL\n",
    "        within_value_threshold = group[np.abs(group['Average_RNFL_Thickness(μm)'] - max_value) <= rnfl_threshold] # identify eyes within threshold\n",
    "\n",
    "        if len(within_value_threshold) > 1 or len(within_value_threshold) == 0: # if both eyes \"equal\" RNFL OR if RNFL is NaN, then try MD\n",
    "            max_other_column = within_value_threshold['MD'].min() # min is more severe for MD\n",
    "            within_other_column_threshold = within_value_threshold[np.abs(within_value_threshold['MD'] - max_other_column) <= md_threshold]\n",
    "\n",
    "            if len(within_other_column_threshold) > 1 or len(within_other_column_threshold) == 0: # if both eyes \"equal\" MD OR if MD is NaN, then try CDR\n",
    "                return group.sort_values(by=['CDR'], ascending=[False]).iloc[0] # since i didn't set CDR threshold, this will always pick something (even if NaN)\n",
    "            else:\n",
    "                return within_other_column_threshold.iloc[0]\n",
    "        else:\n",
    "            return within_value_threshold.iloc[0]\n",
    "    return df.groupby('RID_Subject').apply(select_row).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_train_nothresh = pick_severe_eye(wide_train_raw, 0, 0)\n",
    "wide_test_nothresh = pick_severe_eye(wide_test_raw, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnfl_thresh = 0\n",
    "md_thresh = 0\n",
    "wide_train = pick_severe_eye(wide_train_raw, rnfl_thresh, md_thresh)\n",
    "wide_test = pick_severe_eye(wide_test_raw, rnfl_thresh, md_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show which subjects changed eyes by adding thresholds\n",
    "diff_values = wide_train.compare(wide_train_nothresh, align_axis=0, keep_shape=True, keep_equal=True) #keep_equal=False --> values that are equal are represented as NaN\n",
    "diff_values = diff_values.drop_duplicates(keep=False) # drop rows that have a duplicate\n",
    "print(\"# subjects where eye choice changed: %i\" % (len(diff_values)/2))\n",
    "diff_values[['RID_Subject', 'Side', 'Label', 'Average_RNFL_Thickness(μm)', 'MD', 'CDR']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in features and target variable\n",
    "demographic_fx = ['Gender', 'Ethnicity', 'Age']\n",
    "clinic_fx = ['LogMAR_VA', 'IOP'] # 'Gonioscopy' - mostly NaN, not standardized annotation # CCT - mostly NaN\n",
    "CDR_fx = ['CDR']\n",
    "RNFL_fx = ['Average_RNFL_Thickness(μm)'] # Average_C/D_Ratio - for RNFL-derived CDR\n",
    "RNFL_clockhr_fx = ['Clock_Hours_1', 'Clock_Hours_2', 'Clock_Hours_3', 'Clock_Hours_4', 'Clock_Hours_5', 'Clock_Hours_6', 'Clock_Hours_7', 'Clock_Hours_8', 'Clock_Hours_9', 'Clock_Hours_10', 'Clock_Hours_11', 'Clock_Hours_12'] # if I want to use each clock hour\n",
    "RNFL_quad_fx = ['Quadrants_S', 'Quadrants_N', 'Quadrants_T', 'Quadrants_I']\n",
    "RNFL_IS_fx = ['Quadrants_S', 'Quadrants_I']\n",
    "HVF_fx = ['MD', 'VFI'] # 'PSD' - mostly NaN. I think PSD and PSD.1 columns should be merged to use this column if desired\n",
    "GHT = ['GHT']\n",
    "\n",
    "## Adjust fx_cols depending on what variables I want to include in model\n",
    "# Clinic Data\n",
    "fx_cols = demographic_fx + clinic_fx\n",
    "# OCT+HVF\n",
    "#fx_cols = ['MD'] + RNFL_fx\n",
    "# CDR+OCT+HVF\n",
    "#fx_cols = ['CDR', 'MD'] + RNFL_fx\n",
    "\n",
    "# All Project Fx - also use this if running univariate analysis or elastic net\n",
    "#fx_cols = demographic_fx + clinic_fx + CDR_fx + HVF_fx + RNFL_fx + RNFL_IS_fx + GHT\n",
    "\n",
    "# Domain knowledge fx\n",
    "#fx_cols = ['Age', 'Gender', 'LogMAR_VA', 'CDR'] + RNFL_fx + ['MD', 'GHT']  # Age, Gender, VA, CDR, avg RNFL, MD, GHT outside normal limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Train and Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transferred to eye_ai.py\n",
    "# def transform_data(multimodal_wide, fx_cols, y_method=\"all_glaucoma\" or \"urgent_glaucoma\"):\n",
    "# Returns: X_transformed, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_keep_missing, y_train_keep_missing = EA.transform_data(wide_train, fx_cols, y_method=\"all_glaucoma\")\n",
    "X_test_keep_missing, y_test_keep_missing = EA.transform_data(wide_test, fx_cols, y_method=\"all_glaucoma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counts / data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train_keep_missing) + len(X_test_keep_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.unique(y_train_keep_missing, return_counts=True)\n",
    "print(counts) # #GS vs #Glaucoma\n",
    "print(\"Percent GS vs Glaucoma in TRAIN:\", counts[1] / sum(counts[1])) # percent\n",
    "\n",
    "counts = np.unique(y_test_keep_missing, return_counts=True)\n",
    "print(counts) # #GS vs #Glaucoma\n",
    "print(\"Percent GS vs Glaucoma in TEST:\", counts[1] / sum(counts[1])) # percent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = sum(X_train_keep_missing['Gender_M']) + sum(X_test_keep_missing['Gender_M'])\n",
    "print(\"Num male:\", counts)\n",
    "counts = sum(X_train_keep_missing['Gender_F']) + sum(X_test_keep_missing['Gender_F'])\n",
    "print(\"Num female:\", counts)\n",
    "\n",
    "mean_age = (np.sum(X_train_keep_missing['Age']) + np.sum(X_test_keep_missing['Age'])) / (len(X_train_keep_missing) + len(X_test_keep_missing))\n",
    "print(\"Mean age:\", mean_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #NAN\n",
    "### the number of rows with nan in any column will increase if I choose more features\n",
    "\n",
    "# count number / percent of rows with nan value\n",
    "num_rows_with_nan = X_train_keep_missing.isnull().any(axis=1).sum()\n",
    "print (\"Number of train rows with any nan: %i\" % num_rows_with_nan)\n",
    "\n",
    "# Calculate the percentage of rows with NaN values\n",
    "print (\"Percent of train rows with any nan: %f\" % ((num_rows_with_nan / len(X_train_keep_missing)) * 100))\n",
    "\n",
    "# count number / percent of rows with nan value\n",
    "num_rows_with_nan = X_test_keep_missing.isnull().any(axis=1).sum()\n",
    "print (\"Number of test rows with any nan: %i\" % num_rows_with_nan)\n",
    "\n",
    "# Calculate the percentage of rows with NaN values\n",
    "print (\"Percent of test rows with any nan: %f\" % ((num_rows_with_nan / len(X_test_keep_missing)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize Data\n",
    "#### may not be required for univariate analysis to improve interpretability (but still good to center/scale to improve Gaussian-ness of distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### normalize numeric training data (so that features are on same scale instead of wildly different scales)\n",
    "# not required for typical logistic regression, but do need for regularized regression\n",
    "# I didn't put this in transform_data because I want to use the scaler fitted on train for test too\n",
    "\n",
    "# how? https://datascience.stackexchange.com/questions/54908/data-normalization-before-or-after-train-test-split\n",
    "# why? https://stackoverflow.com/questions/52670012/convergencewarning-liblinear-failed-to-converge-increase-the-number-of-iterati\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "categorical_vars = ['Gender', 'Ethnicity', 'GHT']\n",
    "numeric_vars = sorted(set(fx_cols) - set(categorical_vars), key=fx_cols.index)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "normalized_numeric_X_train = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train_keep_missing[numeric_vars]),\n",
    "    columns = numeric_vars\n",
    ")\n",
    "cat_df = X_train_keep_missing.drop(numeric_vars, axis=1)\n",
    "X_train_keep_missing = pd.concat([normalized_numeric_X_train.set_index(cat_df.index), cat_df], axis=1)\n",
    "\n",
    "# normalize test data, but using scaler fitted to training data to prevent data leakage\n",
    "normalized_numeric_X_test = pd.DataFrame(\n",
    "    scaler.transform(X_test_keep_missing[numeric_vars]),\n",
    "    columns = numeric_vars\n",
    ")\n",
    "cat_df = X_test_keep_missing.drop(numeric_vars, axis=1)\n",
    "X_test_keep_missing = pd.concat([normalized_numeric_X_test.set_index(cat_df.index), cat_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# A) Simple imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat = 'mean'\n",
    "# NOTE: the following code imputes X_test based on the imputer fitted to X_train\n",
    "\n",
    "\"\"\"\n",
    "STRATEGIES\n",
    "If “mean”, then replace missing values using the mean along each column. Can only be used with numeric data.\n",
    "\n",
    "If “median”, then replace missing values using the median along each column. Can only be used with numeric data.\n",
    "\n",
    "If “most_frequent”, then replace missing using the most frequent value along each column. Can be used with strings or numeric data. If there is more than one such value, only the smallest is returned.\n",
    "\n",
    "If “constant”, then replace missing values with fill_value. Can be used with strings or numeric data.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy=strat)\n",
    "imputer = imputer.fit(X_train_keep_missing)\n",
    "X_train_imputed = imputer.transform(X_train_keep_missing)\n",
    "X_test_imputed = imputer.transform(X_test_keep_missing)\n",
    "# convert into pandas dataframe instead of np array\n",
    "X_train = pd.DataFrame(X_train_imputed, columns=X_train_keep_missing.columns)\n",
    "X_test = pd.DataFrame(X_test_imputed, columns=X_test_keep_missing.columns)\n",
    "\n",
    "y_train = y_train_keep_missing\n",
    "y_test = y_test_keep_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# B) Multiple imputations (10 imputations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good article on MCAR vs MAR vs MNAR and how to appropriately handle missing values in each case: https://datascience.stackexchange.com/questions/116622/what-should-you-do-with-nan-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return list of pandas dataframes, each containing 1 of 10 imputations\n",
    "def mult_impute_missing(X, train_data=None):\n",
    "    if train_data is None:\n",
    "        train_data = X\n",
    "\n",
    "    ### multiple imputation method using IterativeImputer from sklearn \n",
    "    from sklearn.experimental import enable_iterative_imputer\n",
    "    from sklearn.impute import IterativeImputer\n",
    "\n",
    "    imp = IterativeImputer(max_iter=10, random_state=0, sample_posterior=True)\n",
    "\n",
    "    imputed_datasets = []\n",
    "    for i in range(10): # 3-10 imputations standard\n",
    "        imp.random_state = i\n",
    "        imp.fit(train_data)\n",
    "        X_imputed = imp.transform(X)\n",
    "        imputed_datasets.append(pd.DataFrame(X_imputed, columns=X.columns))\n",
    "\n",
    "    # ALTERNATIVE\n",
    "    #from statsmodels.imputation import mice.MICEData # alternative package for MICE imputation\n",
    "    # official docs: https://www.statsmodels.org/dev/generated/statsmodels.imputation.mice.MICE.html#statsmodels.imputation.mice.MICE\n",
    "    # multiple imputation example using statsmodels: https://github.com/kshedden/mice_workshop\n",
    "    #imp = mice.MICEData(data)\n",
    "    #fml = 'y ~ x1 + x2 + x3 + x4' # variables used in multiple imputation model\n",
    "    #mice = mice.MICE(fml, sm.OLS, imp) # OLS chosen; can change this up\n",
    "    #results = mice.fit(10, 10) # 10 burn-in cycles to skip, 10 imputations\n",
    "    #print(results.summary())\n",
    "    \n",
    "    return imputed_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputedsets = mult_impute_missing(X_train_keep_missing) # list of 10 imputed X_trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_imputedsets = mult_impute_missing(X_test_keep_missing, train_data=X_test_keep_missing) # Impute test data using model fit with training data, not with test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train_keep_missing\n",
    "y_test = y_test_keep_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C) Drop NA\n",
    "### DON'T use this with the univariate loop -- incorrectly drops rows (dropNA in univariate loop instead to drop only for the univariate variable in question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with nan\n",
    "X_train = X_train_keep_missing.dropna()\n",
    "X_test = X_test_keep_missing.dropna()\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "\n",
    "y_train = y_train_keep_missing[y_train_keep_missing.index.isin(X_train.index)]\n",
    "y_test = y_test_keep_missing[y_test_keep_missing.index.isin(X_test.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transferred to eye_ai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    def model_summary(self, model, X_train):\n",
    "    def calc_stats(self, y_pred, y_test):\n",
    "    def compute_performance(self, model, X_test, y_test):\n",
    "    def compute_performance_youden(self, model, X_test, y_test, plot=True):\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Simple Logistic Regression loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_NA=True # change if do simple imputation instead\n",
    "if drop_NA:\n",
    "    X_train = X_train_keep_missing\n",
    "    X_test = X_test_keep_missing\n",
    "    y_train = y_train_keep_missing\n",
    "    y_test = y_test_keep_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all feature columns\n",
    "# (code isn't all that different from the simple one below, I just wanted to use loop instead of doing by hand)\n",
    "\n",
    "# MUST DROP REFERENCE COLUMN FOR ONE-HOT-ENCODED VARIABLES\n",
    "chosen_ref_labels = ['GHT_Within Normal Limits', 'Gender_M', 'Ethnicity_Other']\n",
    "X_train = X_train.drop(columns=chosen_ref_labels)\n",
    "X_test = X_test.drop(columns=chosen_ref_labels)\n",
    "\n",
    "penalty=None#'l1', 'l2', 'elasticnet', or None\n",
    "solver='saga' # 'lbfgs', 'saga' (only saga supports l1 and elasticnet)\n",
    "\n",
    "# save models in dict to access later\n",
    "models_univariate = {} # model label name: (model, associated X_test, associated y_test)\n",
    "\n",
    "def process_fx(fx, X, X_t, Y, Y_t):\n",
    "    logreg = LogisticRegression(random_state=16, solver=solver, max_iter=1000, penalty=penalty)\n",
    "\n",
    "    print(fx)\n",
    "    # select all columns that contain fx (because of categorical vars)\n",
    "    cols = [col for col in X.columns if fx in col]\n",
    "    x = X[cols]\n",
    "    x_t = X_t[cols]\n",
    "\n",
    "    if drop_NA:\n",
    "        # Drop NA if desired\n",
    "        x = x.dropna()\n",
    "        x_t = x_t.dropna()\n",
    "\n",
    "        y = Y[Y.index.isin(x.index)]\n",
    "        y_t = Y_t[Y_t.index.isin(x_t.index)]\n",
    "    \n",
    "    # fit the model with data\n",
    "    logreg.fit(x, y)\n",
    "    EA.model_summary(logreg, x)\n",
    "    EA.compute_performance(logreg, x_t, y_t)\n",
    "    EA.compute_performance_youden(logreg, x_t, y_t)\n",
    "    return logreg, x_t, y_t\n",
    "    print(\"\")\n",
    "\n",
    "# fx_cols = demographic_fx + clinic_fx + HVF_fx + RNFL_fx + RNFL_IS_fx + GHT -- must have set this in fx_cols at top before transforming data\n",
    "for fx in fx_cols:\n",
    "    models_univariate[fx] = process_fx(fx, X_train, X_test, y_train, y_test)\n",
    "    print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "    print(\"----------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Logistic Regression DROPNA or SIMPLEIMPUTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIMPLE LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MUST DROP REFERENCE COLUMN FOR ONE-HOT-ENCODED VARIABLES\n",
    "#chosen_ref_labels = ['GHT_Within Normal Limits', 'Gender_M', 'Ethnicity_Other']\n",
    "chosen_ref_labels = ['GHT_Within Normal Limits','GHT_Borderline', 'Gender_M', 'Ethnicity_Other'] \n",
    "drop_cols = [x for x in X_train.columns if x in chosen_ref_labels]\n",
    "X_train = X_train.drop(columns=drop_cols)\n",
    "X_test = X_test.drop(columns=drop_cols)\n",
    "\n",
    "penalty=None#'l1', 'l2', 'elasticnet', or None\n",
    "solver='saga' # 'lbfgs', 'saga' (only saga supports l1 and elasticnet)\n",
    "\n",
    "logreg = LogisticRegression(random_state=16, solver=solver, max_iter=1000, penalty=penalty) \n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(X_train, y_train)\n",
    "EA.model_summary(logreg, X_train)\n",
    "EA.compute_performance(logreg, X_test, y_test)\n",
    "EA.compute_performance_youden(logreg, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge and Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Regularization params\n",
    "k_folds = 10 #5-10 standard\n",
    "scoring = 'roc_auc' # 'neg_log_loss', 'neg_brier_score', 'accuracy' (default), 'roc_auc', 'neg_mean_absolute_error' ...options on sklearn.metrics: https://scikit-learn.org/stable/api/sklearn.metrics.html#module-sklearn.metrics\n",
    "max_iter=1000\n",
    "solver='saga'\n",
    "# for elastic net only:\n",
    "lambda_inverse = 20  # of C's (=inverse of lambda) to try; 10 by default\n",
    "alpha_range = np.linspace(0, 1, 20)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Ridge\n",
    "ridge_cv = LogisticRegressionCV(cv=k_folds, scoring=scoring, solver=solver, max_iter=max_iter)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "# Retrieve the best hyperparameters\n",
    "best_C = ridge_cv.C_[0]\n",
    "print(f\"Best C (inverse of regularization strength): {best_C}\")\n",
    "\n",
    "EA.model_summary(ridge_cv, X_train)\n",
    "EA.compute_performance(ridge_cv, X_test, y_test)\n",
    "EA.compute_performance_youden(ridge_cv, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Elastic Net\n",
    "#https://stackoverflow.com/questions/66787845/how-to-perform-elastic-net-for-a-classification-problem\n",
    "# SAGA should be considered more advanced and used over SAG. For more information, see: https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-defintions\n",
    "en_cv = LogisticRegressionCV(cv=k_folds, scoring=scoring, penalty='elasticnet', Cs = lambda_inverse, l1_ratios=alpha_range, solver=solver, max_iter=max_iter)\n",
    "en_cv.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "best_C = en_cv.C_[0]\n",
    "best_l1_ratio = en_cv.l1_ratio_[0]\n",
    "print(f\"Best C (inverse of regularization strength): {best_C}\")\n",
    "print(f\"Best l1_ratio (mixing parameter): {best_l1_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best C (inverse of regularization strength): {best_C}\")\n",
    "print(f\"Best l1_ratio (mixing parameter): {best_l1_ratio}\")\n",
    "EA.model_summary(en_cv, X_train)\n",
    "EA.compute_performance(en_cv, X_test, y_test)\n",
    "EA.compute_performance_youden(en_cv, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Logistic Regression MULTIPLE IMPUTATIONS\n",
    "### To check if what I did is best method: used mode of y_pred, and averaged prediction probabilities of each imputed model to determine AUC, and averaged p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### After performing logistic regression on each imputed dataset, pool the results using Rubin’s rules to obtain a single set of estimates.\n",
    "\n",
    "# print model coefficients, ORs, p-values\n",
    "def model_summary_mice(logreg_models, Xtrain_finals):\n",
    "    print(\"Training set: %i\" % len(Xtrain_finals[0]))\n",
    "    \n",
    "    # Extract coefficients and standard errors\n",
    "    coefs = np.array([model.coef_[0] for model in logreg_models])\n",
    "    ors = np.exp(coefs)\n",
    "    intercepts = np.array([model.intercept_[0] for model in logreg_models])\n",
    "    p_values = np.array([logit_pvalue(model, Xtrain_finals[i]) for i, model in enumerate(logreg_models)])\n",
    "    \n",
    "    # Calculate pooled estimates\n",
    "    pooled_coefs = np.mean(coefs, axis=0)\n",
    "    pooled_ors = np.mean(ors, axis=0)\n",
    "    pooled_intercept = np.mean(intercepts)\n",
    "    # I think this calculates SES between the imputed datasets\n",
    "    pooled_ses = np.sqrt(np.mean(coefs**2, axis=0) + np.var(coefs, axis=0, ddof=1) * (1 + 1/len(logreg_models)))\n",
    "\n",
    "    pooled_p_values = np.mean(p_values, axis=0)\n",
    "    \n",
    "    # Display pooled results\n",
    "    results = pd.DataFrame({\n",
    "        'Coefficient': format_dec(np.append(pooled_intercept, pooled_coefs)),\n",
    "        'Odds Ratio': format_dec(np.append(np.exp(pooled_intercept), pooled_ors)),\n",
    "        'Standard Error': format_dec(np.append(np.nan, pooled_ses)),  # Intercept SE is not calculated here\n",
    "        'P-value': format_dec(pooled_p_values)\n",
    "    }, index=['Intercept'] + list(Xtrain_finals[0].columns))\n",
    "    print(results)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model performance\n",
    "# https://medium.com/javarevisited/evaluating-the-logistic-regression-ae2decf42d61\n",
    "def compute_performance_mice(logreg_models, Xtest_finals, y_test):\n",
    "    print(\"Test set: %i\" % len(Xtest_finals[0]))\n",
    "    \n",
    "    y_pred_results = []\n",
    "    y_pred_proba_results = []\n",
    "    for model, X_test in zip(logreg_models, Xtest_finals):\n",
    "        y_pred_results.append(model.predict(X_test))\n",
    "        \n",
    "        y_pred_proba_results.append(model.predict_proba(X_test)[::,1])\n",
    "\n",
    "    ypred_df = pd.DataFrame(np.row_stack(y_pred_results))\n",
    "    y_pred = np.array(ypred_df.mode(axis=0).loc[0].astype(int)) ##### used the mode of y_pred across the 10 imputations\n",
    "    y_pred_proba = np.mean(y_pred_proba_results, axis=0)\n",
    "    \n",
    "    \n",
    "    import sklearn.metrics as metrics\n",
    "    # evaluate predictions\n",
    "    mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "    print('MAE: %.3f' % mae)\n",
    "    \n",
    "    # examine the class distribution of the testing set (using a Pandas Series method)\n",
    "    y_test.value_counts()\n",
    "    \n",
    "    # calculate the percentage of ones\n",
    "    # because y_test only contains ones and zeros, we can simply calculate the mean = percentage of ones\n",
    "    y_test.mean()\n",
    "    \n",
    "    # calculate the percentage of zeros\n",
    "    1 - y_test.mean()\n",
    "    \n",
    "    # # Metrics computed from a confusion matrix (before thresholding)\n",
    "    \n",
    "    # Confusion matrix is used to evaluate the correctness of a classification model\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    confusion_matrix = confusion_matrix(y_test,y_pred)\n",
    "    confusion_matrix\n",
    "    \n",
    "    TP = confusion_matrix[1, 1]\n",
    "    TN = confusion_matrix[0, 0]\n",
    "    FP = confusion_matrix[0, 1]\n",
    "    FN = confusion_matrix[1, 0]\n",
    "    \n",
    "    # Classification Accuracy: Overall, how often is the classifier correct?\n",
    "    # use float to perform true division, not integer division\n",
    "    # print((TP + TN) / sum(map(sum, confusion_matrix))) -- this is is the same as the below automatic method\n",
    "    print('Accuracy: %.3f' % metrics.accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    # Sensitivity(recall): When the actual value is positive, how often is the prediction correct?\n",
    "    sensitivity = TP / float(FN + TP)\n",
    "    \n",
    "    print('Sensitivity: %.3f' % sensitivity)\n",
    "    # print('Recall score: %.3f' % metrics.recall_score(y_test, y_pred)) # same thing as sensitivity, but recall term used in ML\n",
    "    \n",
    "    # Specificity: When the actual value is negative, how often is the prediction correct?\n",
    "    specificity = TN / float(TN + FP)\n",
    "    print('Specificity: %.3f' % specificity)\n",
    "    \n",
    "    #from imblearn.metrics import specificity_score\n",
    "    #specificity_score(y_test, y_pred)\n",
    "    \n",
    "    # Precision: When a positive value is predicted, how often is the prediction correct?\n",
    "    precision = TP / float(TP + FP)\n",
    "    #print('Precision: %.3f' % precision)\n",
    "    print('Precision: %.3f' % metrics.precision_score(y_test, y_pred))\n",
    "    \n",
    "    # F score\n",
    "    f_score = 2*TP / float(2*TP + FP + FN)\n",
    "    #print('F score: %.3f' % f_score)\n",
    "    print('F1 score: %.3f' % metrics.f1_score(y_test,y_pred))\n",
    "    \n",
    "    #Evaluate the model using other performance metrics - REDUNDANT, COMMENTED OUT FOR NOW\n",
    "    # from sklearn.metrics import classification_report\n",
    "    # print(classification_report(y_test,y_pred))\n",
    "\n",
    "    # AUC\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "    print('AUC: %.3f' % auc)\n",
    "\n",
    "    # CM matrix plot\n",
    "    from sklearn import metrics\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = None)\n",
    "    \n",
    "    cm_display.plot()\n",
    "    plt.show()\n",
    "    # 0 = GS, 1 = POAG\n",
    "    \n",
    "    # ROC curve plot\n",
    "    plt.plot(fpr,tpr,label=\"auc=\"+str(auc))\n",
    "    plt.xlabel(\"False positive rate (1-specificity)\")\n",
    "    plt.ylabel(\"True positive rate (sensitivity)\")\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to do prediction after multiple imputation:\n",
    "# https://github.com/amices/mice/issues/82\n",
    "# https://stackoverflow.com/questions/68460923/how-to-do-the-prediction-after-multiple-imputation-with-mice-package\n",
    "logreg_models = []\n",
    "Xtrain_finals = []\n",
    "Xtest_finals = []\n",
    "\n",
    "# MUST DROP REFERENCE COLUMN FOR ONE-HOT-ENCODED VARIABLES\n",
    "#chosen_ref_labels = ['GHT_Within Normal Limits', 'Gender_M', 'Ethnicity_Other']\n",
    "chosen_ref_labels = ['GHT_Within Normal Limits','GHT_Borderline', 'Gender_M', 'Ethnicity_Other']\n",
    "penalty=None#'l1', 'l2', 'elasticnet', or None\n",
    "solver='saga' # 'lbfgs', 'saga' (only saga supports l1 and elasticnet)\n",
    "\n",
    "for X_train, X_test in zip(X_train_imputedsets, X_test_imputedsets):\n",
    "    # SIMPLE LOGISTIC REGRESSION\n",
    "    drop_cols = [x for x in X_train.columns if x in chosen_ref_labels]\n",
    "    X_train_dropped = X_train.drop(columns=drop_cols)\n",
    "    X_test_dropped = X_test.drop(columns=drop_cols)\n",
    "\n",
    "    logreg = LogisticRegression(random_state=16, solver=solver, max_iter=1000, penalty=penalty)\n",
    "    logreg.fit(X_train_dropped, y_train)\n",
    "    logreg_models.append(logreg)\n",
    "\n",
    "    Xtrain_finals.append(X_train_dropped)\n",
    "    Xtest_finals.append(X_test_dropped)\n",
    "\n",
    "model_summary_mice(logreg_models, Xtrain_finals)\n",
    "compute_performance_mice(logreg_models, Xtest_finals, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't have to onehotencode, but xgboost performs better if does\n",
    "# keep dummy variables, don't drop ref label for decision trees\n",
    "\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "drop_NA=True\n",
    "if drop_NA:\n",
    "    # Drop NA if desired\n",
    "    x = X_train_keep_missing.dropna()\n",
    "    x_t = X_test_keep_missing.dropna()\n",
    "\n",
    "    y = y_train[y_train.index.isin(x.index)]\n",
    "    y_t = y_test[y_test.index.isin(x_t.index)]\n",
    "\n",
    "print(x.columns)\n",
    "\n",
    "#model = BaggingClassifier(estimator=SVC(), n_estimators=10, random_state=0) # bagged SVC\n",
    "#model=BaggingClassifier() # bagged decision trees (bc DecisionTree is default)\n",
    "model=SVC(probability=True) # probability=True to enable predict_proba function (slow)\n",
    "clf = model.fit(x,y)\n",
    "\n",
    "# define cross-validation evaluation procedure\n",
    "k = 10\n",
    "cv = RepeatedStratifiedKFold(n_splits=k, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, x, y, scoring='roc_auc', cv=cv)\n",
    "# summarize performance\n",
    "print('Mean AUC using %i-fold cross-validation: %.3f' % (k, mean(scores)))# AUC from 10-fold cv on TRAINING set, as opposed to AUC on test set computed in compute_performance -- if this better than AUC for test set, then model probably overfit\n",
    "print(\"\")\n",
    "\n",
    "# test performance\n",
    "EA.compute_performance(clf, x_t, y_t)\n",
    "EA.compute_performance_youden(clf, x_t, y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT multiple ROC curves\n",
    "- current version of this code requires running above multiple times for each roc curve I want to plot, then saving them manually and adding to global dictionary before plotting combined ROC curve\n",
    "- X_test and y_test have different #s for drop_NA bc drop_NA may drop diff # rows depending on which variables are included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models = {} # model label name: (model, associated X_test, associated y_test)\n",
    "# start with univarate models dict\n",
    "#models ={**models, **models_univariate} ## don't overwrite models just in case already contains stuff\n",
    "# map univariate model names\n",
    "key_mapping = {\n",
    "    'Average_RNFL_Thickness(μm)': 'OCT',\n",
    "    'MD': 'HVF',\n",
    "    'ML Feature Selection (Elastic Net)': 'ML Elastic Net'\n",
    "}\n",
    "# Function to rename keys in a dictionary\n",
    "def rename_keys(d, key_map):\n",
    "    return {key_map.get(k, k): v for k, v in d.items()}\n",
    "# Apply the renaming function to the dictionary\n",
    "models = rename_keys(models, key_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual model additions -- EDIT THE NAME AND MODEL NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Manual model additions\n",
    "name = \"Clinic Data\"\n",
    "mod = logreg\n",
    "models[name] = (mod, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## how to combine 2 dictionaries\n",
    "#all_models = {**models_univariate, **selected_models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select which models to plot\n",
    "#all_keys = ['Clinic Data', 'CDR', 'OCT', 'HVF', 'OCT+HVF', 'CDR+OCT+HVF', 'All Features', 'Domain Knowledge', 'All Features (Ridge)', 'ML Elastic Net'] # The keys you want\n",
    "#wanted_keys = ['All Features', 'Domain Knowledge', 'ML Elastic Net']\n",
    "wanted_keys = ['Clinic Data', 'CDR', 'OCT', 'HVF', 'OCT+HVF', 'CDR+OCT+HVF', 'Domain Knowledge', 'ML Elastic Net'] # The keys you want\n",
    "selected_models = dict((k, models[k]) for k in wanted_keys if k in models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 8))\n",
    "for name, (m, xt, yt) in selected_models.items():\n",
    "    print (name)\n",
    "    fpr, tpr, auc, optimal_idx, optimal_threshold = EA.compute_performance_youden(m, xt, yt, plot=False)\n",
    "    #plt.plot(fpr, tpr, label=\"%s (AUC=%s, Youden's=%.3f)\" % (name, auc, (tpr[optimal_idx] - fpr[optimal_idx])))\n",
    "    plt.plot(fpr, tpr, label=\"%s (AUC=%s)\" % (name, auc))\n",
    "    #plt.scatter(fpr[optimal_idx], tpr[optimal_idx], marker='o', color='red')\n",
    "    print (\"\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(\"/home/yukim3003/Figure_1.png\", format=\"png\", dpi=300)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Save a high quality plot - nvm this has to be in same cell as original plot creation to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(configuration_records.working_dir/'Execution_Assets/Multimodal_Figures/')\n",
    "fig_path = configuration_records.working_dir/'Execution_Assets/Multimodal_Figures/Figure_1.png'\n",
    "\n",
    "# Save the plot with higher DPI\n",
    "plt.savefig(fig_path, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workaround\n",
    "plt.savefig(\"/home/yukim3003/Figure_1.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "cache_path = configuration_records.working_dir/'Execution_Assets/Multimodal_Analysis/models_cache.pkl'\n",
    "\n",
    "# Cache the models dictionary to a file\n",
    "with open(cache_path, 'wb') as f:\n",
    "    pickle.dump(models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the cached models dictionary later\n",
    "with open(cache_path, 'rb') as f:\n",
    "    cached_models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access a specific saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a specific model that is saved\n",
    "name = \"Domain Knowledge\"\n",
    "m, xt, yt = models[name]\n",
    "EA.compute_performance(m, xt, yt)\n",
    "EA.compute_performance_youden(m, xt, yt, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_assets = EA.execution_upload(configuration_records.execution_rid, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
